============================= test session starts ==============================
platform linux -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0 -- /home/codey/Projects/langgraph_mcp_agent/.venv/bin/python3
cachedir: .pytest_cache
benchmark: 4.0.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/codey/Projects/langgraph_mcp_agent
configfile: pytest.ini
plugins: anyio-4.11.0, xdist-3.5.0, timeout-2.4.0, mock-3.12.0, cov-4.1.0, asyncio-0.26.0, langsmith-0.4.34, Faker-22.0.0, respx-0.20.2, benchmark-4.0.0
asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 90 items

tests/performance/test_benchmarks.py::TestJWTBenchmarks::test_jwt_encoding_performance PASSED [  1%]
tests/performance/test_benchmarks.py::TestJWTBenchmarks::test_jwt_decoding_performance PASSED [  2%]
tests/performance/test_benchmarks.py::TestJWTBenchmarks::test_jwt_validation_performance PASSED [  3%]
tests/performance/test_benchmarks.py::TestOpenFGABenchmarks::test_authorization_check_performance PASSED [  4%]
tests/performance/test_benchmarks.py::TestOpenFGABenchmarks::test_batch_authorization_performance PASSED [  5%]
tests/performance/test_benchmarks.py::TestLLMBenchmarks::test_llm_request_performance PASSED [  6%]
tests/performance/test_benchmarks.py::TestAgentBenchmarks::test_agent_initialization_performance PASSED [  7%]
tests/performance/test_benchmarks.py::TestAgentBenchmarks::test_message_processing_performance PASSED [  8%]
tests/performance/test_benchmarks.py::TestResourceBenchmarks::test_state_serialization_performance PASSED [ 10%]
tests/performance/test_benchmarks.py::TestResourceBenchmarks::test_state_deserialization_performance PASSED [ 11%]
tests/test_agent.py::TestAgentState::test_agent_state_structure PASSED   [ 12%]
tests/test_agent.py::TestAgentGraph::test_create_agent_graph PASSED      [ 13%]
tests/test_agent.py::TestAgentGraph::test_route_input_to_respond FAILED  [ 14%]
tests/test_agent.py::TestAgentGraph::test_route_input_to_tools FAILED    [ 15%]
tests/test_agent.py::TestAgentGraph::test_route_with_calculate_keyword FAILED [ 16%]
tests/test_agent.py::TestAgentGraph::test_agent_with_conversation_history FAILED [ 17%]
tests/test_agent.py::TestAgentGraph::test_checkpointing_works FAILED     [ 18%]
tests/test_agent.py::TestAgentGraph::test_state_accumulation PASSED      [ 20%]
tests/test_agent.py::TestAgentIntegration::test_real_llm_invocation SKIPPED [ 21%]
tests/test_auth.py::TestAuthMiddleware::test_init PASSED                 [ 22%]
tests/test_auth.py::TestAuthMiddleware::test_authenticate_success PASSED [ 23%]
tests/test_auth.py::TestAuthMiddleware::test_authenticate_user_not_found PASSED [ 24%]
tests/test_auth.py::TestAuthMiddleware::test_authenticate_inactive_user PASSED [ 25%]
tests/test_auth.py::TestAuthMiddleware::test_authorize_with_openfga_success PASSED [ 26%]
tests/test_auth.py::TestAuthMiddleware::test_authorize_with_openfga_denied PASSED [ 27%]
tests/test_auth.py::TestAuthMiddleware::test_authorize_with_openfga_error PASSED [ 28%]
tests/test_auth.py::TestAuthMiddleware::test_authorize_fallback_admin_access PASSED [ 30%]
tests/test_auth.py::TestAuthMiddleware::test_authorize_fallback_premium_user PASSED [ 31%]
tests/test_auth.py::TestAuthMiddleware::test_authorize_fallback_standard_user PASSED [ 32%]
tests/test_auth.py::TestAuthMiddleware::test_authorize_fallback_viewer_access PASSED [ 33%]
tests/test_auth.py::TestAuthMiddleware::test_authorize_fallback_unknown_user PASSED [ 34%]
tests/test_auth.py::TestAuthMiddleware::test_list_accessible_resources_success PASSED [ 35%]
tests/test_auth.py::TestAuthMiddleware::test_list_accessible_resources_no_openfga PASSED [ 36%]
tests/test_auth.py::TestAuthMiddleware::test_list_accessible_resources_error PASSED [ 37%]
tests/test_auth.py::TestAuthMiddleware::test_create_token_success PASSED [ 38%]
tests/test_auth.py::TestAuthMiddleware::test_create_token_expiration PASSED [ 40%]
tests/test_auth.py::TestAuthMiddleware::test_create_token_user_not_found PASSED [ 41%]
tests/test_auth.py::TestAuthMiddleware::test_verify_token_success PASSED [ 42%]
tests/test_auth.py::TestAuthMiddleware::test_verify_token_expired PASSED [ 43%]
tests/test_auth.py::TestAuthMiddleware::test_verify_token_invalid PASSED [ 44%]
tests/test_auth.py::TestAuthMiddleware::test_verify_token_wrong_secret PASSED [ 45%]
tests/test_auth.py::TestRequireAuthDecorator::test_require_auth_success PASSED [ 46%]
tests/test_auth.py::TestRequireAuthDecorator::test_require_auth_no_credentials PASSED [ 47%]
tests/test_auth.py::TestRequireAuthDecorator::test_require_auth_invalid_user PASSED [ 48%]
tests/test_auth.py::TestRequireAuthDecorator::test_require_auth_with_authorization PASSED [ 50%]
tests/test_auth.py::TestRequireAuthDecorator::test_require_auth_authorization_denied PASSED [ 51%]
tests/test_auth.py::TestStandaloneVerifyToken::test_standalone_verify_token_success PASSED [ 52%]
tests/test_auth.py::TestStandaloneVerifyToken::test_standalone_verify_token_default_secret PASSED [ 53%]
tests/test_auth.py::TestStandaloneVerifyToken::test_standalone_verify_token_invalid PASSED [ 54%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_server_info_endpoint FAILED [ 55%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_health_endpoint FAILED [ 56%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_initialize_method FAILED [ 57%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_tools_list_method FAILED [ 58%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_tools_call_method FAILED [ 60%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_tools_call_unauthorized FAILED [ 61%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_streaming_response FAILED [ 62%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_resources_list_method FAILED [ 63%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_invalid_method FAILED [ 64%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_malformed_request FAILED [ 65%]
tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_cors_headers FAILED [ 66%]
tests/test_mcp_streamable.py::TestMCPEndToEnd::test_complete_chat_flow SKIPPED [ 67%]
tests/test_openfga_client.py::TestOpenFGAClient::test_init PASSED        [ 68%]
tests/test_openfga_client.py::TestOpenFGAClient::test_check_permission_allowed PASSED [ 70%]
tests/test_openfga_client.py::TestOpenFGAClient::test_check_permission_denied PASSED [ 71%]
tests/test_openfga_client.py::TestOpenFGAClient::test_check_permission_error PASSED [ 72%]
tests/test_openfga_client.py::TestOpenFGAClient::test_write_tuples_success PASSED [ 73%]
tests/test_openfga_client.py::TestOpenFGAClient::test_write_tuples_error PASSED [ 74%]
tests/test_openfga_client.py::TestOpenFGAClient::test_delete_tuples_success PASSED [ 75%]
tests/test_openfga_client.py::TestOpenFGAClient::test_delete_tuples_error PASSED [ 76%]
tests/test_openfga_client.py::TestOpenFGAClient::test_list_objects_success PASSED [ 77%]
tests/test_openfga_client.py::TestOpenFGAClient::test_list_objects_empty PASSED [ 78%]
tests/test_openfga_client.py::TestOpenFGAClient::test_list_objects_error PASSED [ 80%]
tests/test_openfga_client.py::TestOpenFGAIntegration::test_full_authorization_flow SKIPPED [ 81%]
tests/test_secrets_manager.py::TestSecretsManager::test_init_with_credentials PASSED [ 82%]
tests/test_secrets_manager.py::TestSecretsManager::test_init_without_credentials PASSED [ 83%]
tests/test_secrets_manager.py::TestSecretsManager::test_init_from_environment PASSED [ 84%]
tests/test_secrets_manager.py::TestSecretsManager::test_init_client_error PASSED [ 85%]
tests/test_secrets_manager.py::TestSecretsManager::test_get_secret_success PASSED [ 86%]
tests/test_secrets_manager.py::TestSecretsManager::test_get_secret_with_path PASSED [ 87%]
tests/test_secrets_manager.py::TestSecretsManager::test_get_secret_caching PASSED [ 88%]
tests/test_secrets_manager.py::TestSecretsManager::test_get_secret_no_cache PASSED [ 90%]
tests/test_secrets_manager.py::TestSecretsManager::test_get_secret_fallback_to_env PASSED [ 91%]
tests/test_secrets_manager.py::TestSecretsManager::test_get_secret_fallback_to_default PASSED [ 92%]
tests/test_secrets_manager.py::TestSecretsManager::test_get_secret_error_uses_fallback PASSED [ 93%]
tests/test_secrets_manager.py::TestSecretsManager::test_get_secret_error_tries_env_first PASSED [ 94%]
tests/test_secrets_manager.py::TestSecretsManager::test_create_secret_success PASSED [ 95%]
tests/test_secrets_manager.py::TestSecretsManager::test_update_secret_success PASSED [ 96%]
tests/test_secrets_manager.py::TestSecretsManager::test_delete_secret_success PASSED [ 97%]
tests/test_secrets_manager.py::TestSecretsManager::test_clear_cache FAILED [ 98%]
tests/test_secrets_manager.py::TestSecretsManagerIntegration::test_full_secret_lifecycle SKIPPED [100%]

=================================== FAILURES ===================================
__________________ TestAgentGraph.test_route_input_to_respond __________________
llm_factory.py:139: in invoke
    response: ModelResponse = completion(**params)
                              ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/utils.py:1356: in wrapper
    raise e
.venv/lib/python3.13/site-packages/litellm/utils.py:1231: in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/main.py:3733: in completion
    raise exception_type(
.venv/lib/python3.13/site-packages/litellm/main.py:1146: in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:418: in get_llm_provider
    raise e
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:395: in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
E   litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
E    Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:
tests/test_agent.py:65: in test_route_input_to_respond
    result = graph.invoke(initial_state, config={"configurable": {"thread_id": "test-1"}})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3085: in invoke
    for chunk in self.stream(
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2674: in stream
    for _ in runner.tick(
.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:401: in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
agent.py:87: in generate_response
    response = model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:168: in invoke
    return self._try_fallback(messages, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:272: in _try_fallback
    raise RuntimeError("All models failed including fallbacks")
E   RuntimeError: All models failed including fallbacks
E   During task with name 'respond' and id '81503bf9-d984-3c3f-df02-eb7d6f5e3a70'
----------------------------- Captured stdout call -----------------------------

[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.

----------------------------- Captured stderr call -----------------------------
[92m17:30:33 - LiteLLM:ERROR[0m: vertex_llm_base.py:494 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
------------------------------ Captured log call -------------------------------
ERROR    mcp-server-langgraph:llm_factory.py:157 LLM invocation failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gemini-2.5-pro
ERROR    LiteLLM:vertex_llm_base.py:494 Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gemini-2.5-pro failed: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2249, in exception_type
    raise APIConnectionError(
    ...<8 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: claude-3-5-sonnet-20241022
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model claude-3-5-sonnet-20241022 failed: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 445, in completion
    response = client.post(
        api_base,
    ...<2 lines>...
        timeout=timeout,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 802, in post
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 784, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '401 Unauthorized' for url 'http://0.0.0.0:4000/v1/messages'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2329, in completion
    response = anthropic_chat_completions.completion(
        model=model,
    ...<15 lines>...
        custom_llm_provider=custom_llm_provider,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 460, in completion
    raise AnthropicError(
    ...<3 lines>...
    )
litellm.llms.anthropic.common_utils.AnthropicError: {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 594, in exception_type
    raise AuthenticationError(
    ...<3 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gpt-4o
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gpt-4o failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 745, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 647, in completion
    openai_client: OpenAI = self._get_openai_client(  # type: ignore
                            ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
        is_async=False,
        ^^^^^^^^^^^^^^^
    ...<6 lines>...
        client=client,
        ^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 395, in _get_openai_client
    _new_client = OpenAI(
        api_key=api_key,
    ...<4 lines>...
        organization=organization,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/openai/_client.py", line 137, in __init__
    raise OpenAIError(
        "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
    )
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2125, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2097, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<16 lines>...
        shared_session=shared_session,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 756, in completion
    raise OpenAIError(
    ...<4 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 424, in exception_type
    raise AuthenticationError(
    ...<5 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
___________________ TestAgentGraph.test_route_input_to_tools ___________________
llm_factory.py:139: in invoke
    response: ModelResponse = completion(**params)
                              ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/utils.py:1356: in wrapper
    raise e
.venv/lib/python3.13/site-packages/litellm/utils.py:1231: in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/main.py:3733: in completion
    raise exception_type(
.venv/lib/python3.13/site-packages/litellm/main.py:1146: in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:418: in get_llm_provider
    raise e
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:395: in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
E   litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
E    Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:
tests/test_agent.py:90: in test_route_input_to_tools
    result = graph.invoke(initial_state, config={"configurable": {"thread_id": "test-2"}})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3085: in invoke
    for chunk in self.stream(
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2674: in stream
    for _ in runner.tick(
.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:401: in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
agent.py:87: in generate_response
    response = model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:168: in invoke
    return self._try_fallback(messages, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:272: in _try_fallback
    raise RuntimeError("All models failed including fallbacks")
E   RuntimeError: All models failed including fallbacks
E   During task with name 'respond' and id 'ee16040d-b086-dc52-1d13-0b4441383453'
----------------------------- Captured stdout call -----------------------------

[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.

----------------------------- Captured stderr call -----------------------------
[92m17:30:34 - LiteLLM:ERROR[0m: vertex_llm_base.py:494 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
------------------------------ Captured log call -------------------------------
ERROR    mcp-server-langgraph:llm_factory.py:157 LLM invocation failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gemini-2.5-pro
ERROR    LiteLLM:vertex_llm_base.py:494 Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gemini-2.5-pro failed: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2249, in exception_type
    raise APIConnectionError(
    ...<8 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: claude-3-5-sonnet-20241022
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model claude-3-5-sonnet-20241022 failed: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 445, in completion
    response = client.post(
        api_base,
    ...<2 lines>...
        timeout=timeout,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 802, in post
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 784, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '401 Unauthorized' for url 'http://0.0.0.0:4000/v1/messages'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2329, in completion
    response = anthropic_chat_completions.completion(
        model=model,
    ...<15 lines>...
        custom_llm_provider=custom_llm_provider,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 460, in completion
    raise AnthropicError(
    ...<3 lines>...
    )
litellm.llms.anthropic.common_utils.AnthropicError: {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 594, in exception_type
    raise AuthenticationError(
    ...<3 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gpt-4o
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gpt-4o failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 745, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 647, in completion
    openai_client: OpenAI = self._get_openai_client(  # type: ignore
                            ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
        is_async=False,
        ^^^^^^^^^^^^^^^
    ...<6 lines>...
        client=client,
        ^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 395, in _get_openai_client
    _new_client = OpenAI(
        api_key=api_key,
    ...<4 lines>...
        organization=organization,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/openai/_client.py", line 137, in __init__
    raise OpenAIError(
        "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
    )
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2125, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2097, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<16 lines>...
        shared_session=shared_session,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 756, in completion
    raise OpenAIError(
    ...<4 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 424, in exception_type
    raise AuthenticationError(
    ...<5 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
_______________ TestAgentGraph.test_route_with_calculate_keyword _______________
llm_factory.py:139: in invoke
    response: ModelResponse = completion(**params)
                              ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/utils.py:1356: in wrapper
    raise e
.venv/lib/python3.13/site-packages/litellm/utils.py:1231: in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/main.py:3733: in completion
    raise exception_type(
.venv/lib/python3.13/site-packages/litellm/main.py:1146: in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:418: in get_llm_provider
    raise e
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:395: in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
E   litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
E    Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:
tests/test_agent.py:114: in test_route_with_calculate_keyword
    result = graph.invoke(initial_state, config={"configurable": {"thread_id": "test-3"}})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3085: in invoke
    for chunk in self.stream(
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2674: in stream
    for _ in runner.tick(
.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:401: in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
agent.py:87: in generate_response
    response = model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:168: in invoke
    return self._try_fallback(messages, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:272: in _try_fallback
    raise RuntimeError("All models failed including fallbacks")
E   RuntimeError: All models failed including fallbacks
E   During task with name 'respond' and id '31c295a6-9d9f-b027-16c1-8509a44719e8'
----------------------------- Captured stdout call -----------------------------

[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.

----------------------------- Captured stderr call -----------------------------
[92m17:30:34 - LiteLLM:ERROR[0m: vertex_llm_base.py:494 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
------------------------------ Captured log call -------------------------------
ERROR    mcp-server-langgraph:llm_factory.py:157 LLM invocation failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gemini-2.5-pro
ERROR    LiteLLM:vertex_llm_base.py:494 Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gemini-2.5-pro failed: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2249, in exception_type
    raise APIConnectionError(
    ...<8 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: claude-3-5-sonnet-20241022
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model claude-3-5-sonnet-20241022 failed: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 445, in completion
    response = client.post(
        api_base,
    ...<2 lines>...
        timeout=timeout,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 802, in post
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 784, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '401 Unauthorized' for url 'http://0.0.0.0:4000/v1/messages'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2329, in completion
    response = anthropic_chat_completions.completion(
        model=model,
    ...<15 lines>...
        custom_llm_provider=custom_llm_provider,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 460, in completion
    raise AnthropicError(
    ...<3 lines>...
    )
litellm.llms.anthropic.common_utils.AnthropicError: {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 594, in exception_type
    raise AuthenticationError(
    ...<3 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gpt-4o
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gpt-4o failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 745, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 647, in completion
    openai_client: OpenAI = self._get_openai_client(  # type: ignore
                            ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
        is_async=False,
        ^^^^^^^^^^^^^^^
    ...<6 lines>...
        client=client,
        ^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 395, in _get_openai_client
    _new_client = OpenAI(
        api_key=api_key,
    ...<4 lines>...
        organization=organization,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/openai/_client.py", line 137, in __init__
    raise OpenAIError(
        "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
    )
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2125, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2097, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<16 lines>...
        shared_session=shared_session,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 756, in completion
    raise OpenAIError(
    ...<4 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 424, in exception_type
    raise AuthenticationError(
    ...<5 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
_____________ TestAgentGraph.test_agent_with_conversation_history ______________
llm_factory.py:139: in invoke
    response: ModelResponse = completion(**params)
                              ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/utils.py:1356: in wrapper
    raise e
.venv/lib/python3.13/site-packages/litellm/utils.py:1231: in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/main.py:3733: in completion
    raise exception_type(
.venv/lib/python3.13/site-packages/litellm/main.py:1146: in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:418: in get_llm_provider
    raise e
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:395: in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
E   litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
E    Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:
tests/test_agent.py:141: in test_agent_with_conversation_history
    result = graph.invoke(initial_state, config={"configurable": {"thread_id": "test-4"}})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3085: in invoke
    for chunk in self.stream(
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2674: in stream
    for _ in runner.tick(
.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:401: in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
agent.py:87: in generate_response
    response = model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:168: in invoke
    return self._try_fallback(messages, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:272: in _try_fallback
    raise RuntimeError("All models failed including fallbacks")
E   RuntimeError: All models failed including fallbacks
E   During task with name 'respond' and id 'de60f177-ab08-8f20-1ada-e2440adbfd9d'
----------------------------- Captured stdout call -----------------------------

[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.

----------------------------- Captured stderr call -----------------------------
[92m17:30:35 - LiteLLM:ERROR[0m: vertex_llm_base.py:494 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
------------------------------ Captured log call -------------------------------
ERROR    mcp-server-langgraph:llm_factory.py:157 LLM invocation failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gemini-2.5-pro
ERROR    LiteLLM:vertex_llm_base.py:494 Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gemini-2.5-pro failed: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2249, in exception_type
    raise APIConnectionError(
    ...<8 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: claude-3-5-sonnet-20241022
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model claude-3-5-sonnet-20241022 failed: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 445, in completion
    response = client.post(
        api_base,
    ...<2 lines>...
        timeout=timeout,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 802, in post
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 784, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '401 Unauthorized' for url 'http://0.0.0.0:4000/v1/messages'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2329, in completion
    response = anthropic_chat_completions.completion(
        model=model,
    ...<15 lines>...
        custom_llm_provider=custom_llm_provider,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 460, in completion
    raise AnthropicError(
    ...<3 lines>...
    )
litellm.llms.anthropic.common_utils.AnthropicError: {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 594, in exception_type
    raise AuthenticationError(
    ...<3 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gpt-4o
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gpt-4o failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 745, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 647, in completion
    openai_client: OpenAI = self._get_openai_client(  # type: ignore
                            ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
        is_async=False,
        ^^^^^^^^^^^^^^^
    ...<6 lines>...
        client=client,
        ^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 395, in _get_openai_client
    _new_client = OpenAI(
        api_key=api_key,
    ...<4 lines>...
        organization=organization,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/openai/_client.py", line 137, in __init__
    raise OpenAIError(
        "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
    )
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2125, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2097, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<16 lines>...
        shared_session=shared_session,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 756, in completion
    raise OpenAIError(
    ...<4 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 424, in exception_type
    raise AuthenticationError(
    ...<5 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
___________________ TestAgentGraph.test_checkpointing_works ____________________
llm_factory.py:139: in invoke
    response: ModelResponse = completion(**params)
                              ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/utils.py:1356: in wrapper
    raise e
.venv/lib/python3.13/site-packages/litellm/utils.py:1231: in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/litellm/main.py:3733: in completion
    raise exception_type(
.venv/lib/python3.13/site-packages/litellm/main.py:1146: in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:418: in get_llm_provider
    raise e
.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:395: in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
E   litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
E    Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:
tests/test_agent.py:166: in test_checkpointing_works
    result1 = graph.invoke(state1, config={"configurable": {"thread_id": thread_id}})
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3085: in invoke
    for chunk in self.stream(
.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2674: in stream
    for _ in runner.tick(
.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:401: in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
agent.py:87: in generate_response
    response = model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:168: in invoke
    return self._try_fallback(messages, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
llm_factory.py:272: in _try_fallback
    raise RuntimeError("All models failed including fallbacks")
E   RuntimeError: All models failed including fallbacks
E   During task with name 'respond' and id 'f0e2944c-7bcf-6775-a564-8a8b241bd59a'
----------------------------- Captured stdout call -----------------------------

[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.

----------------------------- Captured stderr call -----------------------------
[92m17:30:36 - LiteLLM:ERROR[0m: vertex_llm_base.py:494 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
------------------------------ Captured log call -------------------------------
ERROR    mcp-server-langgraph:llm_factory.py:157 LLM invocation failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gemini-2.5-pro
ERROR    LiteLLM:vertex_llm_base.py:494 Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gemini-2.5-pro failed: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2249, in exception_type
    raise APIConnectionError(
    ...<8 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: No module named 'google.auth'
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2925, in completion
    model_response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2063, in completion
    _auth_header, vertex_project = self._ensure_access_token(
                                   ~~~~~~~~~~~~~~~~~~~~~~~~~^
        credentials=vertex_credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=vertex_project,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
        custom_llm_provider=custom_llm_provider,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 267, in _ensure_access_token
    return self.get_access_token(
           ~~~~~~~~~~~~~~~~~~~~~^
        credentials=credentials,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        project_id=project_id,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 497, in get_access_token
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 490, in get_access_token
    _credentials, credential_project_id = self.load_auth(
                                          ~~~~~~~~~~~~~~^
        credentials=credentials, project_id=project_id
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 114, in load_auth
    creds, creds_project_id = self._credentials_from_default_auth(
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py", line 158, in _credentials_from_default_auth
    import google.auth as google_auth
ModuleNotFoundError: No module named 'google.auth'

WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: claude-3-5-sonnet-20241022
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model claude-3-5-sonnet-20241022 failed: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 445, in completion
    response = client.post(
        api_base,
    ...<2 lines>...
        timeout=timeout,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 802, in post
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py", line 784, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '401 Unauthorized' for url 'http://0.0.0.0:4000/v1/messages'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2329, in completion
    response = anthropic_chat_completions.completion(
        model=model,
    ...<15 lines>...
        custom_llm_provider=custom_llm_provider,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/anthropic/chat/handler.py", line 460, in completion
    raise AnthropicError(
    ...<3 lines>...
    )
litellm.llms.anthropic.common_utils.AnthropicError: {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 594, in exception_type
    raise AuthenticationError(
    ...<3 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AnthropicException - {"error":{"message":"Authentication Error, LiteLLM Virtual Key expected. Received=test-anthropic-key, expected to start with 'sk-'.","type":"auth_error","param":"None","code":"401"}}
WARNING  mcp-server-langgraph:llm_factory.py:236 Trying fallback model: gpt-4o
ERROR    mcp-server-langgraph:llm_factory.py:266 Fallback model gpt-4o failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 139, in invoke
    response: ModelResponse = completion(**params)
                              ~~~~~~~~~~^^^^^^^^^^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 1146, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 745, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 647, in completion
    openai_client: OpenAI = self._get_openai_client(  # type: ignore
                            ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
        is_async=False,
        ^^^^^^^^^^^^^^^
    ...<6 lines>...
        client=client,
        ^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 395, in _get_openai_client
    _new_client = OpenAI(
        api_key=api_key,
    ...<4 lines>...
        organization=organization,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/openai/_client.py", line 137, in __init__
    raise OpenAIError(
        "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
    )
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2125, in completion
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 2097, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<16 lines>...
        shared_session=shared_session,
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py", line 756, in completion
    raise OpenAIError(
    ...<4 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/llm_factory.py", line 243, in _try_fallback
    response = completion(
        model=fallback_model,
    ...<3 lines>...
        timeout=self.timeout
    )
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1356, in wrapper
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py", line 1231, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py", line 3733, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 424, in exception_type
    raise AuthenticationError(
    ...<5 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
--------------------------- Captured stdout teardown ---------------------------
{
    "name": "secrets.get_secret",
    "context": {
        "trace_id": "0x4f61993426fdeb122df106d2cce4f17a",
        "span_id": "0xe76a559c25e6fe46",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:33.451241Z",
    "end_time": "2025-10-10T21:30:33.451387Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "secret.key": "OPENAI_API_KEY",
        "secret.path": "/"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "secrets.get_secret",
    "context": {
        "trace_id": "0xea63f73719ea97f6433fb741d1c7c7e9",
        "span_id": "0x87b9210f1edff001",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:33.451492Z",
    "end_time": "2025-10-10T21:30:33.451578Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "secret.key": "GOOGLE_API_KEY",
        "secret.path": "/"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "secrets.get_secret",
    "context": {
        "trace_id": "0xf964c3af667fd1c67166596fa39cd50f",
        "span_id": "0x40d7a333de71d5eb",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:33.451637Z",
    "end_time": "2025-10-10T21:30:33.451719Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "secret.key": "AZURE_API_KEY",
        "secret.path": "/"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "secrets.get_secret",
    "context": {
        "trace_id": "0x65e56aa25ba644b059829c521da90c2f",
        "span_id": "0x4baf2c8e9b8c8cc3",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:33.451766Z",
    "end_time": "2025-10-10T21:30:33.451842Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "secret.key": "AWS_ACCESS_KEY_ID",
        "secret.path": "/"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "secrets.get_secret",
    "context": {
        "trace_id": "0x8b3d02146d4c545063c1efd9191e6ad5",
        "span_id": "0x7a062489cbc978e6",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:33.451891Z",
    "end_time": "2025-10-10T21:30:33.451961Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "secret.key": "AWS_SECRET_ACCESS_KEY",
        "secret.path": "/"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "secrets.get_secret",
    "context": {
        "trace_id": "0xc671dfbb701713cb5c9fa605915f1c90",
        "span_id": "0x31913c4b3f53cf3e",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:33.452005Z",
    "end_time": "2025-10-10T21:30:33.452233Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "secret.key": "LANGSMITH_API_KEY",
        "secret.path": "/"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "secrets.get_secret",
    "context": {
        "trace_id": "0x855c40c70957bc86d974bd0f7bb779da",
        "span_id": "0x7022ef3bb06048ab",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:33.452289Z",
    "end_time": "2025-10-10T21:30:33.452359Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "secret.key": "LANGGRAPH_API_KEY",
        "secret.path": "/"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "llm.invoke",
    "context": {
        "trace_id": "0x4f0875cab0a1b87862439cd0a7855e2d",
        "span_id": "0x6076898c6d4b2f6a",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:33.471343Z",
    "end_time": "2025-10-10T21:30:33.585201Z",
    "status": {
        "status_code": "ERROR",
        "description": "RuntimeError: All models failed including fallbacks"
    },
    "attributes": {
        "llm.provider": "google",
        "llm.model": "gemini-2.5-flash-002"
    },
    "events": [
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:33.496196Z",
            "attributes": {
                "exception.type": "litellm.exceptions.BadRequestError",
                "exception.message": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
                "exception.escaped": "False"
            }
        },
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:33.585054Z",
            "attributes": {
                "exception.type": "RuntimeError",
                "exception.message": "All models failed including fallbacks",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 168, in invoke\n    return self._try_fallback(messages, **kwargs)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 272, in _try_fallback\n    raise RuntimeError(\"All models failed including fallbacks\")\nRuntimeError: All models failed including fallbacks\n",
                "exception.escaped": "False"
            }
        }
    ],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "llm.invoke",
    "context": {
        "trace_id": "0x90f4ba0ab8419d8c391ccb04eda3f95e",
        "span_id": "0x799fb253834d223d",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:34.230784Z",
    "end_time": "2025-10-10T21:30:34.319678Z",
    "status": {
        "status_code": "ERROR",
        "description": "RuntimeError: All models failed including fallbacks"
    },
    "attributes": {
        "llm.provider": "google",
        "llm.model": "gemini-2.5-flash-002"
    },
    "events": [
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:34.241256Z",
            "attributes": {
                "exception.type": "litellm.exceptions.BadRequestError",
                "exception.message": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
                "exception.escaped": "False"
            }
        },
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:34.319656Z",
            "attributes": {
                "exception.type": "RuntimeError",
                "exception.message": "All models failed including fallbacks",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 168, in invoke\n    return self._try_fallback(messages, **kwargs)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 272, in _try_fallback\n    raise RuntimeError(\"All models failed including fallbacks\")\nRuntimeError: All models failed including fallbacks\n",
                "exception.escaped": "False"
            }
        }
    ],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "llm.invoke",
    "context": {
        "trace_id": "0xdffe32d6752f50bf8dfc2d2eddd7a15d",
        "span_id": "0x9deb931c9152e519",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:34.885009Z",
    "end_time": "2025-10-10T21:30:34.973067Z",
    "status": {
        "status_code": "ERROR",
        "description": "RuntimeError: All models failed including fallbacks"
    },
    "attributes": {
        "llm.provider": "google",
        "llm.model": "gemini-2.5-flash-002"
    },
    "events": [
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:34.895509Z",
            "attributes": {
                "exception.type": "litellm.exceptions.BadRequestError",
                "exception.message": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
                "exception.escaped": "False"
            }
        },
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:34.973046Z",
            "attributes": {
                "exception.type": "RuntimeError",
                "exception.message": "All models failed including fallbacks",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 168, in invoke\n    return self._try_fallback(messages, **kwargs)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 272, in _try_fallback\n    raise RuntimeError(\"All models failed including fallbacks\")\nRuntimeError: All models failed including fallbacks\n",
                "exception.escaped": "False"
            }
        }
    ],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "llm.invoke",
    "context": {
        "trace_id": "0x0bb8c859c0c65602cc6b6520603adfa9",
        "span_id": "0x6bf5490ed2a579e2",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:35.519686Z",
    "end_time": "2025-10-10T21:30:35.622639Z",
    "status": {
        "status_code": "ERROR",
        "description": "RuntimeError: All models failed including fallbacks"
    },
    "attributes": {
        "llm.provider": "google",
        "llm.model": "gemini-2.5-flash-002"
    },
    "events": [
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:35.530349Z",
            "attributes": {
                "exception.type": "litellm.exceptions.BadRequestError",
                "exception.message": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
                "exception.escaped": "False"
            }
        },
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:35.622614Z",
            "attributes": {
                "exception.type": "RuntimeError",
                "exception.message": "All models failed including fallbacks",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 168, in invoke\n    return self._try_fallback(messages, **kwargs)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 272, in _try_fallback\n    raise RuntimeError(\"All models failed including fallbacks\")\nRuntimeError: All models failed including fallbacks\n",
                "exception.escaped": "False"
            }
        }
    ],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "llm.invoke",
    "context": {
        "trace_id": "0x59962f567733a1e643368b907fba64df",
        "span_id": "0x28a0869e38a73797",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.192715Z",
    "end_time": "2025-10-10T21:30:36.283021Z",
    "status": {
        "status_code": "ERROR",
        "description": "RuntimeError: All models failed including fallbacks"
    },
    "attributes": {
        "llm.provider": "google",
        "llm.model": "gemini-2.5-flash-002"
    },
    "events": [
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:36.205154Z",
            "attributes": {
                "exception.type": "litellm.exceptions.BadRequestError",
                "exception.message": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
                "exception.escaped": "False"
            }
        },
        {
            "name": "exception",
            "timestamp": "2025-10-10T21:30:36.283001Z",
            "attributes": {
                "exception.type": "RuntimeError",
                "exception.message": "All models failed including fallbacks",
                "exception.stacktrace": "Traceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 139, in invoke\n    response: ModelResponse = completion(**params)\n                              ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1356, in wrapper\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1231, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3733, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1146, in completion\n    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n                                                            ~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 418, in get_llm_provider\n    raise e\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 395, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemini-2.5-flash-002\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/trace/__init__.py\", line 589, in use_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n    yield span\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 168, in invoke\n    return self._try_fallback(messages, **kwargs)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n  File \"/home/codey/Projects/langgraph_mcp_agent/llm_factory.py\", line 272, in _try_fallback\n    raise RuntimeError(\"All models failed including fallbacks\")\nRuntimeError: All models failed including fallbacks\n",
                "exception.escaped": "False"
            }
        }
    ],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
_______________ TestMCPStreamableHTTP.test_server_info_endpoint ________________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
__________________ TestMCPStreamableHTTP.test_health_endpoint __________________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
_________________ TestMCPStreamableHTTP.test_initialize_method _________________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
_________________ TestMCPStreamableHTTP.test_tools_list_method _________________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
_________________ TestMCPStreamableHTTP.test_tools_call_method _________________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
______________ TestMCPStreamableHTTP.test_tools_call_unauthorized ______________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
________________ TestMCPStreamableHTTP.test_streaming_response _________________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
_______________ TestMCPStreamableHTTP.test_resources_list_method _______________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
__________________ TestMCPStreamableHTTP.test_invalid_method ___________________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
_________________ TestMCPStreamableHTTP.test_malformed_request _________________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
___________________ TestMCPStreamableHTTP.test_cors_headers ____________________
/usr/lib/python3.13/unittest/mock.py:1423: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.13/contextlib.py:141: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1405: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/contextlib.py:530: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/lib/python3.13/unittest/mock.py:1481: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/lib/python3.13/pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:392: in <module>
    mcp_server = MCPAgentStreamableServer()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:77: in __init__
    self.openfga = openfga_client or self._create_openfga_client()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
mcp_server_streamable.py:97: in _create_openfga_client
    return OpenFGAClient(
openfga_client.py:51: in __init__
    self.client = OpenFgaClient(configuration)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/client/client.py:168: in __init__
    self._api_client = ApiClient(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/api_client.py:103: in __init__
    self.rest_client = rest.RESTClientObject(configuration)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/openfga_sdk/rest.py:168: in __init__
    connector = aiohttp.TCPConnector(limit=maxsize, ssl=ssl_context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/aiohttp/connector.py:963: in __init__
    super().__init__(
.venv/lib/python3.13/site-packages/aiohttp/connector.py:313: in __init__
    loop = loop or asyncio.get_running_loop()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: no running event loop
_____________________ TestSecretsManager.test_clear_cache ______________________
tests/test_secrets_manager.py:321: in test_clear_cache
    manager.clear_cache()
    ^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'SecretsManager' object has no attribute 'clear_cache'

---------- coverage: platform linux, python 3.13.7-final-0 -----------
Name                             Stmts   Miss Branch BrPart  Cover   Missing
----------------------------------------------------------------------------
agent.py                            53      7      6      1    83%   19-21, 44-47, 58->66, 89
auth.py                            104      1     28      2    98%   183, 308->315
config.py                           95     11     24     11    80%   117-123, 130, 135->141, 141->147, 147->153, 153->159, 159->166, 167, 173, 179->186, 186->exit, 203-204, 213-215
example_client.py                   92     92      2      0     0%   4-193
example_openfga_usage.py            62     62      2      0     0%   5-177
health_check.py                     67     67     10      0     0%   4-187
langgraph_platform/__init__.py       1      1      0      0     0%   4
langgraph_platform/agent.py         45     45      4      0     0%   5-108
langsmith_config.py                 46     31     16      1    26%   20-33, 52-69, 86-98, 109, 118-121, 147-154
llm_factory.py                     117     53     30      7    52%   74-85, 103-107, 141-154, 170, 183-228, 234, 251-263, 276-316, 345, 350, 355
mcp_server.py                      120    120     20      0     0%   4-359
mcp_server_http.py                 173    173     28      0     0%   5-535
mcp_server_streamable.py           179    145     32      1    17%   80-85, 103-106, 111-227, 242-324, 333-357, 369-385, 396-606
observability.py                   117     14      8      3    85%   48, 65->69, 87->92, 226-238, 245, 262, 267
openfga_client.py                  100     28      2      0    73%   241-252, 281, 396-430, 437-458
secrets_manager.py                 125     38     30      4    66%   178-216, 239-240, 263-268, 289-290, 312-317, 336-337, 358-363, 372-380, 391->397
setup_infisical.py                  79     79     16      0     0%   5-145
setup_openfga.py                    53     53      6      0     0%   5-138
----------------------------------------------------------------------------
TOTAL                             1628   1020    264     30    36%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml

FAIL Required test coverage of 80% not reached. Total coverage: 36.15%

------------------------------------------------------------------------------------------------------------ benchmark: 10 tests -------------------------------------------------------------------------------------------------------------
Name (time in ns)                                   Min                       Max                    Mean                 StdDev                  Median                   IQR            Outliers  OPS (Kops/s)            Rounds  Iterations
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
test_agent_initialization_performance          323.0991 (1.0)         12,289.0458 (1.72)         368.2596 (1.0)         141.7675 (1.00)         358.6523 (1.0)          7.0082 (1.0)     1752;7194    2,715.4754 (1.0)      140566          20
test_llm_request_performance                 1,352.0475 (4.18)         7,163.9661 (1.0)        1,438.3488 (3.91)        141.7544 (1.0)        1,423.0609 (3.97)        30.0352 (4.29)      166;405      695.2417 (0.26)      20027           1
test_batch_authorization_performance         1,381.8499 (4.28)        43,551.9032 (6.08)       1,528.7916 (4.15)      1,013.6341 (7.15)       1,442.8515 (4.02)        30.0352 (4.29)     173;1105      654.1114 (0.24)      19450           1
test_message_processing_performance          1,383.0140 (4.28)        53,320.0800 (7.44)       1,496.8245 (4.06)        744.9912 (5.26)       1,453.0960 (4.05)        30.7336 (4.39)      111;409      668.0810 (0.25)      20683           1
test_authorization_check_performance         1,402.1061 (4.34)        12,313.0158 (1.72)       1,492.9174 (4.05)        462.7982 (3.26)       1,441.9202 (4.02)        30.0352 (4.29)         5;91      669.8294 (0.25)        666           1
test_jwt_decoding_performance               13,665.9946 (42.30)      195,987.0569 (27.36)     15,570.8283 (42.28)     6,687.9385 (47.18)     15,069.0321 (42.02)      220.0250 (31.40)      60;858       64.2227 (0.02)       7654           1
test_jwt_encoding_performance               14,447.1414 (44.71)      153,939.0069 (21.49)     15,335.0330 (41.64)     5,601.8917 (39.52)     14,898.1344 (41.54)      221.8876 (31.66)      36;446       65.2102 (0.02)       4886           1
test_jwt_validation_performance             15,308.8477 (47.38)      195,757.0203 (27.33)     16,208.1555 (44.01)     5,407.1638 (38.14)     15,889.0616 (44.30)      189.7570 (27.08)     64;1002       61.6973 (0.02)      17663           1
test_state_deserialization_performance     214,081.0248 (662.59)   1,691,540.0047 (236.12)   223,868.1804 (607.91)   67,440.1119 (475.75)   219,461.5081 (611.91)   2,994.6677 (427.31)       3;31        4.4669 (0.00)        826           1
test_state_serialization_performance       220,022.8628 (680.98)   1,693,102.9968 (236.34)   233,031.6924 (632.79)   56,090.2903 (395.69)   228,408.0256 (636.85)   5,740.0321 (819.04)     25;115        4.2913 (0.00)       3637           1
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Legend:
  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.
  OPS: Operations Per Second, computed as 1 / Mean
=========================== short test summary info ============================
FAILED tests/test_agent.py::TestAgentGraph::test_route_input_to_respond - RuntimeError: All models failed including fallbacks
During task with name 'respond' and id '81503bf9-d984-3c3f-df02-eb7d6f5e3a70'
FAILED tests/test_agent.py::TestAgentGraph::test_route_input_to_tools - RuntimeError: All models failed including fallbacks
During task with name 'respond' and id 'ee16040d-b086-dc52-1d13-0b4441383453'
FAILED tests/test_agent.py::TestAgentGraph::test_route_with_calculate_keyword - RuntimeError: All models failed including fallbacks
During task with name 'respond' and id '31c295a6-9d9f-b027-16c1-8509a44719e8'{
    "name": "auth.authenticate",
    "context": {
        "trace_id": "0x5e49a31704da7ec9345531ac652146b1",
        "span_id": "0xff68825437dcb7a3",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.851029Z",
    "end_time": "2025-10-10T21:30:36.851068Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "auth.username": "alice"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authenticate",
    "context": {
        "trace_id": "0x1ad3d1765e822f1532acb409229831cc",
        "span_id": "0x3b067276cb75037a",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.852027Z",
    "end_time": "2025-10-10T21:30:36.852154Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "auth.username": "nonexistent"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authenticate",
    "context": {
        "trace_id": "0x2f8314e26433ef48b1ea7046b2640cae",
        "span_id": "0xde044fed8e63390e",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.853104Z",
    "end_time": "2025-10-10T21:30:36.853194Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "auth.username": "alice"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authorize",
    "context": {
        "trace_id": "0x6aaaaf0ddeab0f0f79339bad5a69ee31",
        "span_id": "0xf62e99e65fb442c6",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.854544Z",
    "end_time": "2025-10-10T21:30:36.854792Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "user.id": "user:alice",
        "auth.relation": "executor",
        "auth.resource": "tool:chat",
        "auth.authorized": true
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authorize",
    "context": {
        "trace_id": "0x26125d450859e684ae8d2a1fce4ce997",
        "span_id": "0x1cb9322325c13675",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.856088Z",
    "end_time": "2025-10-10T21:30:36.856278Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "user.id": "user:bob",
        "auth.relation": "admin",
        "auth.resource": "organization:acme",
        "auth.authorized": false
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authorize",
    "context": {
        "trace_id": "0x81292650cf017bd54536dd58ba33800a",
        "span_id": "0x08a7b62b9634ecb1",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.857498Z",
    "end_time": "2025-10-10T21:30:36.858397Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "user.id": "user:alice",
        "auth.relation": "executor",
        "auth.resource": "tool:chat"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authorize",
    "context": {
        "trace_id": "0x454a955a9fd53e48729309cfb6acfba0",
        "span_id": "0x90d201997d8f51c7",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.859383Z",
    "end_time": "2025-10-10T21:30:36.859473Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "user.id": "user:admin",
        "auth.relation": "executor",
        "auth.resource": "tool:chat"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authorize",
    "context": {
        "trace_id": "0xf736617492cad81f7a79c96827ebc6ab",
        "span_id": "0xdded75d7036a2821",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.860440Z",
    "end_time": "2025-10-10T21:30:36.860528Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "user.id": "user:alice",
        "auth.relation": "executor",
        "auth.resource": "tool:chat"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authorize",
    "context": {
        "trace_id": "0x39b5a9574422597812f2accf6bb554d7",
        "span_id": "0xac7edfde100f91f9",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.861457Z",
    "end_time": "2025-10-10T21:30:36.861544Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "user.id": "user:bob",
        "auth.relation": "executor",
        "auth.resource": "tool:chat"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authorize",
    "context": {
        "trace_id": "0x91c21c06272ba9e47f33da98dbea8ad7",
        "span_id": "0x4cd30404576ed122",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.862480Z",
    "end_time": "2025-10-10T21:30:36.862565Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "user.id": "user:alice",
        "auth.relation": "viewer",
        "auth.resource": "conversation:123"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authorize",
    "context": {
        "trace_id": "0x7a6aea8693f28c26772b56c364d21327",
        "span_id": "0x37765f03735c4cdb",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.865070Z",
    "end_time": "2025-10-10T21:30:36.865162Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "user.id": "user:unknown",
        "auth.relation": "executor",
        "auth.resource": "tool:chat"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authenticate",
    "context": {
        "trace_id": "0x0354192b61aba87835db30b4b0bd0b38",
        "span_id": "0x98206a0bedef686d",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.877944Z",
    "end_time": "2025-10-10T21:30:36.877975Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "auth.username": "alice"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authenticate",
    "context": {
        "trace_id": "0x0efdeb6022b36093e39e7010a858c7d0",
        "span_id": "0x1a4f99f9e041b4ae",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.880175Z",
    "end_time": "2025-10-10T21:30:36.880269Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "auth.username": "nonexistent"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authenticate",
    "context": {
        "trace_id": "0x94f919c2a2849164fb35d3203641c990",
        "span_id": "0x3bbda29a01f47d62",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.881627Z",
    "end_time": "2025-10-10T21:30:36.881654Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "auth.username": "alice"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authorize",
    "context": {
        "trace_id": "0x4b563a388f4e712dcbb02e63c5aa5168",
        "span_id": "0x49f1c55f1274bf34",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.881698Z",
    "end_time": "2025-10-10T21:30:36.881910Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "user.id": "user:alice",
        "auth.relation": "executor",
        "auth.resource": "tool:chat",
        "auth.authorized": true
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}
{
    "name": "auth.authenticate",
    "context": {
        "trace_id": "0xc67575430e65412794058716673dd4e8",
        "span_id": "0xb4749696046d8ede",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-10-10T21:30:36.883334Z",
    "end_time": "2025-10-10T21:30:36.883358Z",
    "status": {
        "status_code": "UNSET"
    },
    "attributes": {
        "auth.username": "bob"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.37.0",
            "service.name": "mcp-server-langgraph",
            "service.version": "1.0.0",
            "deployment.environment": "production"
        },
        "schema_url": ""
    }
}

FAILED tests/test_agent.py::TestAgentGraph::test_agent_with_conversation_history - RuntimeError: All models failed including fallbacks
During task with name 'respond' and id 'de60f177-ab08-8f20-1ada-e2440adbfd9d'
FAILED tests/test_agent.py::TestAgentGraph::test_checkpointing_works - RuntimeError: All models failed including fallbacks
During task with name 'respond' and id 'f0e2944c-7bcf-6775-a564-8a8b241bd59a'
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_server_info_endpoint - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_health_endpoint - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_initialize_method - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_tools_list_method - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_tools_call_method - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_tools_call_unauthorized - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_streaming_response - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_resources_list_method - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_invalid_method - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_malformed_request - RuntimeError: no running event loop
FAILED tests/test_mcp_streamable.py::TestMCPStreamableHTTP::test_cors_headers - RuntimeError: no running event loop
FAILED tests/test_secrets_manager.py::TestSecretsManager::test_clear_cache - AttributeError: 'SecretsManager' object has no attribute 'clear_cache'
================== 17 failed, 69 passed, 4 skipped in 15.25s ===================
Failed to export metrics to localhost:4317, error code: StatusCode.UNAVAILABLE
Transient error StatusCode.UNAVAILABLE encountered while exporting metrics to localhost:4317, retrying in 0.91s.
Failed to export traces to localhost:4317, error code: StatusCode.UNAVAILABLE
Transient error StatusCode.UNAVAILABLE encountered while exporting metrics to localhost:4317, retrying in 2.00s.
Failed to export metrics to localhost:4317, error code: StatusCode.UNAVAILABLE
Exception while exporting metrics
Traceback (most recent call last):
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 550, in _receive_metrics
    self._exporter.export(
    ~~~~~~~~~~~~~~~~~~~~~^
        metrics_data, timeout_millis=timeout_millis
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/codey/Projects/langgraph_mcp_agent/.venv/lib/python3.13/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 170, in export
    self.out.write(self.formatter(metrics_data))
    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: I/O operation on closed file.
Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 3.79s.
Failed to export traces to localhost:4317, error code: StatusCode.UNAVAILABLE
