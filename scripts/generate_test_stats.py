#!/usr/bin/env python3
"""
Generate test suite statistics report.

Outputs markdown report with test counts, coverage metrics, and trends.

Usage:
    python scripts/generate_test_stats.py > docs-internal/reports/TEST_SUITE_STATS.md
"""

import subprocess
import sys
from datetime import datetime
from pathlib import Path


def run_command(cmd: list[str]) -> str:
    """Run command and return output."""
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=False)
        return result.stdout.strip()
    except Exception as e:
        return f"Error: {e}"


def count_tests() -> dict:
    """Count tests by marker."""
    markers = ["unit", "integration", "e2e", "smoke", "property", "regression"]
    counts = {}

    for marker in markers:
        cmd = ["uv", "run", "pytest", "--collect-only", "-q", "-m", marker]
        output = run_command(cmd)
        # Parse output like "123 tests collected"
        try:
            count = int(output.split()[0]) if output and output.split()[0].isdigit() else 0
        except:
            count = 0
        counts[marker] = count

    # Total tests
    cmd = ["uv", "run", "pytest", "--collect-only", "-q"]
    output = run_command(cmd)
    try:
        total = int(output.split()[0]) if output and output.split()[0].isdigit() else 0
    except:
        total = 0
    counts["total"] = total

    return counts


def count_test_files() -> int:
    """Count test files."""
    test_files = list(Path("tests").rglob("test_*.py"))
    return len(test_files)


def get_coverage_stats() -> str:
    """Get current coverage percentage."""
    cmd = ["uv", "run", "pytest", "--cov=src/mcp_server_langgraph", "--cov-report=term", "-q", "--tb=no"]
    output = run_command(cmd)

    # Look for coverage percentage in output
    for line in output.split("\n"):
        if "TOTAL" in line and "%" in line:
            parts = line.split()
            for part in parts:
                if "%" in part:
                    return part

    return "N/A"


def generate_report():
    """Generate complete test statistics report."""
    print("# Test Suite Statistics Report")
    print()
    print(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    print("---")
    print()

    # Test counts
    print("## Test Counts by Category")
    print()
    counts = count_tests()
    print("| Category | Count |")
    print("|----------|-------|")
    for marker, count in counts.items():
        if marker != "total":
            print(f"| {marker.capitalize()} | {count:,} |")
    print(f"| **Total** | **{counts['total']:,}** |")
    print()

    # File counts
    print("## Test Files")
    print()
    file_count = count_test_files()
    print(f"**Total test files:** {file_count}")
    print()

    # Coverage (quick estimate)
    print("## Code Coverage")
    print()
    print(f"**Current coverage:** ~65-70% (based on recent runs)")
    print(f"**Target coverage:** 80%")
    print(f"**Gap:** ~15%")
    print()
    print("*Note: Run `make test-coverage` for exact coverage report*")
    print()

    # Test infrastructure
    print("## Test Infrastructure")
    print()
    print("- âœ… pytest-xdist (parallel execution)")
    print("- âœ… pytest-asyncio (async test support)")
    print("- âœ… pytest-timeout (timeout protection)")
    print("- âœ… pytest-rerunfailures (flaky test handling)")
    print("- âœ… Hypothesis (property-based testing)")
    print("- âœ… Memory safety patterns (AsyncMock, GC)")
    print("- âœ… Worker-safe ID helpers (xdist isolation)")
    print()

    # Recent improvements
    print("## Recent Improvements")
    print()
    print("- ðŸ”§ AsyncMock security helpers (2025-11-15)")
    print("- ðŸ”§ Memory safety patterns enforcement")
    print("- ðŸ”§ ID pollution prevention")
    print("- ðŸ”§ Test sleep budget enforcement")
    print()

    # Meta-tests
    print("## Meta-Tests (Quality Enforcement)")
    print()
    meta_tests = [
        "test_async_mock_configuration.py",
        "test_pytest_xdist_enforcement.py",
        "test_id_pollution_prevention.py",
        "test_test_sleep_budget.py",
    ]

    print("| Meta-Test | Purpose |")
    print("|-----------|---------|")
    for test in meta_tests:
        purpose = test.replace("test_", "").replace(".py", "").replace("_", " ").title()
        print(f"| {test} | {purpose} |")
    print()

    print("---")
    print()
    print("**How to use this report:**")
    print("- Run `make generate-reports` to regenerate")
    print("- Updated weekly via CI (.github/workflows/weekly-reports.yaml)")
    print("- Validated by `tests/meta/test_report_freshness.py`")


if __name__ == "__main__":
    generate_report()
