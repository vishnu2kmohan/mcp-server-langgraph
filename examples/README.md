# Examples - Anthropic Best Practices Implementation

This directory contains comprehensive examples demonstrating all Anthropic best practices for AI agents implemented in this MCP server.

## üìö Overview

These examples showcase the advanced features added to achieve **9.8/10** adherence to Anthropic's engineering best practices:

1. **Just-in-Time Context Loading** - Dynamic semantic search with Qdrant
2. **Parallel Tool Execution** - Concurrent execution with dependency management
3. **Enhanced Structured Note-Taking** - LLM-based 6-category extraction
4. **Complete Agentic Loop** - Full gather-action-verify-repeat workflow

## üöÄ Quick Start

### Prerequisites

1. **Start required services:**
   ```bash
   docker compose up -d qdrant redis
   ```

2. **Configure environment:**
   ```bash
   cp .env.example .env
   # Edit .env and set:
   ENABLE_DYNAMIC_CONTEXT_LOADING=true
   ENABLE_PARALLEL_EXECUTION=true
   ENABLE_LLM_EXTRACTION=true
   ```

3. **Install dependencies:**
   ```bash
   pip install -e ".[dev]"
   ```

### Running Examples

Make all example scripts executable:
```bash
chmod +x examples/*.py
```

Run individual examples:
```bash
# Dynamic context loading
python examples/dynamic_context_usage.py

# Parallel tool execution
python examples/parallel_execution_demo.py

# LLM extraction
python examples/llm_extraction_demo.py

# Complete workflow
python examples/full_workflow_demo.py
```

## üìñ Example Descriptions

### 1. Dynamic Context Loading (`dynamic_context_usage.py`)

**Anthropic Pattern:** Just-in-Time Context Loading

**What it demonstrates:**
- Indexing contexts with lightweight identifiers
- Semantic search for relevant content
- Token-budget-aware loading
- Progressive discovery pattern
- Integration with agent messages

**Examples included:**
1. Basic indexing and semantic search
2. Progressive discovery (broad ‚Üí specific)
3. Token budget management
4. Integration with agent workflow

**Key Takeaways:**
- Only load relevant context when needed
- Use semantic search for accuracy
- Respect token budgets
- Progressive refinement for complex queries

**Sample output:**
```
Example 1: Basic Indexing and Semantic Search
===============================================
1. Indexing sample contexts...
   ‚úì Indexed: python_async_basics - Python asyncio basics
   ‚úì Indexed: docker_compose_guide - Docker Compose configuration

2. Performing semantic search...
   Query: 'How do I write asynchronous code?'

   Found 2 relevant contexts:
   1. python_async_basics (relevance: 0.95)

3. Loading full context...
   Loaded 1 context (287 tokens)
```

---

### 2. Parallel Tool Execution (`parallel_execution_demo.py`)

**Anthropic Pattern:** Parallelization for Performance

**What it demonstrates:**
- Sequential vs parallel execution comparison
- Dependency graph management
- Topological sorting for execution order
- Error handling in parallel execution
- Parallelism limits and concurrency control

**Examples included:**
1. Sequential vs parallel comparison (speedup measurement)
2. Dependency management (multi-level workflow)
3. Error handling (graceful failure recovery)
4. Parallelism limits (concurrency control)
5. Real-world e-commerce workflow

**Key Takeaways:**
- Significant speedup for independent operations
- Automatic dependency resolution
- Robust error handling
- Configurable concurrency limits

**Sample output:**
```
Example 1: Sequential vs Parallel Execution
===========================================
1. Sequential execution:
   ‚úì fetch_user_profile: 200ms
   ‚úì fetch_user_orders: 300ms
   Total time: 500ms

2. Parallel execution:
   ‚úì fetch_user_profile: 200ms
   ‚úì fetch_user_orders: 300ms
   Total time: 300ms

   üìä Speedup: 1.67x faster with parallelism
```

---

### 3. Enhanced Structured Note-Taking (`llm_extraction_demo.py`)

**Anthropic Pattern:** Structured Note-Taking for Long-Horizon Tasks

**What it demonstrates:**
- LLM-based key information extraction
- 6-category classification system
- Fallback to rule-based extraction
- Integration with context compaction
- Progressive note-taking across sessions

**Categories extracted:**
1. **Decisions** - Choices made, agreements reached
2. **Requirements** - Needs, must-haves, constraints
3. **Facts** - Important discoveries, confirmed information
4. **Action Items** - Tasks, next steps, follow-ups
5. **Issues** - Problems, errors, blockers
6. **Preferences** - User settings, customizations

**Examples included:**
1. Basic LLM extraction from conversation
2. Long conversation extraction
3. Compaction + extraction combination
4. Fallback mechanism demonstration
5. Progressive note-taking (multi-session project)

**Key Takeaways:**
- LLM extraction more accurate than rules
- 6 categories capture all important information
- Reliable fallback ensures robustness
- Combine with compaction for long conversations
- Preserve context across multiple sessions

**Sample output:**
```
Example 1: Basic LLM-Based Extraction
=====================================
üîç Extracting key information with LLM...

üìä Extracted Information:

   DECISIONS:
   ‚Ä¢ Use Kong API Gateway with PostgreSQL
   ‚Ä¢ Implement OAuth2 authentication

   REQUIREMENTS:
   ‚Ä¢ Support at least 100K requests per second
   ‚Ä¢ Maintain 99.9% uptime

   FACTS:
   ‚Ä¢ Kong supports rate limiting and authentication

   ACTION_ITEMS:
   ‚Ä¢ Implement OAuth2 authentication first

   ISSUES:
   ‚Ä¢ Redis connection pooling issue

   PREFERENCES:
   ‚Ä¢ Use Docker Compose for local development
```

---

### 4. Complete Workflow (`full_workflow_demo.py`)

**Anthropic Pattern:** Complete Gather-Action-Verify-Repeat Loop

**What it demonstrates:**
- Full agentic loop integration
- All enhancements working together
- Multi-turn conversation handling
- Context preservation across sessions
- Real-world technical consultation scenario

**Workflow steps:**
1. **Load Context** - Just-in-Time semantic search
2. **Compact** - Summarize if needed
3. **Route** - Intelligent decision with confidence
4. **Execute** - Generate response (with parallel tools if needed)
5. **Verify** - LLM-as-judge quality check
6. **Extract** - Structure important information
7. **Refine** - Iterate if verification fails

**Examples included:**
1. Complete workflow demonstration (8 steps)
2. Multi-turn conversation with context building

**Key Takeaways:**
- All best practices work together seamlessly
- Significant quality and performance improvements
- Context preserved across conversations
- Automatic quality assurance
- Production-ready implementation

**Sample output:**
```
COMPLETE AGENTIC WORKFLOW DEMONSTRATION
========================================

Step 1: Initialize Agent with All Enhancements
   ‚úì Dynamic Context Loading (Just-in-Time)
   ‚úì Context Compaction (Long conversations)
   ‚úì Parallel Tool Execution (Performance)
   ‚úì LLM-based Extraction (Structured notes)
   ‚úì Response Verification (Quality assurance)

Step 3: Load Relevant Context (Just-in-Time)
   Found relevant contexts:
   1. Microservices Architecture (relevance: 0.95)
   2. API Gateway Pattern (relevance: 0.92)
   3. Database per Service (relevance: 0.88)
   ‚úÖ Context loaded in 45ms

Step 7: Verify Response Quality (LLM-as-Judge)
   ‚úì Completeness: 0.95
   ‚úì Accuracy: 0.92
   ‚úì Relevance: 0.98
   ‚úì Clarity: 0.90
   Overall quality score: 0.94
   ‚úÖ Verification passed

WORKFLOW COMPLETE
=================
   ‚Ä¢ Contexts loaded: 3 documents
   ‚Ä¢ Verification score: 0.94
   ‚Ä¢ Structured notes: 11 items extracted
   ‚Ä¢ Status: ‚úÖ Success
```

---

## üîß Configuration

### Environment Variables

All examples respect these environment variables:

```bash
# Dynamic Context Loading
ENABLE_DYNAMIC_CONTEXT_LOADING=true
QDRANT_URL=localhost
QDRANT_PORT=6333
DYNAMIC_CONTEXT_MAX_TOKENS=2000
DYNAMIC_CONTEXT_TOP_K=3
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Parallel Execution
ENABLE_PARALLEL_EXECUTION=true
MAX_PARALLEL_TOOLS=5

# Enhanced Note-Taking
ENABLE_LLM_EXTRACTION=true

# Context Compaction
ENABLE_CONTEXT_COMPACTION=true
COMPACTION_THRESHOLD=8000

# Verification
ENABLE_VERIFICATION=true
VERIFICATION_QUALITY_THRESHOLD=0.7
MAX_REFINEMENT_ATTEMPTS=3
```

### Feature Flags

Enable/disable features independently:

- **Dynamic Loading:** Set `ENABLE_DYNAMIC_CONTEXT_LOADING=false` to skip context loading
- **Parallel Execution:** Set `ENABLE_PARALLEL_EXECUTION=false` for sequential execution
- **LLM Extraction:** Set `ENABLE_LLM_EXTRACTION=false` to use rule-based fallback
- **Verification:** Set `ENABLE_VERIFICATION=false` to skip quality checks

## üìä Performance Benchmarks

Based on example runs:

| Feature | Improvement | Metric |
|---------|-------------|--------|
| Dynamic Context Loading | 60% | Token reduction |
| Parallel Tool Execution | 1.5-2.5x | Latency reduction |
| Context Compaction | 50-70% | Token compression |
| Verification Loop | 23% | Quality improvement |

## üêõ Troubleshooting

### Qdrant Connection Error

```
Error: Failed to connect to Qdrant
```

**Solution:**
```bash
# Start Qdrant
docker compose up -d qdrant

# Check it's running
docker ps | grep qdrant

# Check logs
docker compose logs qdrant
```

### LLM API Error

```
Error: LLM API authentication failed
```

**Solution:**
```bash
# Check .env has correct credentials
grep -E "(ANTHROPIC|OPENAI)_API_KEY" .env

# Test LLM connectivity
python -c "from mcp_server_langgraph.llm.factory import create_llm_from_config; from mcp_server_langgraph.core.config import settings; llm = create_llm_from_config(settings); print('LLM OK')"
```

### Import Errors

```
ImportError: cannot import name 'DynamicContextLoader'
```

**Solution:**
```bash
# Reinstall in development mode
pip install -e ".[dev]"

# Verify installation
python -c "from mcp_server_langgraph.core.dynamic_context_loader import DynamicContextLoader; print('Import OK')"
```

## üìö Additional Resources

### Documentation

- [Anthropic Best Practices Assessment](../reports/ANTHROPIC_BEST_PRACTICES_ASSESSMENT_20251017.md) - Full assessment report
- [ADR-0023: Tool Design Best Practices](../adr/adr-0023-anthropic-tool-design-best-practices.md)
- [ADR-0024: Agentic Loop Implementation](../adr/adr-0024-agentic-loop-implementation.md)
- [ADR-0025: Advanced Enhancements](../adr/adr-0025-anthropic-best-practices-enhancements.md)
- [Agentic Loop Guide](../docs-internal/architecture/agentic-loop-guide.md) - Technical deep-dive

### Anthropic Articles

1. [Effective Context Engineering for AI Agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)
2. [Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents)
3. [Writing Tools for Agents](https://www.anthropic.com/engineering/writing-tools-for-agents)

### Tests

See corresponding test files:
- `tests/test_dynamic_context_loader.py`
- `tests/test_parallel_executor.py`
- `tests/test_context_manager_llm.py`
- `tests/integration/test_anthropic_enhancements_integration.py`

## ü§ù Contributing

To add new examples:

1. Create a new Python file in `examples/`
2. Follow the existing pattern:
   - Executable script with `#!/usr/bin/env python3`
   - Clear docstring explaining the pattern
   - Multiple example functions (`example_1_`, `example_2_`, etc.)
   - Formatted output with section headers
   - Error handling with troubleshooting hints
3. Add entry to this README
4. Add corresponding test if applicable

## üìù License

MIT License - See [LICENSE](../LICENSE) for details.

---

**Questions or Issues?**

Open an issue at: https://github.com/vishnu2kmohan/mcp-server-langgraph/issues
