============================= test session starts ==============================
platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /home/vishnu/git/vishnu2kmohan/worktrees/mcp-server-langgraph-session-20251118-221044/.venv/bin/python
cachedir: .pytest_cache
hypothesis profile 'dev' -> max_examples=25, deadline=timedelta(milliseconds=2000)
benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/vishnu/git/vishnu2kmohan/worktrees/mcp-server-langgraph-session-20251118-221044
configfile: pyproject.toml
testpaths: tests
plugins: hypothesis-6.148.2, langsmith-0.4.37, anyio-4.11.0, split-0.10.0, subtests-0.14.2, timeout-2.4.0, xdist-3.8.0, logfire-4.14.2, schemathesis-4.3.8, cov-7.0.0, mock-3.15.1, testmon-2.1.3, asyncio-1.2.0, benchmark-5.1.0
timeout: 60.0s
timeout method: signal
timeout func_only: False
asyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=session, asyncio_default_test_loop_scope=function
collecting ... collected 5174 items / 2 errors / 902 deselected / 4272 selected

<Dir mcp-server-langgraph-session-20251118-221044>
  <Package tests>
    <Package api>
      <Module test_api_versioning.py>
        Tests for API Versioning Strategy

        Validates that the API follows semantic versioning best practices and provides
        version negotiation capabilities to prevent breaking changes.

        Following TDD: These tests define the expected versioning behavior (RED phase).
        <Class TestAPIVersionMetadata>
          Test API version information endpoints
          <Function test_version_endpoint_exists>
            API should expose version metadata at /api/version
          <Function test_version_metadata_structure>
            Version metadata should follow standard structure
          <Function test_openapi_version_consistency>
            OpenAPI schema version should match /api/version
        <Class TestAPIVersionPrefixes>
          Test that all REST API endpoints use consistent /api/v1 prefix
          <Function test_gdpr_endpoints_have_v1_prefix>
            GDPR endpoints should be under /api/v1
          <Function test_api_keys_endpoints_have_v1_prefix>
            API Keys endpoints should be under /api/v1
          <Function test_service_principals_endpoints_have_v1_prefix>
            Service Principals endpoints should be under /api/v1
          <Function test_auth_endpoints_have_version_prefix>
            Auth endpoints should be versioned (except /health)
        <Class TestVersionNegotiation>
          Test API version negotiation via headers
          <Function test_version_header_accepted>
            API should accept X-API-Version header
          <Function test_unsupported_version_returns_error>
            Requesting unsupported version should return clear error or be ignored
        <Class TestDeprecationSupport>
          Test that deprecated endpoints are properly marked
          <Function test_deprecated_endpoints_marked_in_openapi>
            Deprecated endpoints should have 'deprecated: true' in OpenAPI
          <Function test_deprecated_fields_documented>
            Deprecated fields in request/response models should be marked
        <Class TestBackwardCompatibility>
          Test backward compatibility guarantees
          <Function test_all_v1_endpoints_stable>
            All /api/v1 endpoints are considered stable (no breaking changes)
          <Function test_response_schema_extensions_allowed>
            Response schemas can add new optional fields without breaking changes
        <Class TestVersionDocumentation>
          Test that versioning strategy is documented in OpenAPI
          <Function test_openapi_version_field_present>
            OpenAPI schema should have version in info section
          <Function test_openapi_has_description>
            OpenAPI description should explain versioning strategy
          <Function test_servers_include_version_info>
            OpenAPI servers should include version information
      <Module test_bearer_scheme_diagnostic.py>
        Diagnostic Test for Bearer Scheme Override Issues

        Investigates why bearer_scheme override is not preventing 401 errors in
        service principal tests during pytest-xdist execution.

        Run with: pytest tests/api/test_bearer_scheme_diagnostic.py -v -s
        <Class TestBearerSchemeOverrideDiagnostic>
          Diagnostic tests to understand bearer_scheme override behavior.
          <Function test_bearer_scheme_identity_check>
            Diagnostic: Check if imported bearer_scheme matches module bearer_scheme.

            This test helps us understand if the bearer_scheme object being overridden
            is the same object used by get_current_user.
          <Function test_bearer_override_with_direct_import>
            Diagnostic: Test if overriding imported bearer_scheme works.

            Tests the current pattern used in service principal tests.
          <Function test_bearer_override_with_module_reference>
            Diagnostic: Test if overriding via module reference works better.

            Alternative approach: Use module.bearer_scheme instead of imported name.
          <Function test_get_current_user_override_without_bearer_override>
            Diagnostic: Test if we can skip bearer_scheme override entirely.

            If we override get_current_user, do we even need to override bearer_scheme?
          <Function test_actual_service_principal_router_with_diagnostic>
            Diagnostic: Test actual service principals router with detailed logging.

            This mimics the actual test setup to identify where the override breaks.
      <Module test_error_handlers.py>
        Tests for FastAPI exception handlers.

        Tests cover:
        - MCP exception handler registration
        - Custom exception handling with proper status codes
        - Generic exception wrapping
        - Error response formatting
        - Trace ID headers
        - Retry-After headers for rate limits
        - Metrics emission on errors
        <Function test_register_exception_handlers_adds_handlers_to_app>
          Test register_exception_handlers() registers exception handlers successfully.
        <Function test_mcp_exception_handler_returns_correct_status_code>
          Test MCP exception handler returns correct HTTP status code.
        <Function test_mcp_exception_handler_includes_trace_id_in_response>
          Test MCP exception handler includes trace_id in response (auto-generated if not provided).
        <Function test_mcp_exception_handler_returns_structured_error_response>
          Test MCP exception handler returns structured JSON error.
        <Function test_mcp_exception_handler_includes_user_friendly_message>
          Test MCP exception handler includes user-friendly message.
        <Function test_rate_limit_exception_includes_retry_after_header>
          Test rate limit exceptions include Retry-After header.
        <Function test_mcp_exception_handler_emits_metrics>
          Test MCP exception handler emits error metrics.
        <Function test_mcp_exception_handler_continues_if_metrics_fail>
          Test exception handler continues gracefully if metrics emission fails.
        <Function test_create_error_response_with_minimal_params_returns_structured_response>
          Test create_error_response() with only required parameters.
        <Function test_create_error_response_with_all_params_includes_all_fields>
          Test create_error_response() with all parameters.
        <Function test_create_error_response_with_custom_status_code>
          Test create_error_response() respects custom status code.
        <Function test_create_error_response_handles_empty_metadata>
          Test create_error_response() handles None metadata gracefully.
        <Function test_create_error_response_preserves_complex_metadata>
          Test create_error_response() preserves nested metadata structures.
        <Function test_error_response_format_matches_api_spec>
          Test error responses match OpenAPI specification format.
        <Function test_multiple_exception_types_handled_correctly>
          Test different exception types return appropriate status codes.
        <Class TestErrorHandlerEdgeCases>
          P2: Test error handler edge cases and complex scenarios
          <Function test_error_response_with_complex_nested_metadata>
            Test that error responses handle complex nested metadata correctly
          <Function test_error_response_with_very_long_message>
            Test that error responses handle very long error messages (>1000 chars)
          <Function test_register_exception_handlers_logs_registration>
            Test that register_exception_handlers logs successful registration
          <Function test_error_response_with_trace_id_set>
            Test error response when trace_id is explicitly set
    <Package builder>
      <Package api>
        <Module test_server.py>
          Tests for Visual Workflow Builder API Server

          Comprehensive test suite for all builder API endpoints following TDD principles.

          Tests cover:
          - POST /api/builder/generate - Code generation from workflows
          - POST /api/builder/validate - Workflow validation
          - POST /api/builder/save - Save workflow to file
          - GET /api/builder/templates - List templates
          - GET /api/builder/templates/{id} - Get specific template
          - POST /api/builder/import - Import Python code
          - GET /api/builder/node-types - List node types
          - GET / - API information
          <Function test_root_endpoint_returns_api_information>
            Test GET / returns API information.
          <Function test_generate_code_with_valid_workflow_returns_formatted_code>
            Test POST /api/builder/generate with valid workflow generates code.
          <Function test_generate_code_with_minimal_workflow_returns_warnings>
            Test POST /api/builder/generate with minimal workflow returns warnings.
          <Function test_generate_code_with_invalid_workflow_returns_400>
            Test POST /api/builder/generate with invalid workflow returns 400.
          <Function test_generate_code_with_empty_request_returns_422>
            Test POST /api/builder/generate with empty request returns 422.
          <Function test_validate_workflow_with_valid_workflow_returns_valid_true>
            Test POST /api/builder/validate with valid workflow returns valid=True.
          <Function test_validate_workflow_with_empty_nodes_returns_error>
            Test POST /api/builder/validate with empty nodes returns validation error.
          <Function test_validate_workflow_with_invalid_entry_point_returns_error>
            Test POST /api/builder/validate with invalid entry point returns error.
          <Function test_validate_workflow_with_invalid_edge_source_returns_error>
            Test POST /api/builder/validate with invalid edge source returns error.
          <Function test_validate_workflow_with_invalid_edge_target_returns_error>
            Test POST /api/builder/validate with invalid edge target returns error.
          <Function test_validate_workflow_with_unreachable_nodes_returns_warning>
            Test POST /api/builder/validate with unreachable nodes returns warning.
          <Function test_validate_workflow_with_many_terminal_nodes_returns_warning>
            Test POST /api/builder/validate with many terminal nodes returns warning.
          <Function test_validate_workflow_with_invalid_structure_returns_error_not_exception>
            Test POST /api/builder/validate handles exceptions gracefully.
          <Function test_save_workflow_with_valid_workflow_saves_to_file>
            Test POST /api/builder/save saves workflow to file.
          <Function test_save_workflow_with_io_error_returns_500>
            Test POST /api/builder/save handles IO errors.
          <Function test_list_templates_returns_template_list>
            Test GET /api/builder/templates returns list of templates.
          <Function test_get_template_with_research_agent_returns_workflow>
            Test GET /api/builder/templates/research_agent returns workflow.
          <Function test_get_template_with_unknown_id_returns_404>
            Test GET /api/builder/templates/{unknown} returns 404.
          <Function test_import_workflow_with_valid_code_returns_workflow>
            Test POST /api/builder/import with valid Python code returns workflow.
          <Function test_import_workflow_with_syntax_error_returns_400>
            Test POST /api/builder/import with invalid Python syntax returns 400.
          <Function test_import_workflow_with_import_error_returns_500>
            Test POST /api/builder/import with import error returns 500.
          <Function test_list_node_types_returns_all_node_types>
            Test GET /api/builder/node-types returns all available node types.
          <Function test_node_types_tool_has_correct_schema>
            Test GET /api/builder/node-types returns correct schema for tool node.
          <Function test_node_types_llm_has_correct_schema>
            Test GET /api/builder/node-types returns correct schema for LLM node.
          <Function test_generate_and_validate_workflow_integration>
            Test integration: validate workflow then generate code.
          <Function test_get_template_and_generate_code_integration>
            Test integration: get template then generate code from it.
          <Function test_generate_code_with_special_characters_in_node_ids>
            Test POST /api/builder/generate handles special characters in node IDs.
          <Function test_validate_workflow_with_circular_dependencies>
            Test POST /api/builder/validate handles circular dependencies.
          <Function test_cors_headers_are_present>
            Test that CORS headers are configured correctly.
      <Module test_builder_security.py>
        Security tests for Visual Workflow Builder API

        Tests for:
        - CWE-73: External Control of File Name or Path
        - CWE-434: Unrestricted Upload of File with Dangerous Type
        - OWASP A01:2021 - Broken Access Control
        - OWASP A05:2021 - Security Misconfiguration

        These tests validate authentication and path traversal protections.
        <Class TestBuilderAuthentication>
          Test that builder endpoints require authentication
          <Function test_save_workflow_requires_authentication>
            SECURITY TEST: /api/builder/save should require authentication.

            CWE: Missing Authentication for Critical Function
            OWASP A01:2021 - Broken Access Control
          <Function test_generate_code_requires_authentication>
            SECURITY TEST: /api/builder/generate should require authentication.
          <Function test_import_workflow_requires_authentication>
            SECURITY TEST: /api/builder/import should require authentication.
        <Class TestPathTraversalProtection>
          Test protection against path traversal attacks
          <Function test_prevent_path_traversal_absolute_path>
            SECURITY TEST: Prevent writing to arbitrary absolute paths.

            CWE-73: External Control of File Name or Path
          <Function test_prevent_path_traversal_relative_path>
            SECURITY TEST: Prevent directory traversal with ../
          <Function test_prevent_overwriting_application_code>
            SECURITY TEST: Prevent overwriting application source code.
          <Coroutine test_allow_safe_paths_only>
            Only paths within a designated safe directory should be allowed.
        <Class TestCodeGenerationSecurity>
          Test code generation security
          <Function test_sanitize_generated_code>
            Ensure generated code doesn't contain malicious injections.
          <Function test_validate_workflow_before_generation>
            Workflow should be validated before code generation.
        <Class TestBuilderProductionSafety>
          Test that builder is disabled or secured in production
          <Function test_builder_disabled_in_production>
            SECURITY TEST: Builder should be disabled in production by default.
          <Function test_builder_requires_feature_flag>
            Builder should require explicit feature flag to enable in production.
      <Module test_code_generator.py>
        Tests for Code Generator Module

        Comprehensive test suite for the CodeGenerator class that generates
        production-ready Python code from visual workflows.

        Tests cover:
        - State field generation
        - Node function generation for all node types (tool, llm, conditional, approval, custom)
        - Routing function generation for conditional edges
        - Graph construction code generation
        - Black code formatting
        - File writing
        - Edge cases and error handling
        <Function test_workflow_definition_creates_instance_with_valid_data>
          Test WorkflowDefinition creates instance with valid data.
        <Function test_node_definition_with_defaults_creates_instance>
          Test NodeDefinition uses default values correctly.
        <Function test_edge_definition_with_alias_from_creates_instance>
          Test EdgeDefinition accepts 'from' as alias for 'from_node'.
        <Function test_edge_definition_with_condition_stores_condition>
          Test EdgeDefinition stores optional condition.
        <Function test_generate_state_fields_with_custom_schema_returns_formatted_fields>
          Test _generate_state_fields with custom schema returns formatted fields.
        <Function test_generate_state_fields_with_empty_schema_returns_default_fields>
          Test _generate_state_fields with empty schema returns default fields.
        <Function test_generate_node_function_for_tool_node_includes_tool_call>
          Test _generate_node_function for tool node includes tool call logic.
        <Function test_generate_node_function_for_llm_node_includes_completion_call>
          Test _generate_node_function for LLM node includes completion call.
        <Function test_generate_node_function_for_conditional_node_includes_conditional_logic>
          Test _generate_node_function for conditional node includes conditional logic.
        <Function test_generate_node_function_for_approval_node_includes_approval_checkpoint>
          Test _generate_node_function for approval node includes ApprovalNode.
        <Function test_generate_node_function_for_custom_node_includes_todo_comment>
          Test _generate_node_function for custom node includes TODO comment.
        <Function test_generate_node_function_with_dashes_in_id_converts_to_underscores>
          Test _generate_node_function converts dashes in node ID to underscores.
        <Function test_generate_routing_function_with_no_edges_returns_none>
          Test _generate_routing_function with no edges returns None.
        <Function test_generate_routing_function_with_single_edge_returns_none>
          Test _generate_routing_function with single edge returns None.
        <Function test_generate_routing_function_with_conditional_edges_returns_routing_code>
          Test _generate_routing_function with conditional edges generates routing.
        <Function test_generate_routing_function_with_no_conditions_returns_none>
          Test _generate_routing_function with multiple edges but no conditions returns None.
        <Function test_generate_graph_construction_includes_all_nodes>
          Test _generate_graph_construction includes all nodes.
        <Function test_generate_graph_construction_includes_edges>
          Test _generate_graph_construction includes edges.
        <Function test_generate_graph_construction_sets_entry_point>
          Test _generate_graph_construction sets entry point.
        <Function test_generate_graph_construction_sets_finish_points_for_terminal_nodes>
          Test _generate_graph_construction sets finish points for terminal nodes.
        <Function test_generate_graph_construction_with_conditional_edges_uses_routing>
          Test _generate_graph_construction with conditional edges uses routing function.
        <Function test_generate_with_simple_workflow_returns_valid_python_code>
          Test generate() with simple workflow returns valid Python code.
        <Function test_generate_converts_workflow_name_to_class_name>
          Test generate() converts snake_case workflow name to PascalCase class name.
        <Function test_generate_includes_workflow_description_in_docstring>
          Test generate() includes workflow description in module docstring.
        <Function test_generate_formats_code_with_black>
          Test generate() formats code with Black.
        <Function test_generate_with_black_formatting_error_returns_unformatted_code>
          Test generate() returns unformatted code if Black formatting fails.
        <Function test_generate_with_no_routing_functions_includes_comment>
          Test generate() includes comment when no routing functions needed.
        <Function test_generate_to_file_writes_code_to_file>
          Test generate_to_file() writes generated code to file.
        <Function test_generate_to_file_with_invalid_path_raises_exception>
          Test generate_to_file() raises exception for invalid path.
        <Function test_full_workflow_generation_produces_executable_code>
          Test complete workflow generation produces syntactically valid Python.
        <Function test_round_trip_workflow_definition_to_code_and_back_preserves_structure>
          Test workflow can be converted to code (future: and back to workflow).
        <Function test_generate_with_empty_state_schema_uses_defaults>
          Test generate() with empty state_schema uses default fields.
        <Function test_generate_with_special_characters_in_field_names_escapes_correctly>
          Test generate() handles special characters in state field names.
      <Module test_importer.py>
        Tests for Code Importer Module

        Comprehensive test suite for the round-trip import/export functionality
        that enables Visual ↔ Code workflows.

        Tests cover:
        - Import Python code to workflow definition
        - Import from file
        - Workflow validation after import
        - Layout algorithms (hierarchical, force, grid)
        - Round-trip preservation (Code → Visual → Code)
        - Edge cases and error handling
        <Function test_import_from_code_with_valid_code_returns_workflow>
          Test import_from_code() with valid LangGraph code returns workflow definition.
        <Function test_import_from_code_extracts_node_ids>
          Test import_from_code() extracts correct node IDs.
        <Function test_import_from_code_extracts_edges>
          Test import_from_code() extracts edges correctly.
        <Function test_import_from_code_extracts_entry_point>
          Test import_from_code() extracts entry point.
        <Function test_import_from_code_adds_positions_to_nodes>
          Test import_from_code() adds position data to nodes for visual canvas.
        <Function test_import_from_code_with_hierarchical_layout_positions_nodes_vertically>
          Test import_from_code() with hierarchical layout creates top-to-bottom flow.
        <Function test_import_from_code_with_force_layout_spreads_nodes>
          Test import_from_code() with force layout creates distributed positions.
        <Function test_import_from_code_with_grid_layout_aligns_nodes>
          Test import_from_code() with grid layout creates aligned positions.
        <Function test_import_from_code_with_complex_workflow_handles_conditionals>
          Test import_from_code() handles conditional edges correctly.
        <Function test_import_from_code_extracts_state_schema>
          Test import_from_code() extracts state schema from TypedDict.
        <Function test_import_from_code_with_invalid_python_raises_syntax_error>
          Test import_from_code() with invalid Python raises SyntaxError.
        <Function test_import_from_code_with_empty_string_handles_gracefully>
          Test import_from_code() with empty string handles gracefully.
        <Function test_import_from_file_with_valid_file_returns_workflow>
          Test import_from_file() reads and imports from file.
        <Function test_import_from_file_with_layout_algorithm_applies_layout>
          Test import_from_file() with layout algorithm.
        <Function test_import_from_file_with_nonexistent_file_raises_file_not_found>
          Test import_from_file() with nonexistent file raises error.
        <Function test_import_from_file_with_invalid_code_raises_syntax_error>
          Test import_from_file() with invalid Python code raises error.
        <Function test_validate_import_with_valid_workflow_returns_valid_true>
          Test validate_import() with valid workflow returns valid=True.
        <Function test_validate_import_with_missing_name_returns_error>
          Test validate_import() with missing name field returns error.
        <Function test_validate_import_with_missing_nodes_returns_error>
          Test validate_import() with missing nodes returns error.
        <Function test_validate_import_with_missing_edges_returns_error>
          Test validate_import() with missing edges returns error.
        <Function test_validate_import_with_node_missing_position_returns_error>
          Test validate_import() with node missing position returns error.
        <Function test_validate_import_with_empty_state_schema_returns_warning>
          Test validate_import() with empty state schema returns warning.
        <Function test_validate_import_with_unknown_node_type_returns_warning>
          Test validate_import() with unknown node type returns warning.
        <Function test_round_trip_simple_workflow_preserves_structure>
          Test round-trip: Code → Import → Export → Code preserves workflow structure.
        <Function test_round_trip_workflow_with_conditionals_preserves_routing>
          Test round-trip with conditional edges preserves routing logic.
        <Function test_round_trip_preserves_node_count>
          Test round-trip preserves number of nodes.
        <Function test_round_trip_preserves_edge_count>
          Test round-trip preserves number of edges.
        <Function test_import_exported_workflow_matches_original_json>
          Test importing exported workflow matches original JSON structure.
        <Function test_import_validate_and_export_integration>
          Test complete flow: import → validate → export.
        <Function test_import_from_code_with_minimal_workflow>
          Test import_from_code() with minimal valid workflow.
        <Function test_import_handles_different_layout_algorithms>
          Test import_from_code() works with all layout algorithms.
        <Function test_validate_import_with_all_valid_node_types>
          Test validate_import() accepts all known node types.
      <Module test_workflow_builder.py>
        Tests for Workflow Builder API

        Comprehensive test suite for the WorkflowBuilder fluent API that provides
        a programmatic interface for building agent workflows.

        Tests cover:
        - Workflow initialization
        - Node addition with fluent interface
        - Edge addition with conditions
        - State schema management
        - Entry point configuration
        - Workflow building and validation
        - Code export functionality
        - File writing
        - JSON serialization/deserialization
        - Method chaining
        - Edge cases and error handling
        <Function test_workflow_builder_init_with_name_creates_instance>
          Test WorkflowBuilder initialization with name.
        <Function test_workflow_builder_init_with_description_stores_description>
          Test WorkflowBuilder initialization with description.
        <Function test_add_node_with_id_and_type_adds_node>
          Test add_node() adds node to workflow.
        <Function test_add_node_returns_self_for_chaining>
          Test add_node() returns self for method chaining.
        <Function test_add_node_with_label_stores_label>
          Test add_node() with label stores label.
        <Function test_add_node_without_label_uses_node_id_as_label>
          Test add_node() without label uses node_id as label.
        <Function test_add_node_sets_first_node_as_entry_point>
          Test add_node() sets first node as entry point automatically.
        <Function test_add_node_second_node_does_not_change_entry_point>
          Test add_node() does not change entry point for subsequent nodes.
        <Function test_add_node_with_no_config_uses_empty_dict>
          Test add_node() without config uses empty dict.
        <Function test_add_node_supports_method_chaining>
          Test add_node() supports fluent interface chaining.
        <Function test_add_edge_with_source_and_target_adds_edge>
          Test add_edge() adds edge to workflow.
        <Function test_add_edge_returns_self_for_chaining>
          Test add_edge() returns self for method chaining.
        <Function test_add_edge_with_condition_stores_condition>
          Test add_edge() with condition stores condition expression.
        <Function test_add_edge_with_label_stores_label>
          Test add_edge() with label stores label.
        <Function test_add_edge_supports_method_chaining>
          Test add_edge() supports fluent interface chaining.
        <Function test_set_entry_point_changes_entry_point>
          Test set_entry_point() changes the entry point.
        <Function test_set_entry_point_returns_self_for_chaining>
          Test set_entry_point() returns self for chaining.
        <Function test_add_state_field_with_name_and_type_adds_field>
          Test add_state_field() adds field to state schema.
        <Function test_add_state_field_with_complex_type_stores_type>
          Test add_state_field() with complex type annotation.
        <Function test_add_state_field_without_type_defaults_to_str>
          Test add_state_field() without type defaults to str.
        <Function test_add_state_field_returns_self_for_chaining>
          Test add_state_field() returns self for chaining.
        <Function test_add_state_field_supports_method_chaining>
          Test add_state_field() supports fluent interface.
        <Function test_build_with_valid_workflow_returns_workflow_definition>
          Test build() with valid workflow returns WorkflowDefinition.
        <Function test_build_with_no_nodes_raises_value_error>
          Test build() with no nodes raises ValueError.
        <Function test_build_with_invalid_entry_point_raises_value_error>
          Test build() with invalid entry point raises ValueError.
        <Function test_build_with_edge_to_nonexistent_source_raises_value_error>
          Test build() with edge from nonexistent node raises ValueError.
        <Function test_build_with_edge_to_nonexistent_target_raises_value_error>
          Test build() with edge to nonexistent node raises ValueError.
        <Function test_export_code_returns_python_code>
          Test export_code() returns valid Python code.
        <Function test_export_code_with_invalid_workflow_raises_value_error>
          Test export_code() with invalid workflow raises ValueError.
        <Function test_save_code_writes_code_to_file>
          Test save_code() writes generated code to file.
        <Function test_save_code_with_invalid_path_raises_exception>
          Test save_code() with invalid path raises exception.
        <Function test_to_json_returns_json_serializable_dict>
          Test to_json() returns JSON-serializable dictionary.
        <Function test_to_json_with_invalid_workflow_raises_value_error>
          Test to_json() with invalid workflow raises ValueError.
        <Function test_from_json_creates_builder_from_json_data>
          Test from_json() creates WorkflowBuilder from JSON data.
        <Function test_from_json_round_trip_preserves_data>
          Test from_json() round-trip preserves workflow data.
        <Function test_fluent_interface_chaining_builds_complete_workflow>
          Test fluent interface allows complete workflow construction via chaining.
        <Function test_build_export_save_workflow_integration>
          Test complete workflow: build -> export -> save.
        <Function test_json_export_and_code_generation_integration>
          Test workflow can be exported to JSON and code.
        <Function test_workflow_with_single_node_builds_successfully>
          Test workflow with single node builds successfully.
        <Function test_workflow_with_multiple_edges_from_same_node>
          Test workflow with multiple outgoing edges from one node.
        <Function test_workflow_with_complex_state_schema>
          Test workflow with complex state schema types.
        <Function test_workflow_with_no_edges_builds_successfully>
          Test workflow with no edges (single terminal node) builds.
        <Function test_builder_can_be_reused_after_build>
          Test builder can add more nodes/edges after build().
    <Package ci>
      <Module test_dora_metrics.py>
        Tests for DORA Metrics Tracking

        Tests the four key DORA metrics:
        1. Deployment Frequency
        2. Lead Time for Changes
        3. Mean Time to Recovery (MTTR)
        4. Change Failure Rate

        Following TDD practices - tests written before implementation.
        <Class TestDORAMetricsCalculation>
          Test DORA metrics calculation logic
          <Function test_calculate_deployment_frequency_daily>
            Test deployment frequency calculation for daily deployments
          <Function test_calculate_deployment_frequency_multiple_per_day>
            Test deployment frequency with multiple deployments per day
          <Function test_calculate_lead_time_from_commit_to_deploy>
            Test lead time calculation from first commit to deployment
          <Function test_calculate_mttr_from_incident_to_recovery>
            Test MTTR calculation from incident detection to recovery
          <Function test_calculate_change_failure_rate>
            Test change failure rate calculation
          <Function test_calculate_change_failure_rate_all_success>
            Test change failure rate with all successful deployments
          <Function test_classify_dora_performance_elite>
            Test DORA performance classification - Elite
          <Function test_classify_dora_performance_high>
            Test DORA performance classification - High
          <Function test_classify_dora_performance_medium>
            Test DORA performance classification - Medium
        <Class TestDORAMetricsCollection>
          Test DORA metrics data collection from GitHub API
          <Function test_collect_deployment_data_from_github>
            Test collecting deployment data from GitHub deployments API
          <Function test_collect_commit_data_from_github>
            Test collecting commit data from GitHub API
        <Class TestDORAMetricsStorage>
          Test DORA metrics storage and retrieval
          <Function test_save_metrics_to_json_file>
            Test saving DORA metrics to JSON file
          <Function test_load_historical_metrics>
            Test loading historical metrics for trending
        <Class TestDORAMetricsReporting>
          Test DORA metrics reporting and visualization
          <Function test_generate_markdown_report>
            Test generating markdown report for metrics
          <Function test_generate_trend_chart_data>
            Test generating trend chart data for visualization
      <Module test_performance_regression.py>
        Tests for Performance Regression Detection

        Tests automatic detection of performance regressions in:
        - API response times
        - Database query times
        - Memory usage
        - CPU usage

        Following TDD practices.
        <Class TestPerformanceBaseline>
          Test performance baseline establishment
          <Function test_establish_baseline_from_benchmarks>
            Test establishing performance baseline from benchmark results
          <Function test_load_historical_baseline>
            Test loading historical baseline from file
        <Class TestRegressionDetection>
          Test regression detection logic
          <Function test_detect_response_time_regression>
            Test detecting response time regression
          <Function test_no_regression_within_threshold>
            Test no regression detected when within threshold
          <Function test_detect_memory_regression>
            Test detecting memory usage regression
          <Function test_detect_multiple_regressions>
            Test detecting multiple regressions
        <Class TestPerformanceBenchmarking>
          Test performance benchmarking execution
          <Function test_run_api_benchmarks>
            Test running API endpoint benchmarks
          <Function test_calculate_percentiles>
            Test percentile calculation for response times
        <Class TestRegressionReporting>
          Test regression reporting and notifications
          <Function test_generate_regression_report>
            Test generating regression report
          <Function test_create_github_issue_for_regression>
            Test creating GitHub issue for regression
        <Class TestBaselineUpdate>
          Test baseline update logic
          <Function test_update_baseline_after_improvement>
            Test updating baseline after performance improvement
          <Function test_no_baseline_update_for_minor_changes>
            Test not updating baseline for minor changes
          <Function test_save_updated_baseline>
            Test saving updated baseline to file
      <Module test_track_costs.py>
        Test suite for GitHub Actions cost tracking script.

        Following TDD approach - tests written before implementation.
        Tests the cost analysis functionality for workflow runs.
        <Class TestLoadRuns>
          Test the load_runs() function that loads workflow runs from JSON files.
          <Function test_load_runs_from_single_file>
            Test loading workflow runs from a single JSON file.
          <Function test_load_runs_from_multiple_files>
            Test loading workflow runs from page 1 and page 2 JSON files.
          <Function test_load_runs_missing_files>
            Test load_runs() returns empty list when files don't exist.
          <Function test_load_runs_malformed_json>
            Test load_runs() handles malformed JSON gracefully.
          <Function test_load_runs_empty_workflow_runs_array>
            Test load_runs() with valid JSON but empty workflow_runs.
        <Class TestAnalyzeUsage>
          Test the analyze_usage() function that analyzes workflow costs.
          <Function test_analyze_usage_basic_calculation>
            Test basic cost calculation for workflow runs.
          <Function test_analyze_usage_weekly_budget_calculation>
            Test weekly budget calculation and status.
          <Function test_analyze_usage_monthly_budget_calculation>
            Test monthly budget calculation.
          <Function test_analyze_usage_over_budget>
            Test cost analysis when over budget.
          <Function test_analyze_usage_under_budget>
            Test cost analysis when well under budget.
          <Function test_analyze_usage_duration_rounding>
            Test billable minutes are rounded up correctly.
          <Function test_analyze_usage_workflow_grouping>
            Test workflows are grouped correctly in the report.
          <Function test_analyze_usage_high_duration_workflows>
            Test identification of high duration workflows.
          <Function test_analyze_usage_high_frequency_workflows>
            Test identification of high frequency workflows.
          <Function test_analyze_usage_empty_runs>
            Test analyze_usage() with no workflow runs.
          <Function test_analyze_usage_metrics_file_creation>
            Test that metrics are saved to cost-metrics.txt file.
          <Function test_analyze_usage_report_file_creation>
            Test that report is saved to cost-report.md file.
          <Function test_analyze_usage_missing_duration_field>
            Test handling of runs with missing run_duration_ms field.
        <Class TestCostConstants>
          Test cost constants are correct.
          <Function test_cost_per_minute_constant>
            Test COST_PER_MIN_LINUX constant is correct.
          <Function test_weekly_budget_constant>
            Test WEEKLY_BUDGET_MINS constant is correct.
          <Function test_monthly_budget_constant>
            Test MONTHLY_BUDGET_MINS constant is correct.
        <Class TestMainExecution>
          Test the main execution flow of the script.
          <Function test_main_uses_environment_variable_for_period>
            Test main() reads PERIOD_DAYS from environment.
          <Function test_main_defaults_to_7_days>
            Test main() defaults to 7 days when PERIOD_DAYS not set.
        <Class TestEdgeCases>
          Test edge cases and error conditions.
          <Function test_zero_duration_workflow>
            Test workflow with 0 duration.
          <Function test_very_large_duration>
            Test workflow with very large duration.
          <Function test_workflow_name_with_special_characters>
            Test workflow names with special markdown characters.
          <Function test_hundreds_of_workflow_runs>
            Test performance with large number of runs.
    <Dir cli>
      <Module test_add_tool.py>
        Tests for add_tool CLI command.

        Tests the tool file generation and scaffolding functionality.
        Target Coverage: 80%+ (line and branch)

        Tests cover:
        - Tool file generation with valid inputs
        - Test file creation
        - Template substitution (class names, function names)
        - Error handling (FileExistsError)
        - Edge cases (special characters, empty inputs)
        <Class TestGenerateTool>
          Test tool file generation functionality.
          <Function test_generate_tool_creates_tool_file>
            Test generate_tool creates tool file in correct location.
          <Function test_generate_tool_creates_test_file>
            Test generate_tool creates test file.
          <Function test_generate_tool_with_underscores_in_name>
            Test tool generation with snake_case name.
          <Function test_generate_tool_with_hyphens_in_name>
            Test tool generation converts hyphens to underscores in function names.
          <Function test_generate_tool_template_substitution>
            Test template correctly substitutes all placeholders.
          <Function test_generate_tool_includes_pydantic_models>
            Test generated tool includes Pydantic model definitions.
          <Function test_generate_tool_includes_metadata>
            Test generated tool includes MCP metadata.
          <Function test_generate_tool_raises_error_if_file_exists>
            Test generate_tool raises FileExistsError if tool file already exists.
          <Function test_generate_tool_creates_directory_if_not_exists>
            Test generate_tool creates src/tools directory if it doesn't exist.
          <Function test_generate_tool_creates_test_directory_if_not_exists>
            Test generate_tool creates tests/tools directory if it doesn't exist.
          <Function test_generate_tool_test_file_content>
            Test generated test file has correct structure.
          <Function test_generate_tool_single_word_name>
            Test tool generation with single word name.
          <Function test_generate_tool_multiple_words_with_underscores>
            Test tool generation with multiple underscored words.
          <Function test_generate_tool_description_preserved>
            Test tool description is preserved in generated files.
          <Function test_generate_tool_file_is_valid_python>
            Test generated tool file is syntactically valid Python.
          <Function test_generate_tool_test_file_is_valid_python>
            Test generated test file is syntactically valid Python.
        <Class TestToolTemplate>
          Test the tool template structure.
          <Function test_tool_template_contains_required_placeholders>
            Test TOOL_TEMPLATE contains all required placeholder variables.
          <Function test_tool_template_has_input_output_models>
            Test template includes Pydantic input/output models.
          <Function test_tool_template_has_function_definition>
            Test template includes function definition.
          <Function test_tool_template_has_metadata>
            Test template includes metadata for MCP registration.
          <Function test_tool_template_has_error_handling>
            Test template includes error handling in function.
          <Function test_tool_template_can_be_formatted>
            Test template can be formatted with sample values.
        <Class TestGenerateToolEdgeCases>
          Test edge cases and error scenarios for generate_tool.
          <Function test_generate_tool_with_numbers_in_name>
            Test tool generation with numbers in name.
          <Function test_generate_tool_preserves_tool_metadata_structure>
            Test metadata structure is valid Python dict.
          <Function test_generate_tool_test_imports_correct_module>
            Test test file imports from correct module path.
          <Function test_generate_tool_empty_description>
            Test tool generation with empty description.
          <Function test_generate_tool_very_long_name>
            Test tool generation with very long name.
          <Function test_generate_tool_preserves_directory_structure>
            Test generate_tool doesn't interfere with existing files.
      <Module test_cli.py>
        CLI Smoke Tests

        Basic smoke tests for CLI commands to ensure they can be invoked without errors.
        These tests don't verify full functionality, but ensure commands are accessible
        and provide proper help text.

        Following TDD principles - these tests verify CLI commands are properly wired.
        <Class TestCLISmoke>
          Smoke tests for CLI commands.
          <Function test_cli_main_help>
            Test that main CLI help command works.
          <Function test_cli_version>
            Test that version command works.
          <Function test_init_command_help>
            Test that init command help works.
          <Function test_create_agent_command_help>
            Test that create-agent command help works.
          <Function test_add_tool_command_help>
            Test that add-tool command help works.
          <Function test_migrate_command_help>
            Test that migrate command help works.
        <Class TestCLITemplateOptions>
          Test that CLI commands accept valid template options.
          <Function test_init_accepts_valid_templates>
            Test that init command accepts valid template choices.
          <Function test_create_agent_accepts_valid_templates>
            Test that create-agent command accepts valid template choices.
          <Function test_migrate_accepts_valid_frameworks>
            Test that migrate command accepts valid framework choices.
      <Module test_create_agent.py>
        Tests for create_agent CLI command.

        Tests the agent file generation and scaffolding functionality.
        Target Coverage: 80%+ (line and branch)

        Tests cover:
        - Agent file generation with valid inputs
        - Template selection (basic, research, customer-support)
        - Test file creation
        - Template substitution
        - Error handling (ValueError, FileExistsError)
        - Edge cases
        <Class TestGenerateAgent>
          Test agent file generation functionality.
          <Function test_generate_agent_creates_agent_file>
            Test generate_agent creates agent file in correct location.
          <Function test_generate_agent_creates_test_file>
            Test generate_agent creates test file.
          <Function test_generate_agent_basic_template>
            Test agent generation with basic template.
          <Function test_generate_agent_research_template>
            Test agent generation with research template.
          <Function test_generate_agent_customer_support_template>
            Test agent generation with customer-support template.
          <Function test_generate_agent_invalid_template_raises_error>
            Test generate_agent raises ValueError for invalid template.
          <Function test_generate_agent_raises_error_if_file_exists>
            Test generate_agent raises FileExistsError if agent file already exists.
          <Function test_generate_agent_with_underscores_in_name>
            Test agent generation with snake_case name.
          <Function test_generate_agent_creates_directory_if_not_exists>
            Test generate_agent creates src/agents directory if it doesn't exist.
          <Function test_generate_agent_creates_test_directory_if_not_exists>
            Test generate_agent creates tests/agents directory if it doesn't exist.
          <Function test_generate_agent_test_file_content>
            Test generated test file has correct structure.
          <Function test_generate_agent_default_template_is_basic>
            Test generate_agent uses 'basic' template by default.
          <Function test_generate_agent_tools_parameter_accepted>
            Test generate_agent accepts tools parameter without error.
          <Function test_generate_agent_single_word_name>
            Test agent generation with single word name.
          <Function test_generate_agent_multiple_words_with_underscores>
            Test agent generation with multiple underscored words.
          <Function test_generate_agent_file_is_valid_python>
            Test generated agent file is syntactically valid Python.
          <Function test_generate_agent_test_file_is_valid_python>
            Test generated test file is syntactically valid Python.
        <Class TestAgentTemplates>
          Test agent template structures.
          <Function test_agent_templates_dict_exists>
            Test AGENT_TEMPLATES dictionary exists and has templates.
          <Function test_agent_templates_has_basic>
            Test basic template exists.
          <Function test_agent_templates_has_research>
            Test research template exists.
          <Function test_agent_templates_has_customer_support>
            Test customer-support template exists.
          <Function test_basic_template_contains_required_placeholders>
            Test basic template contains required placeholder variables.
          <Function test_research_template_has_search_and_summarize>
            Test research template has search and summarize nodes.
          <Function test_customer_support_template_has_classification>
            Test customer-support template has intent classification.
          <Function test_basic_template_can_be_formatted>
            Test basic template can be formatted with sample values.
          <Function test_all_templates_use_langgraph>
            Test all templates import LangGraph.
          <Function test_all_templates_have_typed_dict>
            Test all templates use TypedDict for state.
          <Function test_all_templates_compile_graph>
            Test all templates compile the graph.
        <Class TestGenerateAgentEdgeCases>
          Test edge cases and error scenarios for generate_agent.
          <Function test_generate_agent_with_numbers_in_name>
            Test agent generation with numbers in name.
          <Function test_generate_agent_very_long_name>
            Test agent generation with very long name.
          <Function test_generate_agent_preserves_directory_structure>
            Test generate_agent doesn't interfere with existing files.
          <Function test_generate_agent_test_imports_correct_module>
            Test test file imports from correct module path.
          <Function test_generate_agent_all_templates_work[basic]>
            Test all agent templates can be generated without errors.
          <Function test_generate_agent_all_templates_work[research]>
            Test all agent templates can be generated without errors.
          <Function test_generate_agent_all_templates_work[customer-support]>
            Test all agent templates can be generated without errors.
          <Function test_generate_agent_tools_parameter_none>
            Test generate_agent works with tools=None (default).
          <Function test_generate_agent_tools_parameter_empty_list>
            Test generate_agent works with empty tools list.
          <Function test_generate_agent_multiple_agents_different_templates>
            Test generating multiple agents with different templates.
          <Function test_generate_agent_error_doesnt_create_partial_files>
            Test that validation errors don't create partial files.
          <Function test_generate_agent_case_sensitive_template_validation>
            Test template validation is case-sensitive.
          <Function test_generate_agent_preserves_agent_name_in_test>
            Test agent name is correctly used in test file.
          <Function test_generate_agent_research_template_structure>
            Test research template has correct graph structure.
          <Function test_generate_agent_support_template_has_routing>
            Test customer-support template has conditional routing.
      <Module test_init.py>
        Tests for CLI project initialization.

        Tests cover:
        - Project creation with different templates
        - Directory structure validation
        - Error handling for existing directories
        - Invalid template validation
        <Function test_init_project_creates_quickstart_template_successfully>
          Test init_project() with quickstart template creates project.
        <Function test_init_project_creates_production_template_successfully>
          Test init_project() with production template creates project.
        <Function test_init_project_creates_enterprise_template_successfully>
          Test init_project() with enterprise template creates project.
        <Function test_init_project_uses_production_template_by_default>
          Test init_project() defaults to production template when not specified.
        <Function test_init_project_raises_error_when_directory_exists>
          Test init_project() raises FileExistsError when directory already exists.
        <Function test_init_project_raises_error_for_invalid_template>
          Test init_project() raises ValueError for invalid template name.
        <Function test_quickstart_template_creates_correct_directory_structure>
          Test quickstart template creates expected directory structure.
        <Function test_init_project_accepts_string_path>
          Test init_project() accepts string path (not just Path object).
        <Function test_init_project_creates_nested_parent_directories>
          Test init_project() creates parent directories if needed.
    <Dir core>
      <Package interrupts>
        <Module test_approval.py>
          Tests for Approval System (Human-in-the-Loop Workflows)

          Security-Critical Module: Approval bypass could lead to unauthorized actions.
          Target Coverage: 90%+ (line and branch)

          Tests cover:
          - Approval status enumeration
          - Approval request/response models
          - Approval node execution
          - State management
          - Helper functions (approve/reject/check)
          - Edge cases and error scenarios
          <Class TestApprovalStatus>
            Test approval status enumeration.
            <Function test_approval_status_values>
              Test all approval status values exist.
            <Function test_approval_status_string_conversion>
              Test status can be created from string.
            <Function test_approval_status_invalid_value>
              Test invalid status raises error.
          <Class TestApprovalRequiredModel>
            Test ApprovalRequired model validation and defaults.
            <Function test_approval_required_minimal_fields>
              Test creation with only required fields.
            <Function test_approval_required_all_fields>
              Test creation with all fields populated.
            <Function test_approval_required_serialization>
              Test model can be serialized to dict.
            <Function test_approval_required_json_serialization>
              Test model can be serialized to JSON.
          <Class TestApprovalResponseModel>
            Test ApprovalResponse model validation.
            <Function test_approval_response_minimal_fields>
              Test creation with minimal fields.
            <Function test_approval_response_with_reason_and_modifications>
              Test creation with reason and modifications.
            <Function test_approval_response_rejected_status>
              Test rejected approval response.
          <Class TestApprovalNode>
            Test ApprovalNode class functionality.
            <Function test_approval_node_init_minimal>
              Test ApprovalNode initialization with minimal parameters.
            <Function test_approval_node_init_full>
              Test ApprovalNode initialization with all parameters.
            <Function test_approval_node_call_creates_request>
              Test ApprovalNode __call__ creates approval request in state.
            <Function test_approval_node_call_multiple_requests>
              Test ApprovalNode appends to existing approval requests.
            <Function test_approval_node_call_generates_unique_ids>
              Test each approval node call generates unique approval ID.
            <Function test_approval_node_notification_webhook_none>
              Test notification is NOT sent when webhook is None.
            <Function test_approval_node_notification_webhook_present>
              Test notification placeholder is called when webhook is set.
            <Function test_approval_node_call_preserves_original_state>
              Test approval node preserves user data in state.
          <Class TestCheckApprovalStatus>
            Test check_approval_status function.
            <Function test_check_approval_status_pending_no_responses>
              Test pending status when no responses exist.
            <Function test_check_approval_status_pending_different_id>
              Test pending status when approval ID not in responses.
            <Function test_check_approval_status_approved>
              Test approved status returned correctly.
            <Function test_check_approval_status_rejected>
              Test rejected status returned correctly.
            <Function test_check_approval_status_expired>
              Test expired status returned correctly.
          <Class TestApproveAction>
            Test approve_action function.
            <Function test_approve_action_creates_response>
              Test approve_action creates approval response.
            <Function test_approve_action_clears_pending_flag>
              Test approve_action clears pending_approval flag.
            <Function test_approve_action_without_reason>
              Test approve_action without optional reason.
            <Function test_approve_action_preserves_existing_responses>
              Test approve_action preserves other approval responses.
            <Function test_approve_action_multiple_approvals>
              Test multiple sequential approvals.
          <Class TestRejectAction>
            Test reject_action function.
            <Function test_reject_action_creates_response>
              Test reject_action creates rejection response.
            <Function test_reject_action_sets_workflow_halted>
              Test reject_action sets workflow_halted flag.
            <Function test_reject_action_clears_pending_flag>
              Test reject_action clears pending_approval flag.
            <Function test_reject_action_preserves_existing_responses>
              Test reject_action preserves other approval responses.
          <Class TestCreateApprovalWorkflow>
            Test create_approval_workflow function.
            <Function test_create_approval_workflow_single_node>
              Test adding approval to single node.
            <Function test_create_approval_workflow_multiple_nodes>
              Test adding approval to multiple nodes.
            <Function test_create_approval_workflow_with_webhook>
              Test approval workflow with notification webhook.
            <Function test_create_approval_workflow_empty_list>
              Test create_approval_workflow with empty approval points.
          <Class TestApprovalWorkflowIntegration>
            Integration tests for complete approval workflows.
            <Function test_full_approval_workflow>
              Test complete approval workflow: request -> approve -> check.
            <Function test_full_rejection_workflow>
              Test complete rejection workflow: request -> reject -> check.
            <Function test_multiple_approvals_workflow>
              Test workflow with multiple sequential approvals.
          <Class TestApprovalEdgeCases>
            Test edge cases and error scenarios.
            <Function test_approve_action_on_empty_state>
              Test approve_action creates approval_responses if missing.
            <Function test_reject_action_on_empty_state>
              Test reject_action creates approval_responses if missing.
            <Function test_approval_node_with_empty_state>
              Test ApprovalNode works with completely empty state.
            <Function test_check_approval_status_malformed_response>
              Test check_approval_status handles malformed response gracefully.
            <Function test_approval_request_context_isolation>
              Test approval request context is copy, not reference.
        <Module test_interrupts.py>
          Tests for Interrupt System (Human-in-the-Loop Workflows)

          Security-Critical Module: Interrupt bypass could skip validation checkpoints.
          Target Coverage: 90%+ (line and branch)

          Tests cover:
          - Interrupt type enumeration
          - Interrupt configuration
          - Interrupt handler registration
          - Conditional interrupts
          - Timeout interrupts
          - State management
          - Interrupt history
          - Edge cases
          <Class TestInterruptType>
            Test interrupt type enumeration.
            <Function test_interrupt_type_values>
              Test all interrupt type values exist.
            <Function test_interrupt_type_string_conversion>
              Test interrupt type can be created from string.
            <Function test_interrupt_type_invalid_value>
              Test invalid interrupt type raises error.
          <Class TestInterruptConfig>
            Test InterruptConfig model validation.
            <Function test_interrupt_config_minimal_fields>
              Test creation with minimal fields.
            <Function test_interrupt_config_all_fields>
              Test creation with all fields.
            <Function test_interrupt_config_condition_callable>
              Test condition field accepts callable.
          <Class TestInterruptHandler>
            Test InterruptHandler class functionality.
            <Function test_interrupt_handler_init>
              Test InterruptHandler initialization.
            <Function test_register_interrupt_returns_id>
              Test register_interrupt returns interrupt ID.
            <Function test_register_interrupt_stores_config>
              Test register_interrupt stores configuration.
            <Function test_register_multiple_interrupts>
              Test registering multiple interrupts.
            <Function test_register_interrupt_generates_unique_ids>
              Test each registration generates unique ID.
            <Function test_should_interrupt_no_interrupts_registered>
              Test should_interrupt returns False when no interrupts registered.
            <Function test_should_interrupt_matching_node>
              Test should_interrupt returns True for matching node.
            <Function test_should_interrupt_non_matching_node>
              Test should_interrupt returns False for non-matching node.
            <Function test_should_interrupt_with_condition_true>
              Test should_interrupt with condition that returns True.
            <Function test_should_interrupt_with_condition_false>
              Test should_interrupt with condition that returns False.
            <Function test_should_interrupt_without_condition>
              Test should_interrupt without condition always returns True for matching node.
            <Function test_handle_interrupt_updates_state>
              Test handle_interrupt adds interrupt metadata to state.
            <Function test_handle_interrupt_preserves_original_data>
              Test handle_interrupt preserves original state data.
            <Function test_handle_interrupt_adds_to_history>
              Test handle_interrupt adds entry to interrupt history.
            <Function test_handle_interrupt_multiple_calls_build_history>
              Test multiple handle_interrupt calls build history.
            <Function test_resume_clears_interrupted_flag>
              Test resume clears interrupted flag and adds timestamp.
            <Function test_resume_preserves_state>
              Test resume preserves original state data.
          <Class TestCreateInterruptHandler>
            Test create_interrupt_handler factory function.
            <Function test_create_interrupt_handler_returns_instance>
              Test create_interrupt_handler returns InterruptHandler instance.
            <Function test_create_interrupt_handler_creates_independent_instances>
              Test multiple calls create independent instances.
          <Class TestCreateConditionalInterrupt>
            Test create_conditional_interrupt helper function.
            <Function test_create_conditional_interrupt_returns_config>
              Test create_conditional_interrupt returns InterruptConfig.
            <Function test_create_conditional_interrupt_sets_type>
              Test interrupt type is set to CONDITIONAL.
            <Function test_create_conditional_interrupt_sets_condition>
              Test condition function is stored correctly.
            <Function test_create_conditional_interrupt_sets_node_name>
              Test node_name is set correctly.
            <Function test_create_conditional_interrupt_with_complex_condition>
              Test conditional interrupt with complex business logic.
          <Class TestCreateTimeoutInterrupt>
            Test create_timeout_interrupt helper function.
            <Function test_create_timeout_interrupt_returns_config>
              Test create_timeout_interrupt returns InterruptConfig.
            <Function test_create_timeout_interrupt_sets_type>
              Test interrupt type is set to TIMEOUT.
            <Function test_create_timeout_interrupt_sets_timeout>
              Test timeout_seconds is set correctly.
            <Function test_create_timeout_interrupt_sets_node_name>
              Test node_name is set correctly.
            <Function test_create_timeout_interrupt_various_durations>
              Test timeout interrupt with various durations.
          <Class TestInterruptWorkflowIntegration>
            Integration tests for complete interrupt workflows.
            <Function test_full_interrupt_workflow>
              Test complete interrupt workflow: register -> check -> interrupt -> resume.
            <Function test_conditional_interrupt_workflow>
              Test conditional interrupt only fires when condition met.
            <Function test_multiple_nodes_with_different_interrupts>
              Test multiple nodes with different interrupt configurations.
          <Class TestInterruptEdgeCases>
            Test edge cases and error scenarios.
            <Function test_should_interrupt_empty_state>
              Test should_interrupt with empty state.
            <Function test_should_interrupt_with_condition_empty_state>
              Test conditional interrupt with empty state.
            <Function test_handle_interrupt_empty_state>
              Test handle_interrupt with empty state.
            <Function test_resume_empty_state>
              Test resume with empty state.
            <Function test_interrupt_history_state_isolation>
              Test interrupt history stores state copy, not reference.
            <Function test_multiple_interrupts_same_node_different_conditions>
              Test registering multiple interrupts for same node with different conditions.
            <Function test_notification_channels_stored>
              Test notification_channels are stored in config.
            <Function test_auto_resume_flag>
              Test auto_resume flag is stored correctly.
      <Module test_agent_di.py>
        TDD Tests for Agent Dependency Injection Refactoring

        These tests define the behavior we want after refactoring agent.py
        to use dependency injection instead of global singletons.

        Following TDD:
        1. Write tests first (this file) - RED
        2. Refactor agent.py to pass tests - GREEN
        3. Verify no regressions - REFACTOR
        <Class TestAgentFactory>
          Test the new agent factory function
          <Function test_create_agent_with_container>
            Test creating agent using container
          <Function test_create_agent_with_settings>
            Test creating agent with custom settings
          <Function test_create_agent_returns_different_instances>
            Test that each call creates a new agent instance
          <Function test_create_agent_uses_container_telemetry>
            Test that agent uses container's telemetry provider
        <Class TestAgentGraphFactory>
          Test the graph creation function
          <Function test_create_agent_graph_with_settings>
            Test creating agent graph with settings
          <Function test_create_agent_graph_with_container>
            Test creating agent graph with container
          <Function test_create_agent_graph_is_compiled>
            Test that created graph is compiled and ready
        <Class TestBackwardCompatibility>
          Test that old singleton pattern still works during migration
          <Function test_get_agent_graph_still_works>
            Test that old get_agent_graph() function still works
          <Function test_get_agent_graph_returns_singleton>
            Test that old function still returns singleton
        <Class TestAgentStateManagement>
          Test that agent state works with container pattern
          <Function test_agent_with_memory_checkpointer>
            Test agent uses MemorySaver in test mode
          <Function test_agent_with_redis_checkpointer>
            Test agent can use Redis checkpointer from container
        <Class TestAgentConfiguration>
          Test that agent respects configuration from container
          <Function test_agent_uses_container_settings>
            Test that agent uses settings from container
        <Class TestAgentIsolation>
          Test that agents are properly isolated
          <Function test_multiple_agents_are_independent>
            Test that multiple agent instances don't share state
          <Function test_agents_from_different_containers_are_independent>
            Test that agents from different containers are independent
        <Class TestAgentTestHelperIntegration>
          Test that agent works with test helpers
          <Function test_create_test_agent_uses_new_factory>
            Test that test helper uses the new factory
          <Function test_create_test_agent_with_container>
            Test that test helper works with container fixture
        <Class TestAgentDocumentation>
          Test that refactored functions have good documentation
          <Function test_create_agent_has_docstring>
            Test that create_agent has comprehensive docstring
          <Function test_create_agent_graph_has_docstring>
            Test that create_agent_graph has comprehensive docstring
        <Class TestSettingsInjectionRegression>
          Regression tests for settings injection bug fix.

          SECURITY: These tests verify that Finding 2 from OpenAI Codex analysis is fixed.
          Previously, settings_to_use parameter was ignored, preventing:
          - Testing with custom settings
          - Multi-tenant deployments
          - Feature flags and A/B testing
          <Function test_create_agent_graph_respects_checkpoint_backend_override>
            Test that settings override actually changes checkpoint backend.

            REGRESSION TEST: Previously settings_to_use was discarded.
          <Function test_create_agent_graph_with_disabled_verification>
            Test that feature flags can be disabled via settings override.

            REGRESSION TEST: Feature flags should respect injected settings.
          <Function test_create_agent_graph_with_custom_model_name>
            Test that model configuration can be overridden via settings.

            REGRESSION TEST: Model selection should respect injected settings.
          <Function test_settings_injection_no_global_mutation>
            Test that using settings override doesn't mutate global settings.

            REGRESSION TEST: Finding 4 - Global state mutation in create_checkpointer.
            Previously, global settings object was temporarily mutated, causing race conditions.
          <Function test_concurrent_settings_overrides_no_interference>
            Test that concurrent settings overrides don't interfere with each other.

            REGRESSION TEST: Finding 4 - Race conditions from global state mutation.
          <Function test_create_agent_graph_impl_uses_settings_parameter>
            Test that create_agent_graph_impl actually uses the settings_to_use parameter.

            REGRESSION TEST: Finding 2 - settings_to_use was discarded in create_agent_graph_impl.
            The function had a TODO comment acknowledging it ignored the parameter.
          <Function test_settings_override_for_multi_tenant_scenario>
            Test multi-tenant scenario where different tenants have different settings.

            REGRESSION TEST: Multi-tenancy requires per-tenant settings injection.
      <Module test_cache.py>
        Comprehensive tests for multi-layer caching system.

        Tests:
        - L1 in-memory cache (TTLCache)
        - L2 Redis distributed cache
        - Cache stampede prevention with locks
        - Cache decorator (@cached)
        - Cache key generation and TTL logic
        - Cache statistics and metrics
        - Error handling and fallback behavior
        <Class TestCacheL1Operations>
          Test L1 (in-memory) cache operations
          <Function test_l1_set_and_get>
            Test basic L1 set and get
          <Function test_l1_cache_miss>
            Test L1 cache miss
          <Function test_l1_cache_complex_objects>
            Test L1 caching of complex objects
          <Function test_l1_ttl_expiration>
            Test L1 TTL expiration
          <Function test_l1_max_size_eviction>
            Test L1 cache evicts old entries when full
        <Class TestCacheL2Operations>
          Test L2 (Redis) cache operations
          <Function test_l2_set_and_get>
            Test L2 set and get with Redis
          <Function test_l2_fallback_to_l1_on_redis_failure>
            Test that L2 failures fall back gracefully
          <Function test_l2_cache_promotion>
            Test L2 → L1 promotion on cache hit
        <Class TestCacheDelete>
          Test cache deletion
          <Function test_delete_from_l1>
            Test delete removes from L1
          <Function test_delete_from_l2>
            Test delete removes from both L1 and L2
        <Class TestCacheClear>
          Test cache clearing
          <Function test_clear_all_l1>
            Test clearing all L1 cache
          <Function test_clear_with_pattern>
            Test clearing with pattern (L2 only)
          <Function test_clear_all_l2>
            Test clearing all L2 cache using pattern-based deletion (security fix)
        <Class TestCacheStampedePrevention>
          Test cache stampede prevention with locks
          <Coroutine test_get_with_lock_cache_hit>
            Test get_with_lock returns cached value without calling fetcher
          <Coroutine test_get_with_lock_cache_miss>
            Test get_with_lock calls fetcher on cache miss
          <Coroutine test_get_with_lock_prevents_stampede>
            Test that concurrent requests don't cause stampede
          <Coroutine test_get_with_lock_sync_fetcher>
            Test get_with_lock with synchronous fetcher
        <Class TestCacheTTLLogic>
          Test TTL determination from cache keys
          <Function test_ttl_from_key_prefix_auth>
            Test TTL determination for auth keys
          <Function test_ttl_from_key_prefix_user>
            Test TTL determination for user profile keys
          <Function test_ttl_default_for_unknown_prefix>
            Test default TTL for unknown key prefix
          <Function test_ttl_for_key_without_prefix>
            Test TTL for key without colon separator
        <Class TestCacheStatistics>
          Test cache statistics tracking
          <Function test_statistics_initial_state>
            Test statistics in initial state
          <Function test_statistics_after_operations>
            Test statistics after cache operations
          <Function test_statistics_l2_metrics>
            Test L2 statistics when Redis is available
        <Class TestCachedDecorator>
          Test @cached decorator
          <Coroutine test_cached_decorator_async_function>
            Test @cached decorator with async function
          <Function test_cached_decorator_sync_function>
            Test @cached decorator with synchronous function
          <Coroutine test_cached_decorator_with_kwargs>
            Test @cached decorator with keyword arguments
          <Coroutine test_cached_decorator_long_key_hashing>
            Test that long cache keys are hashed
        <Class TestCacheKeyGeneration>
          Test cache key generation helper
          <Function test_generate_cache_key_basic>
            Test basic cache key generation
          <Function test_generate_cache_key_no_prefix>
            Test cache key generation without prefix
          <Function test_generate_cache_key_long_key_hashing>
            Test that long keys are hashed
        <Class TestCacheInvalidate>
          Test cache invalidation helper
          <Function test_cache_invalidate_pattern>
            Test cache_invalidate with pattern
        <Class TestAnthropicPromptCaching>
          Test Anthropic-specific prompt caching (L3)
          <Function test_create_anthropic_cached_message>
            Test Anthropic prompt caching message structure
        <Class TestCacheErrorHandling>
          Test error handling and resilience
          <Function test_redis_connection_failure_doesnt_crash>
            Test that Redis connection failure doesn't crash initialization
          <Function test_l2_get_failure_returns_none>
            Test that L2 get failures return None gracefully
          <Function test_l2_set_failure_doesnt_crash>
            Test that L2 set failures don't crash
        <Class TestCacheGlobalInstance>
          Test global cache singleton
          <Function test_get_cache_returns_singleton>
            Test get_cache returns same instance
          <Function test_get_cache_creates_instance>
            Test get_cache creates CacheService instance
        <Class TestCacheLayers>
          Test cache layer constants
          <Function test_cache_layer_values>
            Test CacheLayer enum values
        <Class TestCacheTTLConstants>
          Test cache TTL configuration
          <Function test_cache_ttls_defined>
            Test all expected cache TTLs are defined
          <Function test_cache_ttls_reasonable_values>
            Test TTL values are reasonable
      <Module test_exceptions.py>
        Unit tests for custom exception hierarchy.

        Tests exception creation, HTTP mapping, and error responses.
        <Class TestMCPServerExceptionBase>
          Test base MCPServerException class
          <Function test_exception_creation_with_defaults>
            Test creating exception with default values
          <Function test_exception_creation_with_custom_values>
            Test creating exception with custom values
          <Function test_exception_to_dict>
            Test converting exception to dictionary
          <Function test_exception_string_representation>
            Test exception string representation
          <Function test_exception_with_cause>
            Test exception with original cause
        <Class TestAuthenticationExceptions>
          Test authentication exception types
          <Function test_invalid_credentials_error>
            Test InvalidCredentialsError
          <Function test_token_expired_error>
            Test TokenExpiredError
          <Function test_token_invalid_error>
            Test TokenInvalidError
          <Function test_mfa_required_error>
            Test MFARequiredError
        <Class TestAuthorizationExceptions>
          Test authorization exception types
          <Function test_permission_denied_error>
            Test PermissionDeniedError
          <Function test_resource_not_found_error>
            Test ResourceNotFoundError
          <Function test_insufficient_permissions_error>
            Test InsufficientPermissionsError
        <Class TestRateLimitExceptions>
          Test rate limiting exception types
          <Function test_rate_limit_exceeded_error>
            Test RateLimitExceededError
          <Function test_quota_exceeded_error>
            Test QuotaExceededError
        <Class TestValidationExceptions>
          Test validation exception types
          <Function test_input_validation_error>
            Test InputValidationError
          <Function test_schema_validation_error>
            Test SchemaValidationError
          <Function test_constraint_violation_error>
            Test ConstraintViolationError
        <Class TestExternalServiceExceptions>
          Test external service exception types
          <Function test_llm_provider_error>
            Test LLMProviderError
          <Function test_llm_rate_limit_error>
            Test LLMRateLimitError
          <Function test_llm_timeout_error>
            Test LLMTimeoutError
          <Function test_llm_model_not_found_error>
            Test LLMModelNotFoundError
          <Function test_openfga_error>
            Test OpenFGAError
          <Function test_redis_error>
            Test RedisError
          <Function test_keycloak_error>
            Test KeycloakError
        <Class TestResilienceExceptions>
          Test resilience pattern exception types
          <Function test_circuit_breaker_open_error>
            Test CircuitBreakerOpenError
          <Function test_retry_exhausted_error>
            Test RetryExhaustedError
          <Function test_timeout_error>
            Test TimeoutError
          <Function test_bulkhead_rejected_error>
            Test BulkheadRejectedError
        <Class TestComplianceExceptions>
          Test compliance exception types
          <Function test_gdpr_violation_error>
            Test GDPRViolationError
          <Function test_hipaa_violation_error>
            Test HIPAAViolationError
          <Function test_soc2_violation_error>
            Test SOC2ViolationError
        <Class TestStorageExceptions>
          Test storage exception types
          <Function test_data_not_found_error>
            Test DataNotFoundError
          <Function test_data_integrity_error>
            Test DataIntegrityError
        <Class TestExceptionMetadata>
          Test exception metadata handling
          <Function test_exception_with_trace_id>
            Test exception with trace ID
          <Function test_exception_with_metadata>
            Test exception with rich metadata
          <Function test_exception_auto_trace_id>
            Test exception auto-captures trace ID
        <Class TestExceptionHTTPMapping>
          Test HTTP status code mapping
          <Function test_auth_exceptions_map_to_401>
            Test authentication exceptions map to 401
          <Function test_authz_exceptions_map_to_403>
            Test authorization exceptions map to 403
          <Function test_rate_limit_exceptions_map_to_429>
            Test rate limit exceptions map to 429
          <Function test_validation_exceptions_map_to_400>
            Test validation exceptions map to 400
          <Function test_not_found_exceptions_map_to_404>
            Test not found exceptions map to 404
        <Class TestExceptionRetryPolicy>
          Test retry policy classification
          <Function test_client_errors_never_retry>
            Test that client errors have NEVER retry policy
          <Function test_external_errors_always_retry>
            Test that external service errors have ALWAYS retry policy
          <Function test_resilience_errors_conditional_retry>
            Test that resilience errors have CONDITIONAL retry policy
        <Class TestExceptionUserMessages>
          Test user-friendly error messages
          <Function test_auth_error_user_message>
            Test authentication error user message
          <Function test_authz_error_user_message>
            Test authorization error user message
          <Function test_rate_limit_error_user_message>
            Test rate limit error user message
          <Function test_custom_user_message>
            Test custom user message override
        <Class TestExceptionCategories>
          Test exception category classification
          <Function test_client_error_category>
            Test CLIENT_ERROR category
          <Function test_server_error_category>
            Test SERVER_ERROR category
          <Function test_external_error_category>
            Test EXTERNAL_ERROR category
          <Function test_auth_error_category>
            Test AUTH_ERROR category
          <Function test_rate_limit_category>
            Test RATE_LIMIT category
        <Class TestExceptionInheritance>
          Test exception inheritance hierarchy
          <Function test_all_exceptions_inherit_from_base>
            Test that all custom exceptions inherit from MCPServerException
          <Function test_exception_subclass_hierarchy>
            Test exception subclass hierarchy
      <Module test_test_helpers.py>
        TDD Tests for Test Helper Factories

        These helper functions make it easy to create test instances of:
        - Agents
        - MCP Servers
        - Settings
        - Containers

        Following TDD: Write tests first, then implement.
        <Class TestAgentHelpers>
          Test helper functions for creating test agents
          <Function test_create_test_agent_basic>
            Test creating a basic test agent
          <Function test_create_test_agent_with_custom_settings>
            Test creating agent with custom settings
          <Function test_create_test_agent_with_container>
            Test creating agent using container
          <Function test_create_test_agent_returns_different_instances>
            Test that each call creates a new agent instance.

            NOTE: Currently skipped because get_agent_graph() is a singleton.
            This test will pass after Phase 3 refactoring when agents use containers.
        <Class TestServerHelpers>
          Test helper functions for creating test MCP servers
          <Function test_create_test_server_basic>
            Test creating a basic test server
          <Function test_create_test_server_with_container>
            Test creating server using container
          <Function test_create_test_server_has_no_auth_by_default>
            Test that test server has no-op auth by default
        <Class TestSettingsHelpers>
          Test helper functions for creating test settings
          <Function test_create_test_settings_basic>
            Test creating basic test settings
          <Function test_create_test_settings_with_overrides>
            Test creating settings with overrides
          <Function test_create_test_settings_has_safe_defaults>
            Test that test settings have safe defaults
        <Class TestContainerHelpers>
          Test helper functions for container creation
          <Function test_create_test_container_helper_exists>
            Test that create_test_container helper is accessible
        <Class TestMockHelpers>
          Test helper functions for creating mocks
          <Function test_create_mock_llm_response>
            Test creating mock LLM response
          <Function test_create_mock_llm_stream>
            Test creating mock LLM stream
          <Function test_create_mock_mcp_request>
            Test creating mock MCP request
          <Function test_create_mock_jwt_token>
            Test creating mock JWT token
        <Class TestPytestFixtureHelpers>
          Test that helpers work well as pytest fixtures
          <Function test_agent_helper_as_fixture>
            Test using agent helper in a pytest fixture context
          <Function test_server_helper_as_fixture>
            Test using server helper in a pytest fixture context
        <Class TestHelperDocumentation>
          Test that helpers have good documentation
          <Function test_helpers_have_docstrings>
            Test that all helper functions have docstrings
    <Package deployment>
      <Module test_cloud_sql_proxy_config.py>
        Test suite for Cloud SQL Proxy sidecar configuration validation.

        Ensures that Cloud SQL Proxy containers are properly configured with:
        1. HTTP admin server port (--http-port=9801) for health checks
        2. Correct liveness/readiness probe configuration
        3. Required arguments for private IP connectivity

        This test prevents the critical failure mode where health check probes
        attempt to connect to port 9801 but the proxy doesn't expose it,
        causing continuous container restarts.
        <Function test_cloud_sql_proxy_files_exist>
          Test that expected Cloud SQL Proxy patch files exist.
        <Function test_cloud_sql_proxy_http_port_configured[patch_file0]>
          Test that Cloud SQL Proxy has --http-port=9801 configured.

          This is CRITICAL for health checks to work. Without this flag,
          the proxy doesn't expose the HTTP admin server, causing all
          liveness and readiness probes to fail with "connection refused".
        <Function test_cloud_sql_proxy_http_port_configured[patch_file1]>
          Test that Cloud SQL Proxy has --http-port=9801 configured.

          This is CRITICAL for health checks to work. Without this flag,
          the proxy doesn't expose the HTTP admin server, causing all
          liveness and readiness probes to fail with "connection refused".
        <Function test_cloud_sql_proxy_has_required_args[patch_file0]>
          Test that Cloud SQL Proxy has all required arguments.
        <Function test_cloud_sql_proxy_has_required_args[patch_file1]>
          Test that Cloud SQL Proxy has all required arguments.
        <Function test_cloud_sql_proxy_health_probes_configured[patch_file0]>
          Test that Cloud SQL Proxy has liveness and readiness probes configured correctly.
        <Function test_cloud_sql_proxy_health_probes_configured[patch_file1]>
          Test that Cloud SQL Proxy has liveness and readiness probes configured correctly.
        <Function test_cloud_sql_proxy_resource_limits[patch_file0]>
          Test that Cloud SQL Proxy has appropriate resource limits.
        <Function test_cloud_sql_proxy_resource_limits[patch_file1]>
          Test that Cloud SQL Proxy has appropriate resource limits.
        <Function test_cloud_sql_proxy_security_context[patch_file0]>
          Test that Cloud SQL Proxy has proper security context.
        <Function test_cloud_sql_proxy_security_context[patch_file1]>
          Test that Cloud SQL Proxy has proper security context.
      <Module test_codex_findings_validation.py>
        Comprehensive Validation Tests for OpenAI Codex Findings

        These tests validate fixes for 14 issues discovered during Codex deployment review:
        - 5 Critical (P0) issues that block production
        - 2 Medium (P1) security/reliability issues
        - 3 Low (P2-P3) technical debt issues
        - 2 Additional issues discovered during investigation

        Following TDD principles:
        1. These tests should FAIL before fixes are applied (RED phase)
        2. After implementing fixes, tests should PASS (GREEN phase)
        3. Refactor while keeping tests green (REFACTOR phase)

        Reference: OpenAI Codex Deployment Configuration Review (2025-01-09)
        <Class TestCriticalIssues>
          P0 Critical Issues - Production Blockers

          CODEX FINDING #1: These tests require kustomize CLI tool.
          Tests will skip gracefully if kustomize is not installed.
          <Function test_redis_ssl_configuration_matches_url_scheme>
            Test that redis_ssl setting matches URL scheme across all overlays.

            Issue #2 (Critical): Production ConfigMap enables redis_ssl: "true"
            while using redis:// URLs instead of rediss:// URLs.

            Expected behavior:
            - If redis_ssl: "true", URLs must use rediss:// scheme
            - If redis_ssl: "false", URLs must use redis:// scheme
            - Mismatched configuration causes TLS handshake failures

            Validates: deployments/overlays/production/configmap-patch.yaml:29
          <Function test_environment_variable_casing_consistent>
            Test that environment variables use correct casing (uppercase).

            Issue #3 (Critical): staging-gke deployment-patch.yaml uses lowercase
            redis_url/checkpoint_redis_url but application expects uppercase.

            Expected: REDIS_URL, CHECKPOINT_REDIS_URL (uppercase)
            Actual: redis_url, checkpoint_redis_url (lowercase)

            Impact: Application may fail to read configuration even with
            case_sensitive=False due to direct env lookups in code.

            Validates: deployments/overlays/staging-gke/deployment-patch.yaml:78,83
          <Function test_no_hardcoded_internal_ips>
            Test that no hard-coded internal IPs exist in deployment configs.

            Issue #4 (Critical): staging-gke overlay hard-codes internal IPs:
            - 10.138.129.37 (Memorystore Redis)
            - 10.110.0.3 (Cloud SQL)
            - 10.110.1.4 (Memorystore Redis)

            Impact: IPs change on failover, regional move, or project recreation.
            Solution: Use Cloud DNS names instead.

            Validates:
            - deployments/overlays/staging-gke/redis-session-endpoints.yaml:10
            - deployments/overlays/staging-gke/configmap-patch.yaml:24,33
          <Function test_no_unsubstituted_kustomize_variables>
            Test that Kustomize variables are properly substituted.

            Issue #5 (Critical): production-gke kustomization.yaml has
            $(GCP_PROJECT_ID) in image name without proper substitution.

            Impact: Image pull fails with literal $(GCP_PROJECT_ID) in name.
            Solution: Migrate to Helm-based templating.

            Validates: deployments/overlays/production-gke/kustomization.yaml:76
          <Function test_no_placeholder_values_in_production>
            Test that production configs have no placeholder values.

            Issue #6 (Critical): production-gke configmap-patch.yaml contains:
            - YOUR_PROJECT_ID
            - ${GCP_PROJECT_ID} (unsubstituted)

            Impact: Pods start with invalid configuration.
            Solution: Use Helm values or External Secrets.

            Validates: deployments/overlays/production-gke/configmap-patch.yaml:48
        <Class TestMediumPriorityIssues>
          P1 Medium Priority Issues - Security & Reliability
          <Function test_no_hardcoded_gcp_project_in_serviceaccount>
            Test that service account annotations use variables, not hard-coded projects.

            Issue #8 (Medium): production-gke serviceaccount-patch.yaml hard-codes:
            iam.gke.io/gcp-service-account: mcp-prod-app-sa@my-gcp-project.iam...

            Impact: Workload Identity fails in actual GCP project.
            Solution: Use Helm templating with project ID variable.

            Validates: deployments/overlays/production-gke/serviceaccount-patch.yaml:8
          <Function test_main_application_has_rbac_role>
            Test that main application service account has explicit RBAC role.

            Issue #16 (Additional): Main app service account 'mcp-server-langgraph'
            has no Role/RoleBinding defined. Only supporting services have RBAC.

            Impact: Relies on default permissions (security risk).
            Solution: Create explicit Role granting access to mcp-server-langgraph-secrets.

            Validates: deployments/base/serviceaccount-roles.yaml
        <Class TestLowPriorityIssues>
          P2-P3 Low Priority Issues - Technical Debt

          CODEX FINDING #1: Tests in this class may require kustomize CLI tool.
          <Function test_no_unused_secret_generators>
            Test that all secret generators are actually used.

            Issue #13 (Low): production-gke kustomization.yaml defines
            app-config-secrets generator but nothing references it.

            Impact: Dead code, confusion.
            Solution: Remove unused generator.

            Validates: deployments/overlays/production-gke/kustomization.yaml:100-104
          <Function test_cors_origin_not_example_domain>
            Test that CORS origins use appropriate values, not example.com.

            Issue #14 (Low): base ingress-http.yaml defaults to:
            nginx.ingress.kubernetes.io/cors-allow-origin: "https://app.example.com"

            Impact: Production ingress rejects legitimate origins.
            Solution: Use environment-specific overlay patches.

            Validates: deployments/base/ingress-http.yaml:23,82
          <Function test_argocd_uses_environment_specific_values>
            Test that ArgoCD applications use environment-specific values files.

            Issue #15 (Low): argocd/applications/mcp-server-app.yaml uses:
            valueFiles: [values.yaml] instead of values-production.yaml

            Impact: Production uses generic values instead of prod-specific config.
            Solution: Use values-production.yaml or document inline overrides.

            Validates: deployments/argocd/applications/mcp-server-app.yaml:30
        <Class TestDocumentationConsistency>
          Documentation validation tests
          <Function test_deployment_readme_structure_matches_actual>
            Test that README documents actual directory structure.

            Issue #9 (Medium): deployments/README.md documents kustomize/
            hierarchy that doesn't exist.

            Actual structure: deployments/base, deployments/overlays
            Documented: kustomize/base, kustomize/overlays

            Validates: deployments/README.md:11
        <Class TestRedisSSLConsistency>
          Cross-environment configuration consistency
          <Function test_redis_ssl_consistent_across_environments>
            Test that Redis SSL settings are consistent and documented.

            Issue #17 (Additional): Redis SSL settings vary across environments:
            - production: redis_ssl="true" with redis:// (MISMATCH)
            - staging: redis_ssl="false" (CORRECT for private VPC)
            - base: not set

            Validates: All configmap patches
      <Module test_configmap_secret_validation.py>
        Test suite for validating ConfigMap keys and Secret references in Kubernetes deployments.

        This test suite validates that:
        1. All ConfigMap keys referenced in deployments exist in the actual ConfigMaps
        2. All Secret names match between patches and external-secrets
        3. Kustomize namePrefix is correctly applied to secret references
        4. No missing ConfigMap keys that would cause CreateContainerConfigError

        These tests follow TDD principles and are designed to prevent the pod crash issues
        that occurred on 2025-11-12 in staging-mcp-server-langgraph namespace.

        Root Causes Prevented:
        - Issue #1: Missing ConfigMap keys (session_cookie_secure, rate_limit_per_minute, etc.)
        - Issue #2: Secret name mismatch (mcp-server-langgraph-secrets vs staging-mcp-server-langgraph-secrets)
        <Class TestConfigMapValidation>
          Test that all ConfigMap key references are valid.
          <Function test_all_configmap_keys_exist[deployments/overlays/staging-gke-staging-mcp-server-langgraph]>
            Test that all ConfigMap keys referenced in deployments actually exist.

            This test would have caught the staging pod crash issue where keys like:
            - session_cookie_secure
            - session_cookie_samesite
            - session_max_age_seconds
            - rate_limit_per_minute
            - rate_limit_burst
            - circuit_breaker_*
            - retry_*
            - gdpr_retention_days
            were referenced but not defined in the ConfigMap.
          <Function test_all_configmap_keys_exist[deployments/overlays/production-gke-production-mcp-server-langgraph]>
            Test that all ConfigMap keys referenced in deployments actually exist.

            This test would have caught the staging pod crash issue where keys like:
            - session_cookie_secure
            - session_cookie_samesite
            - session_max_age_seconds
            - rate_limit_per_minute
            - rate_limit_burst
            - circuit_breaker_*
            - retry_*
            - gdpr_retention_days
            were referenced but not defined in the ConfigMap.
          <Function test_required_configmap_keys_present_staging[deployments/overlays/staging-gke]>
            Test that required ConfigMap keys are present for the application (staging only).

            This is a whitelist approach - ensuring critical keys always exist.
            Production may have different requirements.
        <Class TestSecretValidation>
          Test that Secret names and references are consistent.
          <Function test_primary_app_secret_names_match_external_secrets[deployments/overlays/staging-gke-staging-]>
            Test that primary application secret names match ExternalSecret targets.

            This test would have caught the issue where deployment referenced
            'mcp-server-langgraph-secrets' but ExternalSecret created
            'staging-mcp-server-langgraph-secrets'.

            Note: Only validates secrets that should be managed by ExternalSecrets.
            Other secrets (like otel-collector-secrets) may be managed differently.
          <Function test_primary_app_secret_names_match_external_secrets[deployments/overlays/production-gke-production-]>
            Test that primary application secret names match ExternalSecret targets.

            This test would have caught the issue where deployment referenced
            'mcp-server-langgraph-secrets' but ExternalSecret created
            'staging-mcp-server-langgraph-secrets'.

            Note: Only validates secrets that should be managed by ExternalSecrets.
            Other secrets (like otel-collector-secrets) may be managed differently.
          <Function test_all_external_secret_data_mappings_exist[deployments/overlays/staging-gke-staging-]>
            Test that all ExternalSecret data mappings are complete.

            This ensures all secret keys used in templates have corresponding data mappings.
            Note: Template uses kebab-case (e.g. 'keycloak-client-id') while data mappings
            use camelCase (e.g. 'keycloakClientId'), and templates can reference constructed
            values using Go templating syntax.
          <Function test_all_external_secret_data_mappings_exist[deployments/overlays/production-gke-production-]>
            Test that all ExternalSecret data mappings are complete.

            This ensures all secret keys used in templates have corresponding data mappings.
            Note: Template uses kebab-case (e.g. 'keycloak-client-id') while data mappings
            use camelCase (e.g. 'keycloakClientId'), and templates can reference constructed
            values using Go templating syntax.
          <Function test_all_secret_keys_referenced_are_created[deployments/overlays/staging-gke]>
            Test that all secret keys referenced in deployments are actually created by ExternalSecrets.

            This would catch missing secret keys like the ones we just added:
            - keycloak-client-id
            - keycloak-client-secret
            - openfga-store-id
            - etc.

            Only validates secrets managed by ExternalSecrets.
          <Function test_all_secret_keys_referenced_are_created[deployments/overlays/production-gke]>
            Test that all secret keys referenced in deployments are actually created by ExternalSecrets.

            This would catch missing secret keys like the ones we just added:
            - keycloak-client-id
            - keycloak-client-secret
            - openfga-store-id
            - etc.

            Only validates secrets managed by ExternalSecrets.
        <Class TestKustomizePrefixConsistency>
          Test that Kustomize namePrefix is correctly applied.
          <Function test_patch_files_use_prefixed_secret_names[deployments/overlays/staging-gke-staging-]>
            Test that patch files use the prefixed secret names.

            This catches the issue where deployment-redis-url-json-patch.yaml
            referenced 'mcp-server-langgraph-secrets' instead of
            'staging-mcp-server-langgraph-secrets'.
          <Function test_patch_files_use_prefixed_secret_names[deployments/overlays/production-gke-production-]>
            Test that patch files use the prefixed secret names.

            This catches the issue where deployment-redis-url-json-patch.yaml
            referenced 'mcp-server-langgraph-secrets' instead of
            'staging-mcp-server-langgraph-secrets'.
      <Module test_external_secrets_template_validation.py>
        Validation tests for External Secrets templates.

        This test module ensures that Kubernetes External Secrets templates have proper
        URL encoding filters for Redis passwords, preventing the production incident
        (staging-758b8f744) from recurring.

        Tests validate:
        1. Redis URL templates use | urlquery filter for password encoding
        2. Consistency with database URL templates (which already use urlquery)
        3. Regression prevention for accidental filter removal
        <Class TestExternalSecretsRedisURLEncoding>
          Validate External Secrets templates have proper URL encoding.
          <Function test_redis_url_template_has_urlquery_filter>
            Test that redis-url template uses | urlquery filter for password encoding.
          <Function test_checkpoint_redis_url_template_has_urlquery_filter>
            Test that checkpoint-redis-url template uses | urlquery filter.
          <Function test_consistency_with_database_url_templates>
            Test that Redis URLs follow same pattern as database URLs.

            Database URLs (keycloak-db-url, openfga-datastore-uri) already use
            | urlquery filter. Redis URLs must follow this established pattern.
          <Function test_redis_password_template_variable_correct>
            Test that Redis URLs reference correct template variable.
          <Function test_redis_url_structure_is_valid>
            Test that Redis URL template has valid structure.
          <Function test_comments_document_url_encoding_requirement>
            Test that YAML file includes comments about URL encoding.
        <Class TestExternalSecretsRegressionPrevention>
          Regression tests to prevent accidental removal of URL encoding filters.
          <Function test_no_unencoded_redis_password_in_template>
            Test that no Redis URL uses unencoded password.

            This prevents accidental regression where someone removes the urlquery filter.
          <Function test_urlquery_filter_count_matches_redis_url_count>
            Test that number of urlquery filters matches number of Redis URLs.

            Ensures every Redis URL with password has corresponding urlquery filter.
          <Function test_production_incident_prevention_marker>
            Test that template includes reference to production incident.

            This serves as documentation and reminder for future maintainers.
        <Class TestExternalSecretsMultiEnvironmentConsistency>
          Test that URL encoding is consistent across all deployment environments.
          <Function test_all_environments_use_urlquery_filter[deployments/overlays/staging-gke/external-secrets.yaml]>
            Test that all deployment overlays use urlquery filter for Redis URLs.
      <Module test_helm_configuration.py>
        TDD Tests for Helm Chart Configuration

        These tests ensure deployment configuration issues can never occur again by:
        1. Validating all secret keys referenced by deployment exist in secret template
        2. Ensuring no hard-coded credentials in configuration files
        3. Validating CORS configuration security
        4. Checking placeholder patterns are not committed
        5. Verifying ExternalSecrets key alignment
        6. Validating Helm chart lints successfully
        <Function test_helm_chart_lints_successfully>
          Test that Helm chart passes `helm lint` validation.

          This test prevents deployment blockers caused by:
          - Hyphenated keys in Go template expressions (e.g., .Values.kube-prometheus-stack.enabled)
          - Template syntax errors
          - Invalid Kubernetes resource schemas
          - Missing required values

          Red phase: This test will fail until hyphenated keys are wrapped with index function.
          Green phase: After fixing templates, helm lint should pass.
        <Function test_deployment_secret_keys_exist_in_template>
          Test that all secret keys referenced in deployment.yaml exist in secret.yaml.

          This test prevents the critical issue where pods crash because deployment
          references secret keys that don't exist in the secret template.
        <Function test_values_yaml_has_all_secret_fields>
          Test that values.yaml defines all secrets referenced in secret template.

          This ensures users can configure all secrets via values.yaml.
        <Function test_no_hardcoded_credentials_in_configmap>
          Test that configmap doesn't contain hard-coded credentials.

          This prevents security issues from credentials in plain text.
        <Function test_kong_cors_not_wildcard_with_credentials>
          Test that Kong CORS doesn't use wildcard origins with credentials enabled.

          This prevents the critical security vulnerability where any website can
          make authenticated requests to the API.
        <Function test_ingress_cors_not_wildcard>
          Test that Ingress CORS annotations don't use wildcard in production configs.

          This is checked at the base level - dev overlays can override.
        <Function test_no_dangerous_placeholders_in_production_configs[**/*.yaml-dangerous_patterns0]>
          Test that production configs don't contain unresolved placeholders.

          This prevents deployments that will fail due to invalid configuration.
        <Function test_external_secrets_keys_match_helm_template>
          Test that ExternalSecrets data keys match Helm secret template keys.

          This ensures ExternalSecrets will populate secrets with correct keys.
        <Function test_overlay_namespaces_have_patches>
          Test that each overlay either defines its own namespace or patches it.

          This prevents namespace confusion between environments.
        <Function test_deployment_image_versions_consistent>
          Test that all deployment configurations use consistent image versions.

          This prevents version drift across different deployment methods.
        <Function test_redis_password_not_optional>
          Test that Redis password is required (not optional) in deployments.

          This prevents Redis running without authentication.
        <Function test_base_service_no_cloud_specific_annotations>
          Test that base service doesn't have cloud-specific annotations.

          Cloud-specific annotations should be in overlays only.
      <Module test_helm_placeholder_validation.py>
        Helm Placeholder Validation Tests

        These tests validate that Helm values files don't contain unresolved
        placeholders that could cause deployment failures or security issues.

        Placeholders like PROJECT_ID, YOUR_STAGING_PROJECT_ID should either:
        1. Be resolved/replaced before deployment
        2. Be in .local.yaml files that aren't committed
        3. Have clear documentation on how to replace them
        <Class TestHelmPlaceholderValidation>
          Validate Helm values files for dangerous placeholders
          <Function test_helm_values_staging_has_no_dangerous_placeholders>
            Test that staging values don't have unresolved dangerous placeholders.

            Staging values should either:
            1. Use actual project IDs
            2. Use environment variable substitution
            3. Be documented as templates requiring replacement
          <Function test_helm_values_production_has_no_dangerous_placeholders>
            Test that production values don't have unresolved dangerous placeholders.

            Production values MUST NOT have placeholders - they should use:
            1. Actual project IDs
            2. Environment variable substitution (${{ vars.GCP_PROJECT_ID }})
            3. Secret references
          <Function test_helm_values_use_environment_variable_pattern>
            Test that Helm values use environment variable substitution pattern.

            Recommended pattern: ${{ vars.GCP_PROJECT_ID }} or ${GCP_PROJECT_ID}
            This allows GitHub Actions to substitute values securely.
          <Function test_local_yaml_pattern_documented>
            Test that .local.yaml pattern is documented for actual deployments.

            Users should create values-staging.local.yaml with real values,
            and .local.yaml files should be in .gitignore.
          <Function test_no_hardcoded_gcp_project_ids_in_non_vishnu_repos>
            Test that GCP project IDs are not hardcoded (except in vishnu's sandbox).

            This test would fail for other users, prompting them to replace
            vishnu-sandbox-* with their own project IDs.
          <Function test_serviceaccount_annotations_use_actual_project_or_var>
            Test that service account annotations use actual project ID or variable.

            Pattern: iam.gke.io/gcp-service-account: sa-name@PROJECT_ID.iam.gserviceaccount.com

            Should be either:
            1. sa-name@${GCP_PROJECT_ID}.iam.gserviceaccount.com
            2. sa-name@vishnu-sandbox-20250310.iam.gserviceaccount.com
        <Class TestPlaceholderDetectionPreCommit>
          Test that pre-commit hooks detect placeholders
          <Function test_precommit_config_has_placeholder_check>
            Test that pre-commit config includes placeholder validation.

            This ensures dangerous placeholders are caught before commit.
      <Module test_helm_templates.py>
        Test Helm chart templates can be rendered successfully.

        This module provides tests to catch Helm template errors early in development,
        before they cause failures in CI/CD or production deployments.

        TDD Approach:
        1. Test that helm template renders without errors
        2. Test that helm lint passes
        3. Test that rendered manifests are valid YAML
        4. Test checksum annotations are properly generated

        Regression Prevention:
        - Helm unittest plugin template resolution errors (Run #19311976718)
        - Checksum template pattern issues with include (print $.Template.BasePath)
        <Class TestHelmTemplateRendering>
          Test Helm chart template rendering.
          <Function test_helm_template_renders_without_errors>
            Test that helm template command succeeds.
          <Function test_helm_template_produces_valid_yaml>
            Test that rendered templates are valid YAML.
          <Function test_helm_template_checksum_annotations_present>
            Test that deployment has checksum annotations for config/secret changes.
        <Class TestHelmLint>
          Test Helm chart lint passes.
          <Function test_helm_lint_passes>
            Test that helm lint passes without warnings or errors.
          <Function test_helm_lint_output_contains_success>
            Test that helm lint output indicates success.
        <Class TestHelmDependencies>
          Test Helm chart dependencies are available.
          <Function test_helm_command_available>
            Test that helm command is available in PATH.
          <Function test_chart_yaml_exists>
            Test that Chart.yaml exists.
          <Function test_values_yaml_exists>
            Test that values.yaml exists.
          <Function test_templates_directory_exists>
            Test that templates directory exists and contains files.
        <Class TestHelmTemplateWithValues>
          Test Helm template rendering with different values.
          <Function test_helm_template_with_custom_values>
            Test rendering with custom values doesn't fail.
          <Function test_helm_template_with_staging_values>
            Test rendering with staging values if available.
          <Function test_helm_template_with_production_values>
            Test rendering with production values if available.
      <Module test_kustomize_build.py>
        Test suite for Kustomize build validation.

        Ensures that all Kustomize overlays build successfully and produce
        valid Kubernetes manifests without errors or warnings.
        <Function test_overlay_builds_successfully[overlay_dir0]>
          Test that overlay builds without errors.
        <Function test_overlay_produces_valid_yaml[overlay_dir0]>
          Test that overlay produces valid YAML.
        <Function test_all_manifests_have_required_fields[overlay_dir0]>
          Test that all manifests have required Kubernetes fields.
        <Function test_namespace_consistency[overlay_dir0]>
          Test that all namespaced resources use the correct namespace.
        <Function test_no_duplicate_resources[overlay_dir0]>
          Test that there are no duplicate resources (same kind/name/namespace).
        <Function test_deployments_have_valid_selectors[overlay_dir0]>
          Test that all Deployments have matching selectors and labels.
        <Function test_services_have_valid_selectors[overlay_dir0]>
          Test that all Services (except ExternalName) have valid selectors.
        <Function test_container_images_have_tags[overlay_dir0]>
          Test that all container images have explicit tags (not 'latest').
        <Function test_kustomization_resources_exist[overlay_dir0]>
          Test that all resource files referenced in kustomization.yaml actually exist.

          This catches errors where kustomization.yaml references files that don't exist,
          which would cause kustomize build to fail or silently skip resources.
        <Function test_cloud_overlay_builds_successfully[overlay_dir0]>
          Test that cloud-specific overlays (AWS, GCP, Azure) build without errors.

          Codex Finding #2 (P0): These overlays use configMapGenerator with behavior: replace
          against a non-generated ConfigMap, causing build failures.

          Red phase: This test will fail until ConfigMaps are converted to patches.
          Green phase: After fixing configMapGenerator to use patches, builds should succeed.
        <Function test_cloud_overlay_builds_successfully[overlay_dir1]>
          Test that cloud-specific overlays (AWS, GCP, Azure) build without errors.

          Codex Finding #2 (P0): These overlays use configMapGenerator with behavior: replace
          against a non-generated ConfigMap, causing build failures.

          Red phase: This test will fail until ConfigMaps are converted to patches.
          Green phase: After fixing configMapGenerator to use patches, builds should succeed.
        <Function test_cloud_overlay_builds_successfully[overlay_dir2]>
          Test that cloud-specific overlays (AWS, GCP, Azure) build without errors.

          Codex Finding #2 (P0): These overlays use configMapGenerator with behavior: replace
          against a non-generated ConfigMap, causing build failures.

          Red phase: This test will fail until ConfigMaps are converted to patches.
          Green phase: After fixing configMapGenerator to use patches, builds should succeed.
        <Function test_cloud_overlay_otel_config_present[overlay_dir0]>
          Test that cloud overlays include OTEL collector configuration.

          Validates that the overlay-specific OTEL configuration is properly applied.
        <Function test_cloud_overlay_otel_config_present[overlay_dir1]>
          Test that cloud overlays include OTEL collector configuration.

          Validates that the overlay-specific OTEL configuration is properly applied.
        <Function test_cloud_overlay_otel_config_present[overlay_dir2]>
          Test that cloud overlays include OTEL collector configuration.

          Validates that the overlay-specific OTEL configuration is properly applied.
        <Function test_aws_overlay_container_patches_apply>
          Test that AWS overlay patches are applied to the correct container.

          Codex Finding #4 (P1): AWS deployment-patch.yaml references container 'mcp-server'
          but base deployment uses 'mcp-server-langgraph', causing patches to be ignored.

          Red phase: AWS-specific env vars (AWS_REGION, etc.) will be missing from deployment.
          Green phase: After fixing container name, env vars should be present.
      <Module test_network_policies.py>
        Test suite for validating Kubernetes NetworkPolicy configurations.

        This test suite validates that:
        1. Database ports are correctly configured (PostgreSQL: 5432, Redis: 6379)
        2. Network policies allow necessary egress traffic
        3. Ingress rules are properly scoped
        4. No overly permissive selectors exist

        Following TDD principles - these tests should FAIL before fixes are applied.
        <Class TestNetworkPolicyPorts>
          Test that NetworkPolicies use correct database ports.
          <Function test_staging_postgresql_uses_correct_port>
            Test that staging NetworkPolicies use port 5432 for PostgreSQL.

            Validates Finding #6: Cloud SQL ports using 3307 instead of 5432

            PostgreSQL (and Cloud SQL PostgreSQL) uses port 5432, not 3307.
            Port 3307 is the MySQL default port.
          <Function test_production_postgresql_uses_correct_port>
            Test that production NetworkPolicies use port 5432 for PostgreSQL.

            PostgreSQL (and Cloud SQL PostgreSQL) uses port 5432, not 3307.
          <Function test_staging_redis_uses_correct_port>
            Test that Redis connections use port 6379.

            Redis default port is 6379 (unless explicitly configured otherwise).
          <Function test_no_overly_permissive_egress>
            Test that egress rules are not overly permissive.

            Overly permissive patterns:
            - Empty to: [] (allows all destinations)
            - to: [{}] (allows all destinations)
        <Class TestNetworkPolicySelectors>
          Test that NetworkPolicy selectors are properly scoped.
          <Function test_ingress_namespace_selectors_are_scoped>
            Test that ingress namespaceSelectors are properly scoped.

            Validates Finding (High Priority): Empty namespace selectors

            Pattern to avoid:
              namespaceSelector: {}  # Allows ALL namespaces

            Prefer:
              namespaceSelector:
                matchLabels:
                  name: specific-namespace
        <Class TestNetworkPolicyComments>
          Test that NetworkPolicy configurations have clear documentation.
          <Function test_database_egress_rules_have_comments>
            Test that database egress rules have explanatory comments.

            This helps prevent confusion about which port is for which database.
        <Function test_network_policy_coverage>
          Test that critical components have NetworkPolicies defined.

          CODEX FINDING #1: This test requires kustomize CLI tool.

          Components that should have network policies:
          - Main application pods
          - Database pods (if self-hosted)
          - Keycloak
          - OpenFGA
      <Module test_placeholder_validation.py>
        TDD Tests for Placeholder Validation in Deployment Overlays

        Ensures that production-ready overlays don't contain unresolved placeholders
        that would cause runtime failures.

        Codex Finding #3 (P0 Blocker): production-gke overlay emits placeholders after build
        <Function test_production_overlay_no_placeholders[overlay_path0]>
          Test that production overlays don't contain unresolved placeholders.

          Codex Finding #3 (P0): production-gke emits PLACEHOLDER_GCP_PROJECT_ID,
          PLACEHOLDER_SET_VIA_ENV, and PRODUCTION_DOMAIN after kustomize build.

          Red phase: This test will fail until placeholders are replaced with
          actual values or Kustomize replacements.

          Green phase: After fixing placeholder handling, no dangerous patterns
          should appear in the built manifest.
        <Function test_production_overlay_no_placeholders[overlay_path1]>
          Test that production overlays don't contain unresolved placeholders.

          Codex Finding #3 (P0): production-gke emits PLACEHOLDER_GCP_PROJECT_ID,
          PLACEHOLDER_SET_VIA_ENV, and PRODUCTION_DOMAIN after kustomize build.

          Red phase: This test will fail until placeholders are replaced with
          actual values or Kustomize replacements.

          Green phase: After fixing placeholder handling, no dangerous patterns
          should appear in the built manifest.
        <Function test_workload_identity_service_account_valid[overlay_path0]>
          Test that Workload Identity service account annotations are valid.

          Validates that ServiceAccount has proper GCP service account format:
          {sa-name}@{project-id}.iam.gserviceaccount.com
        <Function test_workload_identity_service_account_valid[overlay_path1]>
          Test that Workload Identity service account annotations are valid.

          Validates that ServiceAccount has proper GCP service account format:
          {sa-name}@{project-id}.iam.gserviceaccount.com
        <Function test_environment_variables_no_placeholders[overlay_path0]>
          Test that Deployment environment variables don't contain placeholders.

          Validates that env vars in Deployments have actual values, not placeholders.
        <Function test_environment_variables_no_placeholders[overlay_path1]>
          Test that Deployment environment variables don't contain placeholders.

          Validates that env vars in Deployments have actual values, not placeholders.
      <Module test_security_hardening.py>
        Regression tests for Kubernetes security hardening requirements.

        Prevents recurrence of security misconfigurations detected by Trivy scans.

        TDD Context:
        - RED (2025-11-12): Keycloak container had readOnlyRootFilesystem: false
        - Trivy scan flagged as HIGH severity (AVD-KSV-0014)
        - GREEN: Set readOnlyRootFilesystem: true with proper volume mounts
        - REFACTOR: This test prevents regression

        Following TDD: Tests written FIRST to catch security violations, then fixes applied.
        <Class TestKubernetesSecurityHardening>
          Test Kubernetes deployments follow security best practices.
          <Function test_all_containers_have_readonly_root_filesystem>
            Test: All containers must have readOnlyRootFilesystem: true.

            RED (Before Fix - 2025-11-12):
            - Keycloak container had readOnlyRootFilesystem: false
            - Trivy scan flagged as HIGH severity (AVD-KSV-0014)
            - Security risk: allows container to tamper with filesystem
            - Workflow: Deploy to GKE Staging (Run #19309378657) FAILED

            GREEN (After Fix):
            - Set readOnlyRootFilesystem: true
            - Added volume mounts for /tmp, /opt/keycloak/data, etc.
            - Trivy scan passes
            - Deployment workflow succeeds

            REFACTOR (2025-11-14 - Codex finding):
            - This test prevents regression
            - Validates all deployment manifests automatically
            - Runs in CI pre-commit hook and deployment validation
            - NOW ALLOWS documented exceptions with TODO comments + justification
            - Staging Keycloak has documented temporary exception for Quarkus AOT compilation
            - Documented exceptions must have TODO + clear reason within 5 lines
          <Function test_containers_with_readonly_fs_have_volume_mounts>
            Test: Containers with readOnlyRootFilesystem: true must have volume mounts for writable dirs.

            Ensures containers don't fail to start due to missing writable directories.
            Common directories needing mounts: /tmp, /var/cache, /opt/*/data
          <Function test_security_contexts_follow_least_privilege>
            Test: All containers follow least-privilege security principles.

            Validates:
            - runAsNonRoot: true
            - allowPrivilegeEscalation: false
            - capabilities.drop: [ALL]
      <Module test_service_accounts.py>
        Test suite for validating Service Account configuration and separation.

        This test suite validates that:
        1. Service Accounts follow principle of least privilege
        2. Different components use separate Service Accounts
        3. Workload Identity bindings are correctly configured
        4. RBAC permissions are appropriately scoped

        Following TDD principles - these tests should FAIL before fixes are applied.
        <Class TestServiceAccountSeparation>
          Test that different components use separate Service Accounts.
          <Function test_components_use_separate_service_accounts>
            Test that different components use separate Service Accounts.

            Validates Finding (High Priority): Service Account separation

            Components that should have separate SAs:
            - Main application (mcp-server-langgraph)
            - PostgreSQL (if self-hosted)
            - Redis (if self-hosted)
            - Keycloak
            - OpenFGA
            - Qdrant

            After fix, each component should have its own ServiceAccount.
          <Function test_service_accounts_exist_for_components>
            Test that ServiceAccount resources are defined for each component.

            After implementing SA separation, we should have:
            - postgres-sa
            - redis-sa
            - keycloak-sa
            - openfga-sa
            - qdrant-sa
            - mcp-server-langgraph (main app)
        <Class TestWorkloadIdentityBindings>
          Test Workload Identity configuration for GKE.
          <Function test_staging_workload_identity_annotation_format>
            Test that Workload Identity annotations use correct format.

            Format: iam.gke.io/gcp-service-account: <gsa>@<project>.iam.gserviceaccount.com
          <Function test_production_workload_identity_annotation_format>
            Test that Workload Identity annotations use correct format in production.
          <Function test_critical_components_have_workload_identity>
            Test that components needing GCP access have Workload Identity configured.

            Components that need GCP access:
            - Main app (for Secret Manager, Cloud SQL, etc.)
            - OpenFGA (for Cloud SQL)
            - Keycloak (for Cloud SQL)
            - External Secrets Operator
        <Class TestRBACConfiguration>
          Test RBAC (Roles and RoleBindings) configuration.
          <Function test_roles_follow_least_privilege>
            Test that RBAC Roles follow principle of least privilege.

            Checks for overly permissive patterns:
            - verbs: ["*"] (all verbs)
            - resources: ["*"] (all resources)
            - apiGroups: ["*"] (all API groups)
          <Function test_role_bindings_reference_existing_service_accounts>
            Test that RoleBindings reference ServiceAccounts that exist.
      <Module test_service_dependencies.py>
        Test suite for service dependency validation in init containers.

        Ensures that all service names referenced in init containers actually
        exist in the deployed manifests. This prevents pods from getting stuck
        in Init state waiting for non-existent services.

        Critical issue prevented:
        - Init containers with 'nc -z <service> <port>' that reference services
          that don't exist, causing "bad address" errors and infinite wait loops.
        <Function test_staging_gke_overlay_builds>
          Test that staging-gke overlay builds without errors.
        <Function test_all_init_container_services_exist>
          Test that all services referenced in init containers actually exist.

          This is a CRITICAL test that prevents the failure mode where init
          containers wait indefinitely for services that don't exist, blocking
          all pod startup.
        <Function test_staging_services_have_prefix>
          Test that staging services have the 'staging-' prefix applied by Kustomize.
        <Function test_init_container_refs_match_staging_prefix>
          Test that init container service refs use staging- prefix.

          This catches the bug where base manifests reference unprefixed service
          names but Kustomize applies a prefix, causing name mismatches.
        <Function test_required_services_exist_in_staging>
          Test that all required services exist in staging deployment.
        <Function test_external_name_services_have_endpoints>
          Test that ExternalName services have valid external endpoints.
        <Function test_external_name_services_use_dns_not_ip>
          Test that ExternalName services use DNS names, not IP addresses.

          ExternalName services create CNAME records, which require DNS names.
          Using IP addresses in externalName will cause service resolution failures.

          Kubernetes ExternalName service spec requires a fully qualified domain name (FQDN),
          not an IP address. IP addresses should use a headless Service with Endpoints instead.
        <Function test_deployment_init_containers_timeout>
          Test that init containers have reasonable timeouts implied by their logic.
      <Module test_serviceaccount_naming_regression.py>
        Regression tests for ServiceAccount naming consistency.

        This module prevents regression of the ServiceAccount naming issue discovered
        in CI Run #19311976718 where overlay ServiceAccounts were missing the -sa suffix,
        causing Workload Identity binding failures.

        TDD Approach:
        1. Test that all base ServiceAccounts follow naming convention
        2. Test that overlay ServiceAccounts match base naming (after removing env prefix)
        3. Test that critical components have correct naming
        4. Test validation script catches naming errors

        Regression Prevention:
        - Run #19311976718: ServiceAccount 'staging-openfga-sa' missing annotation
          Root cause: ServiceAccount was named 'openfga' instead of 'openfga-sa'
        - Run #19311976718: 5 deployment validation failures
          Root cause: Naming inconsistencies between base and overlays
        <Class TestBaseServiceAccountNaming>
          Test base ServiceAccount naming conventions.
          <Function test_all_base_serviceaccounts_have_sa_suffix>
            All base ServiceAccounts must end with -sa suffix.
          <Function test_critical_infrastructure_components_exist>
            Critical infrastructure components have ServiceAccounts in base.
        <Class TestOverlayServiceAccountNaming>
          Test overlay ServiceAccount naming matches base.
          <Function test_staging_openfga_serviceaccount_has_sa_suffix>
            Regression test for Run #19311976718.

            The overlay ServiceAccount for OpenFGA must be named 'openfga-sa',
            not 'openfga'. After kustomization patches, it becomes 'staging-openfga-sa'.
          <Function test_staging_keycloak_serviceaccount_has_sa_suffix>
            Regression test for Run #19311976718.

            The overlay ServiceAccount for Keycloak must be named 'keycloak-sa',
            not 'keycloak'. After kustomization patches, it becomes 'staging-keycloak-sa'.
          <Function test_all_overlay_serviceaccounts_match_base_or_allowed>
            All overlay ServiceAccounts must either:
            1. Match a base ServiceAccount (after removing env prefix), OR
            2. Be in the overlay-only allowed list
        <Class TestWorkloadIdentityAnnotations>
          Test that critical ServiceAccounts have Workload Identity annotations.
          <Function test_staging_openfga_has_workload_identity>
            OpenFGA ServiceAccount must have Workload Identity annotation.
          <Function test_staging_keycloak_has_workload_identity>
            Keycloak ServiceAccount must have Workload Identity annotation.
        <Class TestValidationScript>
          Test that the validation script works correctly.
          <Function test_validation_script_passes_on_correct_naming>
            Validation script should pass when all ServiceAccounts are correctly named.
          <Function test_validation_script_is_executable>
            Validation script should be executable.
        <Class TestRegressionPrevention>
          Tests that prevent specific regression scenarios.
          <Function test_cannot_create_serviceaccount_without_sa_suffix_in_overlays>
            Prevent regression: ServiceAccounts in overlays matching base components
            must use the -sa suffix.

            This test would catch if someone creates serviceaccount-openfga.yaml
            with name: openfga instead of name: openfga-sa
      <Module test_staging_deployment_requirements.py>
        Pre-deployment validation tests for staging environment.

        Prevents deployment failures by validating configuration before deploy.

        TDD Context:
        - RED (2025-11-12): Deployment failed due to missing Cloud SQL Proxy sidecar config
        - GREEN: Added validation tests to catch config issues early
        - REFACTOR: These tests run in CI to prevent deployment failures

        Following TDD: Tests written FIRST to catch deployment issues, preventing production incidents.
        <Class TestStagingDeploymentRequirements>
          Test staging deployment meets all requirements.
          <Function test_keycloak_has_cloud_sql_proxy_sidecar>
            Test: Staging Keycloak must have Cloud SQL Proxy sidecar for database connectivity.

            RED (Before Fix):
            - Keycloak deployment missing Cloud SQL Proxy sidecar
            - Pod fails to start - cannot connect to database
            - No validation caught this before deployment

            GREEN (After Fix):
            - Cloud SQL Proxy sidecar added to keycloak-patch.yaml
            - Proper port configuration (5432)
            - Health checks configured

            REFACTOR:
            - This test prevents regression
            - Validates staging overlay has sidecar
          <Function test_keycloak_db_url_points_to_localhost>
            Test: Staging Keycloak must connect to database via Cloud SQL Proxy on localhost.

            Validates KC_DB_URL environment variable points to 127.0.0.1:5432
            (Cloud SQL Proxy sidecar listens on localhost)
          <Function test_cloud_sql_proxy_has_correct_instance_connection_string>
            Test: Cloud SQL Proxy must have correct instance connection string for staging.

            Validates the connection string format and project ID.
          <Function test_cloud_sql_proxy_has_health_checks>
            Test: Cloud SQL Proxy must have liveness and readiness probes.

            Ensures Kubernetes can detect proxy failures and restart if needed.
          <Function test_cloud_sql_proxy_uses_private_ip>
            Test: Cloud SQL Proxy must use --private-ip flag for VPC connectivity.

            Ensures proxy connects to Cloud SQL instance via private IP, not public.
          <Function test_keycloak_has_required_volume_mounts>
            Test: Keycloak container has all required volume mounts.

            NOTE: Staging uses readOnlyRootFilesystem: false due to Quarkus JIT compilation
            requirements (see GitHub issue #10150). This is a documented exception.
            Production uses readOnlyRootFilesystem: true with explicit volume mounts.

            Cross-validates with security hardening requirements.
          <Function test_staging_uses_staging_secrets>
            Test: Staging deployment references staging-specific secrets.

            Prevents accidentally using production secrets in staging.
          <Function test_keycloak_resource_limits_appropriate_for_staging>
            Test: Keycloak resource requests/limits are appropriate for staging environment.

            Staging should have lower limits than production but sufficient for testing.
          <Function test_mcp_server_has_topology_spread_constraints>
            Test: MCP Server deployment must have topology spread constraints for HA.

            RED (Before Fix):
            - No topology spread constraints configured
            - Pods may cluster in single zone
            - Violates high availability requirements

            GREEN (After Fix):
            - Zone constraint: maxSkew 1, whenUnsatisfiable ScheduleAnyway (flexible for single-zone GKE Autopilot)
            - Hostname constraint: maxSkew 2, whenUnsatisfiable DoNotSchedule (spread across nodes)

            REFACTOR:
            - This ensures pods distributed across zones for resilience while supporting single-zone clusters
          <Function test_mcp_server_has_required_pod_anti_affinity>
            Test: MCP Server must have REQUIRED pod anti-affinity for hostname.

            RED (Before Fix):
            - Only preferred anti-affinity (soft constraint)
            - Scheduler may place multiple replicas on same node
            - Reduces blast radius resilience

            GREEN (After Fix):
            - Required anti-affinity for hostname (strict - prevents co-location)
            - Preferred anti-affinity for zone (soft - distributes across zones)

            REFACTOR:
            - This prevents accidental pod co-location on same node
          <Function test_mcp_server_has_startup_probe>
            Test: MCP Server must have startup probe for slow initialization.

            RED (Before Fix):
            - No startup probe configured
            - Liveness probe triggers during startup
            - Pod killed prematurely → CrashLoopBackOff

            GREEN (After Fix):
            - Startup probe with high failureThreshold (30+)
            - Liveness probe doesn't start until startup succeeds

            REFACTOR:
            - This allows MCP Server 60+ seconds to initialize
    <Dir e2e>
      <Module test_real_clients.py>
        Unit tests for real E2E client implementations.

        These tests validate the RealKeycloakAuth and RealMCPClient classes
        work correctly with mock HTTP responses (not actual infrastructure).
        Integration tests in test_full_user_journey.py use actual test infrastructure.
        <Class TestRealKeycloakAuth>
          Unit tests for RealKeycloakAuth client.

          GIVEN: RealKeycloakAuth class for Keycloak authentication
          WHEN: Testing login, refresh, logout, introspect operations
          THEN: Should correctly format requests and handle responses
          <Coroutine test_login_success>
            GIVEN: RealKeycloakAuth client
            WHEN: Calling login with valid credentials
            THEN: Should make POST request to token endpoint and return tokens
          <Coroutine test_context_manager_closes_client>
            GIVEN: real_keycloak_auth context manager
            WHEN: Using async with statement
            THEN: Should properly close HTTP client on exit
        <Class TestRealMCPClient>
          Unit tests for RealMCPClient.

          GIVEN: RealMCPClient class for MCP protocol communication
          WHEN: Testing initialize, list_tools, call_tool operations
          THEN: Should correctly format MCP requests and handle responses
          <Coroutine test_initialize_session>
            GIVEN: RealMCPClient
            WHEN: Calling initialize to start MCP session
            THEN: Should POST to /mcp/initialize with protocol version
          <Coroutine test_list_tools>
            GIVEN: RealMCPClient with initialized session
            WHEN: Calling list_tools
            THEN: Should GET /mcp/tools/list and return tools array
          <Coroutine test_context_manager_closes_client>
            GIVEN: real_mcp_client context manager
            WHEN: Using async with statement
            THEN: Should properly close HTTP client on exit
        <Class TestBackwardsCompatibility>
          Test backwards compatibility aliases.

          GIVEN: Backwards compatibility aliases for gradual migration
          WHEN: Using mock_* names
          THEN: Should map to real_* implementations
          <Function test_class_aliases_exist>
            GIVEN: real_clients module
            WHEN: Checking for backwards compatibility aliases
            THEN: MockKeycloakAuth and MockMCPClient should exist
          <Function test_function_aliases_exist>
            GIVEN: real_clients module
            WHEN: Checking for backwards compatibility function aliases
            THEN: mock_keycloak_auth and mock_mcp_client should exist
      <Module test_real_clients_resilience.py>
        Tests for E2E client resilience and error handling.

        Following TDD principles - these tests are written FIRST before implementing
        the retry logic and enhanced error handling in real_clients.py.
        <Class TestKeycloakAuthErrorHandling>
          Test error handling in RealKeycloakAuth client
          <Coroutine test_timeout_exception_has_clear_message>
            Verify timeout exceptions have actionable error messages
          <Coroutine test_connect_error_has_clear_message>
            Verify connection errors have actionable error messages
          <Coroutine test_http_status_error_has_context>
            Verify HTTP status errors include response context
        <Class TestMCPClientRetryLogic>
          Test retry logic in RealMCPClient
          <Coroutine test_initialize_retries_on_transient_error>
            Verify initialize() retries on transient network errors
          <Coroutine test_list_tools_retries_and_fails_after_max_attempts>
            Verify client retries and fails after exhausting max retry attempts
        <Class TestClientConfiguration>
          Test client configuration and initialization
          <Function test_keycloak_client_has_timeout_configured>
            Verify Keycloak client has timeout to prevent hanging
          <Function test_mcp_client_has_timeout_configured>
            Verify MCP client has timeout to prevent hanging
          <Function test_client_user_agent_identifies_test_suite>
            Verify clients set User-Agent header for observability
    <Package helpers>
      <Module test_async_mock_helpers.py>
        Test suite for AsyncMock helper fixtures.

        This module tests the safe AsyncMock factory functions that prevent
        security vulnerabilities from unconfigured mocks.

        Tests follow TDD principles:
        - RED phase: These tests will fail until helpers are implemented
        - GREEN phase: Implement helpers to make tests pass
        - REFACTOR phase: Apply helpers across codebase

        Security Context:
            Unconfigured AsyncMock instances return truthy MagicMock objects,
            causing authorization checks to incorrectly pass. These helpers
            enforce explicit configuration to prevent such vulnerabilities.

        Related:
            - scripts/check_async_mock_configuration.py (validator)
            - tests/meta/test_async_mock_configuration.py (meta-test)
            - SCIM security bug (commit abb04a6a) - historical incident
        <Class TestConfiguredAsyncMock>
          Test suite for configured_async_mock factory.
          <Function test_returns_async_mock_instance>
            GIVEN: Factory is called
            WHEN: Creating a configured mock
            THEN: Should return AsyncMock instance
          <Function test_default_return_value_is_none>
            GIVEN: Factory is called without arguments
            WHEN: Mock is called (not awaited)
            THEN: Should return a coroutine object
          <Coroutine test_async_call_returns_none_by_default>
            GIVEN: Factory is called without arguments
            WHEN: Mock is awaited in async context
            THEN: Should return None
          <Coroutine test_custom_return_value>
            GIVEN: Factory is called with custom return_value
            WHEN: Mock is awaited
            THEN: Should return configured value
          <Coroutine test_return_value_false_is_preserved>
            GIVEN: Factory is called with return_value=False
            WHEN: Mock is awaited
            THEN: Should return False (not None)

            This test ensures boolean False is preserved,
            critical for authorization checks.
          <Coroutine test_side_effect_exception>
            GIVEN: Factory is called with side_effect exception
            WHEN: Mock is awaited
            THEN: Should raise configured exception
          <Coroutine test_side_effect_callable>
            GIVEN: Factory is called with side_effect callable
            WHEN: Mock is awaited with arguments
            THEN: Should invoke callable with arguments
          <Function test_spec_parameter_honored>
            GIVEN: Factory is called with spec parameter
            WHEN: Accessing mock attributes
            THEN: Should enforce spec restrictions
        <Class TestConfiguredAsyncMockDeny>
          Test suite for configured_async_mock_deny factory (authorization denials).
          <Function test_returns_async_mock_instance>
            GIVEN: Factory is called
            WHEN: Creating a deny mock
            THEN: Should return AsyncMock instance
          <Coroutine test_always_returns_false>
            GIVEN: Factory is called
            WHEN: Mock is awaited
            THEN: Should return False (explicit deny)

            Critical for authorization tests - prevents security bypasses.
          <Coroutine test_returns_false_regardless_of_arguments>
            GIVEN: Deny mock is created
            WHEN: Called with various arguments
            THEN: Should always return False
          <Function test_spec_parameter_honored>
            GIVEN: Factory is called with spec parameter
            WHEN: Accessing mock attributes
            THEN: Should enforce spec restrictions while denying
          <Coroutine test_usage_in_authorization_context>
            GIVEN: Authorization check using deny mock
            WHEN: Checking permissions
            THEN: Should deny access (security-safe default)

            This test demonstrates the security-critical use case.
        <Class TestConfiguredAsyncMockRaise>
          Test suite for configured_async_mock_raise factory (error scenarios).
          <Function test_returns_async_mock_instance>
            GIVEN: Factory is called with exception
            WHEN: Creating a raise mock
            THEN: Should return AsyncMock instance
          <Coroutine test_raises_configured_exception>
            GIVEN: Factory is called with ValueError
            WHEN: Mock is awaited
            THEN: Should raise ValueError with message
          <Coroutine test_raises_different_exception_types>
            GIVEN: Factory is called with various exception types
            WHEN: Mocks are awaited
            THEN: Should raise corresponding exceptions
          <Coroutine test_raises_regardless_of_arguments>
            GIVEN: Raise mock is created
            WHEN: Called with various arguments
            THEN: Should always raise configured exception
          <Function test_spec_parameter_honored>
            GIVEN: Factory is called with spec parameter
            WHEN: Accessing mock attributes
            THEN: Should enforce spec restrictions while raising
          <Coroutine test_usage_in_error_handling_test>
            GIVEN: Error handling test using raise mock
            WHEN: Testing error recovery
            THEN: Should simulate failure scenarios correctly

            This test demonstrates the error-testing use case.
        <Class TestAsyncMockHelperIntegration>
          Integration tests ensuring helpers work together correctly.
          <Coroutine test_mixed_helper_usage>
            GIVEN: Multiple helper mocks in same test
            WHEN: Testing complex scenarios
            THEN: Should behave independently and correctly
          <Coroutine test_authorization_workflow_simulation>
            GIVEN: Complete authorization workflow with helpers
            WHEN: Testing auth flow
            THEN: Should correctly simulate success and failure paths
        <Class TestRedPhaseVerification>
          Meta-test to verify we're in RED phase (helpers not yet implemented).
          <Function test_helpers_not_yet_implemented>
            GIVEN: TDD RED phase
            WHEN: Attempting to import helpers
            THEN: Should fail (confirming RED phase)

            This test will fail once helpers are implemented (GREEN phase).
      <Module test_verifier.py>
        Tests for Output Verifier (LLM-as-Judge Pattern)

        Tests both LLM-based and rules-based verification.
        Uses mocking to avoid actual LLM calls for fast execution.
        <Class TestOutputVerifier>
          Unit tests for OutputVerifier.
          <Function test_initialization>
            Test OutputVerifier initializes with correct config.
          <Coroutine test_verify_response_good_quality>
            Test verification of high-quality response.
          <Coroutine test_verify_response_poor_quality>
            Test verification of low-quality response.
          <Coroutine test_verify_response_with_context>
            Test verification with conversation context.
          <Coroutine test_verify_response_strict_mode>
            Test verification in strict mode (higher threshold).
          <Coroutine test_verify_response_lenient_mode>
            Test verification in lenient mode (lower threshold).
          <Coroutine test_verify_response_error_handling>
            Test that verification fails gracefully on errors.
          <Coroutine test_verify_with_rules_all_pass>
            Test rules-based verification when all rules pass.
          <Coroutine test_verify_with_rules_min_length_fail>
            Test rules-based verification fails on min length.
          <Coroutine test_verify_with_rules_required_keywords>
            Test rules-based verification with required keywords.
          <Coroutine test_verify_with_rules_missing_keywords>
            Test rules-based verification fails on missing keywords.
          <Coroutine test_verify_with_rules_forbidden_keywords>
            Test rules-based verification detects forbidden keywords.
          <Coroutine test_verify_with_rules_code_requirement>
            Test rules-based verification for code inclusion.
          <Function test_parse_verification_judgment_well_formed>
            Test parsing of well-formed LLM judgment.
          <Function test_parse_verification_judgment_malformed>
            Test parsing handles malformed judgments.
          <Function test_get_threshold_for_mode>
            Test threshold calculation for different modes.
          <Function test_build_verification_prompt>
            Test verification prompt building.
        <Class TestVerificationCriterion>
          Test VerificationCriterion enum.
          <Function test_all_criteria_defined>
            Test that all criteria are properly defined.
          <Function test_criterion_values>
            Test criterion enum values.
        <Class TestVerificationResult>
          Test VerificationResult model.
          <Function test_verification_result_creation>
            Test creating VerificationResult.
          <Function test_verification_result_validation>
            Test VerificationResult validation.
        <Class TestConvenienceFunctions>
          Test convenience functions.
          <Coroutine test_verify_output_convenience>
            Test verify_output convenience function.
    <Package infrastructure>
      <Module test_app_factory.py>
        TDD Tests for FastAPI App Factory

        These tests define the behavior we want after extracting infrastructure
        concerns (FastAPI setup, middleware, CORS) from server modules.

        Following TDD:
        1. Write tests first (this file) - RED
        2. Implement app factory - GREEN
        3. Verify no regressions - REFACTOR
        <Class TestAppFactory>
          Test the FastAPI app factory function
          <Function test_create_app_basic>
            Test creating basic FastAPI app
          <Function test_create_app_with_container>
            Test creating app with container
          <Function test_create_app_with_settings>
            Test creating app with custom settings
          <Function test_create_app_has_health_endpoint>
            Test that app has health check endpoint
          <Function test_create_app_has_cors_middleware>
            Test that app has CORS middleware configured
      <Module test_ci_workflows.py>
        Infrastructure Tests: CI/CD Workflow Configuration Validation

        Tests validate that GitHub Actions workflows are properly configured:
        - Terraform binary available for pre-commit hooks
        - Tool caching for performance
        - Proper error handling and verbosity
        - Helm dependency management

        TDD Approach:
        - RED: Tests fail initially (configurations missing)
        - GREEN: Tests pass after implementing workflow fixes
        - REFACTOR: Improve workflow efficiency while maintaining test coverage
        <Class TestCIWorkflowTerraformSetup>
          Test that CI workflow properly sets up Terraform for pre-commit hooks.

          Issue: Pre-commit hooks require Terraform but CI doesn't install it.
          Fix: Add hashicorp/setup-terraform@v3 step before pre-commit run.
          <Function test_precommit_hooks_require_terraform>
            Verify that pre-commit config includes Terraform hooks.

            This confirms the need for Terraform in CI.
          <Function test_ci_precommit_job_exists>
            Validate that CI workflow has a pre-commit job.
          <Function test_ci_precommit_job_installs_terraform>
            Validate that pre-commit job installs Terraform.

            Issue: Pre-commit Terraform hooks fail if terraform binary is not available.
            Fix: Add hashicorp/setup-terraform@v3 step before running pre-commit.
          <Function test_terraform_setup_before_precommit_run>
            Validate that Terraform setup occurs before pre-commit run.

            Requirement: Tools must be installed before pre-commit hooks execute.
          <Function test_terraform_version_consistency>
            Validate that Terraform version is consistent across workflows.

            Requirement: All workflows should use the same Terraform version.
        <Class TestDeploymentValidationWorkflow>
          Test deployment validation workflow configuration.

          Validates:
          - Helm dependency management
          - kubeconform verbosity
          - Proper error handling
          <Function test_helm_dependency_update_before_lint>
            Validate that helm dependency update runs before helm lint/template.

            Requirement: Vendor charts must be downloaded before validation.
          <Function test_kubeconform_has_verbose_on_failure>
            Validate that kubeconform re-runs with verbose output on failure.

            Issue: kubeconform -summary only shows "Errors: 3" without details.
            Fix: Re-run with -verbose or -output json on failure.
          <Function test_kubeconform_uses_pipefail>
            Validate that kubeconform steps use 'set -o pipefail'.

            Requirement: Pipeline failures should propagate exit codes.
        <Class TestWorkflowPerformanceOptimizations>
          Test workflow performance optimizations.

          Validates:
          - Tool caching
          - Conditional artifact downloads
          - Efficient dependency installation
          <Function test_download_artifact_has_continue_on_error>
            Validate that download-artifact has continue-on-error or conditional.

            Issue: First run emits warnings when no historical coverage exists.
            Fix: Add continue-on-error: true or if: condition to suppress noise.
          <Function test_workflows_cache_expensive_tools>
            Validate that workflows cache expensive tool downloads.

            Recommendation: Cache helm, kubeconform, kube-score binaries.
        <Class TestStagingDeployWorkflow>
          Test staging deployment workflow configuration.

          Validates:
          - Trivy scan configuration
          - .trivyignore usage
          <Function test_trivy_scan_uses_trivyignores>
            Validate that Trivy scan uses .trivyignore file.

            Requirement: Document suppressed findings with justifications.
          <Function test_staging_trivyignore_exists_and_documented>
            Validate that .trivyignore exists and has documentation comments.

            Requirement: All suppressions must have justifications.
      <Module test_database_ha.py>
        Test suite for database high availability configurations.

        Tests cloud-managed PostgreSQL instances (CloudSQL, RDS, Azure Database)
        to ensure proper HA setup, failover capabilities, and backup configurations.

        Following TDD principles: Write tests first, then ensure infrastructure meets requirements.
        <Class TestDatabaseHA>
          Test database HA configurations across cloud providers.
          <Function test_cloudsql_has_regional_availability>
            Test CloudSQL is configured for regional (multi-zone) HA.
          <Function test_cloudsql_has_automated_backups>
            Test CloudSQL has automated backups enabled.
          <Function test_cloudsql_has_read_replicas>
            Test CloudSQL supports read replicas for HA.
          <Function test_rds_multi_az_enabled>
            Test RDS is configured for Multi-AZ deployment.
          <Function test_rds_has_automated_backups>
            Test RDS has automated backups configured.
          <Function test_rds_has_encryption_at_rest>
            Test RDS has encryption at rest enabled.
          <Function test_rds_has_enhanced_monitoring>
            Test RDS has enhanced monitoring enabled.
          <Function test_azure_database_module_exists>
            Test Azure Database for PostgreSQL module exists.
          <Function test_azure_database_has_ha_configuration>
            Test Azure Database has zone-redundant HA.
          <Function test_azure_database_has_geo_redundant_backup>
            Test Azure Database has geo-redundant backup.
          <Function test_helm_chart_supports_external_database>
            Test Helm chart can be configured with external database.
          <Function test_helm_chart_has_cloudsql_proxy_sidecar>
            Test Helm chart supports CloudSQL proxy sidecar.
          <Function test_database_connection_pooling_configured>
            Test database connection pooling (PgBouncer) is available.
      <Module test_terraform_security.py>
        Infrastructure Tests: Terraform Security Configuration Validation

        Tests validate that Terraform modules follow security best practices:
        - Encryption with Customer Managed Keys (CMK) in production
        - Purge protection enabled by default
        - Network access restrictions
        - Deletion protection for state storage
        - Secure defaults that pass Checkov compliance scans

        TDD Approach:
        - RED: Tests fail initially (secure defaults not implemented)
        - GREEN: Tests pass after implementing security fixes
        - REFACTOR: Improve code quality while maintaining test coverage
        <Class TestAzureKeyVaultSecurityDefaults>
          Test Azure Key Vault module security defaults.

          Validates:
          - CKV_AZURE_42: Purge protection enabled
          - CKV_AZURE_189: Network ACLs configured with deny-by-default
          <Function test_purge_protection_enabled_by_default>
            CKV_AZURE_42: Purge protection should be enabled by default.

            Requirement: Prevent accidental deletion of key vaults and secrets.
            Breaking Change: Yes (was false, now true)
          <Function test_network_default_action_deny>
            CKV_AZURE_189: Network default action should be 'Deny'.

            Requirement: Deny all access by default, require explicit IP allowlisting.
            Breaking Change: Yes (was 'Allow', now 'Deny')
          <Function test_allowed_ip_ranges_variable_exists>
            Validate that allowed_ip_ranges variable exists for IP whitelisting.

            Requirement: Support environment-specific IP allowlists.
          <Function test_network_acls_use_allowed_ip_ranges>
            Validate that network_acls block uses allowed_ip_ranges variable.

            Requirement: IP allowlist must be configurable via variable.
        <Class TestAWSSecretsManagerSecurity>
          Test AWS Secrets Manager module security configuration.

          Validates:
          - CKV_AWS_149: CMK encryption in production
          - Recovery window configuration
          <Function test_kms_key_id_variable_exists>
            CKV_AWS_149: Validate kms_key_id variable exists for CMK encryption.

            Requirement: Support CMK encryption (production) and AWS-managed keys (dev/staging).
          <Function test_recovery_window_variable_exists>
            Validate recovery_window_in_days variable exists.

            Requirement: Configurable recovery window (0-30 days).
          <Function test_secret_uses_kms_key_id>
            Validate that aws_secretsmanager_secret uses kms_key_id variable.

            Requirement: CMK encryption when kms_key_id is provided.
          <Function test_secret_uses_recovery_window>
            Validate that aws_secretsmanager_secret uses recovery_window_in_days variable.

            Requirement: Configurable recovery window.
        <Class TestDynamoDBBackendSecurity>
          Test DynamoDB backend security configuration.

          Validates:
          - Deletion protection enabled
          - KMS encryption in production
          <Function test_deletion_protection_variable_exists>
            Validate deletion_protection_enabled variable exists.

            Requirement: Prevent accidental deletion of state lock table.
          <Function test_deletion_protection_enabled_by_default>
            Validate deletion protection is enabled by default.

            Requirement: Protect Terraform state from accidental deletion.
          <Function test_kms_key_arn_variable_exists>
            Validate kms_key_arn variable exists for CMK encryption.

            Requirement: Support CMK encryption in production.
          <Function test_dynamodb_uses_deletion_protection>
            Validate DynamoDB table uses deletion_protection_enabled.

            Requirement: Apply deletion protection configuration.
          <Function test_dynamodb_server_side_encryption_uses_kms>
            Validate DynamoDB server_side_encryption uses KMS key variable.

            Requirement: Support CMK encryption when kms_key_arn is provided.
        <Class TestS3BackendSecurity>
          Test S3 backend bucket security configuration.

          Validates:
          - Logging configuration
          - Versioning with MFA delete (documented)
          <Function test_s3_bucket_logging_configured>
            Validate S3 bucket logging is configured.

            Requirement: Audit access to Terraform state bucket.
          <Function test_mfa_delete_documented>
            Validate that MFA delete requirement is documented.

            Requirement: Document MFA delete as manual post-deployment step.
            Note: MFA delete requires root account and cannot be fully automated in Terraform.
        <Class TestCheckovCompliance>
          Integration test: Run Checkov on Terraform modules to validate compliance.

          This test runs actual Checkov scans to ensure all security controls pass.
          <Function test_checkov_azure_secrets_compliance>
            Run Checkov on Azure secrets module (production profile).

            Expected to pass:
            - CKV_AZURE_42: Purge protection enabled
            - CKV_AZURE_189: Network ACLs deny-by-default
          <Function test_checkov_aws_secrets_compliance>
            Run Checkov on AWS secrets module (production profile).

            Expected to pass:
            - CKV_AWS_149: CMK encryption available
          <Function test_checkov_backend_setup_compliance>
            Run Checkov on backend setup (Terraform state storage).

            Expected to pass:
            - DynamoDB deletion protection
            - S3 bucket security controls
      <Module test_topology_spread.py>
        Test suite for topology spread constraints and zone-based pod spreading.

        Tests that pods are distributed across availability zones for high availability.

        Following TDD principles: Write tests first.
        <Class TestTopologySpreadConstraints>
          Test topology spread constraints configuration.
          <Function test_base_deployment_has_topology_spread_constraints>
            Test base deployment has topologySpreadConstraints defined.
          <Function test_topology_spread_uses_zone_distribution>
            Test topology spread constraints use zone topology key.
          <Function test_topology_spread_has_reasonable_max_skew>
            Test maxSkew is set appropriately for zone distribution.

            Zone-based spreading should have maxSkew=1 for even distribution across
            availability zones. Hostname-based spreading can have higher maxSkew values
            to allow more flexibility in node placement.
          <Function test_topology_spread_has_label_selector>
            Test topology spread has proper label selector.
          <Function test_helm_chart_has_topology_spread_constraints>
            Test Helm chart deployment template has topology spread.
          <Function test_helm_values_allows_topology_spread_configuration>
            Test Helm values.yaml has topology spread configuration.
        <Class TestPodAntiAffinity>
          Test pod anti-affinity for HA.
          <Function test_base_deployment_has_required_anti_affinity>
            Test deployment uses required anti-affinity for production.
          <Function test_anti_affinity_uses_zone_topology>
            Test anti-affinity uses zone topology key.
        <Class TestMultiZoneSupport>
          Test multi-zone configuration.
          <Function test_terraform_gke_uses_multiple_zones>
            Test GKE Terraform module uses multiple zones.
          <Function test_terraform_eks_uses_multiple_azs>
            Test EKS Terraform module uses multiple AZs.
          <Function test_terraform_aks_module_structure>
            Test AKS Terraform module structure exists (foundational check).
          <Function test_stateful_services_have_topology_spread>
            Test stateful services (Redis, PostgreSQL) have topology spread.
      <Module test_validation.py>
        Comprehensive validation tests for all Kubernetes best practices implementations.

        Tests all 11 items to ensure configurations are valid and complete.
        <Class TestPhase1Validation>
          Validate Phase 1: High Availability & Data Protection.
          <Function test_azure_database_terraform_module_valid>
            Test Azure Database Terraform module is valid.
          <Function test_helm_external_database_configuration>
            Test Helm chart has external database configuration.
          <Function test_topology_spread_constraints_in_base>
            Test topology spread constraints in base deployment.
          <Function test_topology_spread_in_helm_values>
            Test topology spread constraints in Helm values.
          <Function test_velero_configurations_exist>
            Test Velero configurations for all cloud providers.
          <Function test_velero_backup_schedules>
            Test Velero backup schedules are configured.
        <Class TestPhase2Validation>
          Validate Phase 2: Security Hardening.
          <Function test_istio_enabled_in_helm_values>
            Test Istio service mesh is enabled in Helm values.
          <Function test_istio_config_has_mtls_strict>
            Test Istio configuration has mTLS STRICT mode.
          <Function test_pod_security_standards_in_namespace>
            Test Pod Security Standards are enforced.
          <Function test_istio_injection_enabled_in_namespace>
            Test Istio injection is enabled in namespace.
          <Function test_network_policies_exist_for_all_services>
            Test network policies exist for all services.
          <Function test_network_policies_have_ingress_egress>
            Test network policies have both ingress and egress rules.
        <Class TestPhase3Validation>
          Validate Phase 3: Observability & Cost Management.
          <Function test_loki_stack_configuration_exists>
            Test Loki stack configuration exists.
          <Function test_loki_retention_configured>
            Test Loki has 30-day retention.
          <Function test_resource_quota_exists>
            Test ResourceQuota configuration exists.
          <Function test_limit_range_exists>
            Test LimitRange configuration exists.
          <Function test_resource_quota_has_cpu_memory_limits>
            Test ResourceQuota has CPU and memory limits.
          <Function test_kubecost_configuration_exists>
            Test Kubecost configuration exists.
          <Function test_kubecost_cloud_cost_enabled>
            Test Kubecost has cloud cost monitoring enabled.
        <Class TestPhase4Validation>
          Validate Phase 4: Infrastructure Optimization.
          <Function test_karpenter_terraform_module_exists>
            Test Karpenter Terraform module exists.
          <Function test_karpenter_iam_roles_configured>
            Test Karpenter IAM roles are configured.
          <Function test_karpenter_provisioners_exist>
            Test Karpenter provisioner configurations exist.
          <Function test_karpenter_spot_interruption_handling>
            Test Karpenter has spot interruption handling.
          <Function test_vpa_configurations_exist>
            Test VPA configurations exist for all stateful services.
          <Function test_vpa_update_modes_configured>
            Test VPA update modes are properly configured.
          <Function test_vpa_resource_constraints>
            Test VPA has min/max resource constraints.
        <Class TestDocumentation>
          Validate documentation is complete.
          <Function test_implementation_guide_exists>
            Test implementation guide exists.
          <Function test_implementation_summary_exists>
            Test implementation summary exists.

            Implementation summaries may be in docs/, terraform/, or session-specific
            files depending on the context.
          <Function test_restore_procedure_exists>
            Test restore procedure documentation exists.
        <Class TestYAMLSyntax>
          Validate all YAML files have correct syntax.
          <Function test_all_yaml_files_valid>
            Test all YAML files can be parsed.
    <Package integration>
      <Package api>
        <Module test_api_keys_endpoints.py>
          Tests for API Key Management REST Endpoints

          Tests all API key operations including creation, listing, rotation, revocation,
          and the internal validation endpoint used by Kong for API key→JWT exchange.

          See ADR-0034 for API key to JWT exchange pattern.
          <Class TestCreateAPIKey>
            Tests for POST /api/v1/api-keys/
            <Function test_create_api_key_success>
              Test successful API key creation
            <Function test_create_api_key_custom_expiration>
              Test API key creation with custom expiration
            <Function test_create_api_key_max_keys_exceeded>
              Test API key creation when user has reached the limit (5 keys)
            <Function test_create_api_key_missing_name>
              Test API key creation without required name field
            <Function test_create_api_key_invalid_expiration>
              Test API key creation with invalid expiration days
          <Class TestListAPIKeys>
            Tests for GET /api/v1/api-keys/
            <Function test_list_api_keys_success>
              Test successful listing of user's API keys
            <Function test_list_api_keys_empty>
              Test listing when user has no API keys
          <Class TestRotateAPIKey>
            Tests for POST /api/v1/api-keys/{key_id}/rotate
            <Function test_rotate_api_key_success>
              Test successful API key rotation
            <Function test_rotate_api_key_not_found>
              Test rotating non-existent or unauthorized API key
            <Function test_rotate_api_key_another_users_key>
              Test rotating another user's API key (should fail)
          <Class TestRevokeAPIKey>
            Tests for DELETE /api/v1/api-keys/{key_id}
            <Function test_revoke_api_key_success>
              Test successful API key revocation
            <Function test_revoke_api_key_idempotent>
              Test revoking already revoked key is idempotent
          <Class TestValidateAPIKey>
            Tests for POST /api/v1/api-keys/validate (Kong plugin)
            <Function test_validate_api_key_success>
              Test successful API key validation and JWT exchange
            <Function test_validate_api_key_missing_header>
              Test validation without X-API-Key header
            <Function test_validate_api_key_invalid>
              Test validation with invalid API key
            <Function test_validate_api_key_expired>
              Test validation with expired API key
            <Function test_validate_api_key_jwt_issuance_fails>
              Test validation when JWT issuance fails
            <Function test_validate_api_key_not_in_openapi_schema>
              Test that validate endpoint is excluded from public API docs
          <Class TestAPIKeyEndpointAuthorization>
            Tests for endpoint authorization (JWT required)
            <Function test_create_without_auth>
              Test creating API key without authentication fails
            <Function test_list_without_auth>
              Test listing API keys without authentication fails
        <Module test_health.py>
          TDD Tests for Health Check and Startup Validation

          Tests validate that health checks detect all critical system issues
          identified in OpenAI Codex security audit.
          <Class TestObservabilityValidation>
            Test observability validation checks
            <Function test_validate_observability_when_initialized>
              Test validation passes when observability is initialized
            <Function test_validate_observability_when_not_initialized>
              Test validation fails when observability not initialized
          <Class TestSessionStoreValidation>
            Test session store validation checks
            <Function test_validate_session_store_when_token_mode>
              Test validation passes when using token auth (no sessions)
            <Function test_validate_session_store_when_properly_registered_memory>
              Test validation passes when memory session store registered
            <Function test_validate_session_store_when_miswired_redis>
              Test validation fails when Redis configured but using fallback
          <Class TestAPICacheValidation>
            Test API key cache validation checks
            <Function test_validate_cache_when_disabled>
              Test validation passes when caching disabled
            <Function test_validate_cache_when_enabled_and_configured>
              Test validation passes when caching properly configured
            <Function test_validate_cache_when_enabled_but_no_redis>
              Test validation passes when caching enabled but no Redis URL
            <Function test_validate_cache_when_invalid_ttl>
              Test validation fails when TTL is invalid
          <Class TestStartupValidation>
            Test complete startup validation
            <Function test_run_startup_validation_all_healthy>
              Test startup validation passes when all systems healthy
            <Function test_run_startup_validation_fails_on_critical_error>
              Test startup validation raises SystemValidationError on failure
            <Function test_run_startup_validation_logs_warnings>
              Test startup validation logs warnings for non-critical issues
          <Class TestHealthCheckEndpoint>
            Test health check HTTP endpoint
            <Function test_health_endpoint_returns_200_when_healthy>
              Test health endpoint returns 200 when all systems healthy
            <Function test_health_endpoint_shows_degraded_with_warnings>
              Test health endpoint shows degraded status with warnings
            <Function test_health_endpoint_shows_unhealthy_with_errors>
              Test health endpoint shows unhealthy status with errors
        <Module test_openapi_compliance.py>
          OpenAPI Compliance Tests

          Validates that API endpoints match OpenAPI specification.
          <Class TestOpenAPIStructure>
            Test OpenAPI schema structure
            <Function test_schema_has_required_fields>
              OpenAPI schema must have required top-level fields
            <Function test_openapi_version_is_3>
              OpenAPI version should be 3.x
            <Function test_info_section_complete>
              Info section must have title, version, description
            <Function test_has_paths_defined>
              Schema must define at least one path
            <Function test_has_components_section>
              Schema should have components section for reusable schemas
          <Class TestEndpointDocumentation>
            Test that all endpoints are properly documented
            <Function test_all_endpoints_have_summary>
              All endpoints should have summary or description
            <Function test_all_endpoints_have_responses>
              All endpoints must define responses
            <Function test_all_endpoints_have_tags>
              All endpoints should be tagged for organization
          <Class TestSchemaDefinitions>
            Test request/response schema definitions
            <Function test_all_schemas_have_type>
              All schema definitions must specify a type
            <Function test_object_schemas_have_properties>
              Object schemas should define properties
            <Function test_required_fields_exist_in_properties>
              Required fields must be defined in properties
          <Class TestResponseSchemas>
            Test response schema compliance
            <Function test_success_responses_have_schema>
              2xx responses should have schema definitions
            <Function test_error_responses_documented>
              Common error responses should be documented
          <Class TestBreakingChanges>
            Test for API breaking changes
            <Function test_no_breaking_changes_from_baseline>
              Detect breaking changes compared to baseline schema
          <Class TestSecuritySchemes>
            Test API security documentation
            <Function test_security_schemes_defined>
              Security schemes should be defined if API is protected
            <Function test_protected_endpoints_documented>
              Protected endpoints should document required auth
        <Module test_pagination.py>
          Tests for Standardized Pagination

          Validates that all list endpoints follow consistent pagination patterns for
          production-grade API behavior.

          Following TDD: These tests define the expected pagination behavior (RED phase).
          <Class TestPaginationModels>
            Test that pagination models are properly defined
            <Function test_pagination_params_model_exists>
              PaginationParams model should exist for request parsing
            <Function test_paginated_response_model_exists>
              PaginatedResponse model should exist for response wrapping
            <Function test_pagination_metadata_model_exists>
              PaginationMetadata model should exist for pagination info
          <Class TestPaginationDefaults>
            Test default pagination values
            <Function test_pagination_params_defaults>
              PaginationParams should have sensible defaults
            <Function test_pagination_max_page_size>
              PaginationParams should enforce maximum page size
          <Class TestPaginationOpenAPISchema>
            Test that pagination is documented in OpenAPI schema
            <Function test_pagination_params_in_openapi>
              Pagination parameters should be documented in OpenAPI schema
            <Function test_pagination_metadata_in_openapi>
              Pagination metadata should be documented in OpenAPI schema
          <Class TestPaginationResponse>
            Test pagination response structure
            <Function test_paginated_response_structure>
              PaginatedResponse should have standard structure
            <Function test_paginated_response_calculates_total_pages>
              PaginationMetadata should correctly calculate total_pages
          <Class TestPaginationLinks>
            Test that pagination includes navigation links
            <Function test_pagination_metadata_has_link_fields>
              PaginationMetadata should include next/prev page indicators
        <Module test_service_principals_endpoints.py>
          Tests for Service Principal Management REST Endpoints

          Tests all service principal operations including creation, listing, secret rotation,
          deletion, and user association for permission inheritance.

          See ADR-0033 for service principal design decisions.
          <Class TestCreateServicePrincipal>
            Tests for POST /api/v1/service-principals/
            <Function test_create_service_principal_success>
              Test successful service principal creation
            <Function test_create_service_principal_minimal>
              Test service principal creation with minimal fields
            <Function test_create_service_principal_service_account_mode>
              Test creating service principal with service_account_user mode
            <Function test_create_service_principal_invalid_auth_mode>
              Test creating service principal with invalid authentication mode
            <Function test_create_service_principal_duplicate_id>
              Test creating service principal when ID already exists
            <Function test_create_service_principal_missing_secret>
              Test error handling when secret generation fails
          <Class TestListServicePrincipals>
            Tests for GET /api/v1/service-principals/
            <Function test_list_service_principals_success>
              Test successful listing of user's service principals
            <Function test_list_service_principals_empty>
              Test listing when user has no service principals
          <Class TestGetServicePrincipal>
            Tests for GET /api/v1/service-principals/{service_id}
            <Function test_get_service_principal_success>
              Test successful retrieval of service principal details
            <Function test_get_service_principal_not_found>
              Test retrieving non-existent service principal
            <Function test_get_service_principal_unauthorized>
              Test retrieving another user's service principal
          <Class TestRotateServicePrincipalSecret>
            Tests for POST /api/v1/service-principals/{service_id}/rotate-secret
            <Function test_rotate_secret_success>
              Test successful secret rotation
            <Function test_rotate_secret_not_found>
              Test rotating secret for non-existent service principal
            <Function test_rotate_secret_unauthorized>
              Test rotating secret for another user's service principal
          <Class TestDeleteServicePrincipal>
            Tests for DELETE /api/v1/service-principals/{service_id}
            <Function test_delete_service_principal_success>
              Test successful service principal deletion
            <Function test_delete_service_principal_not_found>
              Test deleting non-existent service principal
            <Function test_delete_service_principal_unauthorized>
              Test deleting another user's service principal
          <Class TestAssociateServicePrincipalWithUser>
            Tests for POST /api/v1/service-principals/{service_id}/associate-user
            <Function test_associate_with_user_success>
              Test successful user association (requires admin role)
            <Function test_associate_with_user_not_found>
              Test associating non-existent service principal
            <Function test_associate_with_user_unauthorized>
              Test associating another user's service principal
            <Function test_associate_with_user_update_fails>
              Test error when retrieving updated SP fails after association (requires admin role)
      <Package compliance>
        <Module test_hipaa.py>
          Tests for HIPAA compliance controls

          Covers HIPAA Security Rule technical safeguards:
          - 164.312(a)(2)(i): Emergency Access Procedure
          - 164.312(b): Audit Controls
          - 164.312(c)(1): Integrity Controls
          <Class TestEmergencyAccess>
            Test emergency access functionality (HIPAA 164.312(a)(2)(i))
            <Coroutine test_grant_emergency_access_success>
              Test granting emergency access
            <Coroutine test_grant_emergency_access_validates_request>
              Test emergency access request validation
            <Coroutine test_grant_emergency_access_with_custom_duration>
              Test emergency access with custom duration
            <Coroutine test_revoke_emergency_access>
              Test revoking emergency access
            <Coroutine test_revoke_nonexistent_grant>
              Test revoking non-existent grant
            <Coroutine test_check_emergency_access_active>
              Test checking active emergency access
            <Coroutine test_check_emergency_access_revoked>
              Test that revoked grants are not returned
            <Coroutine test_check_emergency_access_no_grants>
              Test checking emergency access with no grants
          <Class TestPHIAuditLogging>
            Test PHI audit logging (HIPAA 164.312(b))
            <Coroutine test_log_phi_access_success>
              Test logging successful PHI access
            <Coroutine test_log_phi_access_failure>
              Test logging failed PHI access
            <Coroutine test_log_phi_access_with_optional_fields>
              Test logging PHI access with minimal fields
          <Class TestDataIntegrity>
            Test data integrity controls (HIPAA 164.312(c)(1))
            <Function test_generate_checksum>
              Test generating HMAC checksum
            <Function test_verify_checksum_valid>
              Test verifying valid checksum
            <Function test_verify_checksum_invalid>
              Test verifying invalid checksum (tampered data)
            <Function test_checksum_with_different_secrets>
              Test that different secrets produce different checksums
            <Function test_checksum_constant_time_comparison>
              Test that checksum verification uses constant-time comparison
          <Class TestHIPAAControlsGlobal>
            Test global HIPAA controls instance management
            <Function test_get_hipaa_controls_singleton>
              Test that get_hipaa_controls returns singleton
            <Function test_set_hipaa_controls>
              Test setting custom HIPAA controls instance
          <Class TestPydanticModels>
            Test Pydantic model validation
            <Function test_emergency_access_request_validation>
              Test EmergencyAccessRequest validation
            <Function test_emergency_access_request_reason_too_short>
              Test validation fails for short reason
            <Function test_emergency_access_request_duration_validation>
              Test duration validation (1-24 hours)
            <Function test_phi_audit_log_model>
              Test PHIAuditLog model
            <Function test_data_integrity_check_model>
              Test DataIntegrityCheck model
          <Class TestHIPAAIntegration>
            Integration tests for HIPAA controls
            <Coroutine test_full_emergency_access_workflow>
              Test complete emergency access workflow
            <Coroutine test_integrity_protection_workflow>
              Test data integrity protection workflow
      <Package contract>
        <Module test_mcp_contract.py>
          Contract tests for MCP protocol compliance

          Validates that the MCP server strictly adheres to the Model Context Protocol specification.
          <Class TestJSONRPCFormat>
            Test basic JSON-RPC 2.0 format compliance
            <Function test_request_has_required_fields>
              All MCP requests must follow JSON-RPC 2.0 format
            <Function test_response_has_required_fields>
              All MCP responses must follow JSON-RPC 2.0 format
            <Function test_error_response_format>
              Error responses must have code and message
            <Function test_response_cannot_have_both_result_and_error>
              JSON-RPC responses must have either result or error, not both
            <Function test_jsonrpc_version_must_be_2_0>
              JSON-RPC version must be exactly "2.0"
          <Class TestInitializeContract>
            Test initialize handshake contract
            <Function test_initialize_request_format>
              Initialize request must include protocolVersion, clientInfo, and capabilities
            <Function test_initialize_request_missing_clientinfo>
              Initialize request without clientInfo should fail
            <Function test_initialize_response_format>
              Initialize response must include serverInfo and capabilities
            <Function test_initialize_response_missing_capabilities>
              Initialize response must include capabilities
          <Class TestToolsContract>
            Test tools/list and tools/call contracts
            <Function test_tools_list_request_format>
              tools/list request should follow MCP spec
            <Function test_tools_list_response_format>
              tools/list response must contain array of tools
            <Function test_tool_must_have_name_and_description>
              Each tool must have name and description
            <Function test_tools_call_request_format>
              tools/call request must include name and arguments
            <Function test_tools_call_response_format>
              tools/call response must contain content array
            <Function test_tools_call_response_must_have_content>
              tools/call response must include content field
          <Class TestResourcesContract>
            Test resources/list contract
            <Function test_resources_list_request_format>
              resources/list request should follow MCP spec
            <Function test_resources_list_response_format>
              resources/list response must contain array of resources
            <Function test_resource_must_have_uri_and_name>
              Each resource must have uri and name
          <Class TestSchemaCompleteness>
            Meta-tests ensuring our schemas are comprehensive
            <Function test_all_required_schemas_defined>
              Verify all key MCP message types have schemas
            <Function test_schemas_are_valid_json_schema>
              All schemas should be valid JSON Schema Draft 7
      <Package core>
        <Module test_container.py>
          TDD Test Suite for Dependency Injection Container

          Following TDD principles:
          1. Write tests first (this file)
          2. Run tests (they should fail - Red)
          3. Implement minimum code to pass (Green)
          4. Refactor (Refactor)

          These tests define the behavior we want from the container pattern.
          <Class TestContainerConfiguration>
            Test container configuration
            <Function test_container_config_defaults>
              Test that container config has sensible defaults
            <Function test_container_config_test_mode>
              Test that test mode is detected from environment
            <Function test_container_config_production_mode>
              Test that production mode enforces requirements
      <Package patterns>
        <Module test_supervisor.py>
          Tests for Supervisor Pattern

          Following TDD best practices with comprehensive coverage.
          <Function test_supervisor_initialization>
            Test supervisor can be initialized with agents.
          <Function test_supervisor_custom_prompt>
            Test supervisor accepts custom prompt.
          <Function test_supervisor_state_creation>
            Test SupervisorState can be created with required fields.
          <Function test_supervisor_build_graph>
            Test supervisor builds valid LangGraph.
          <Function test_supervisor_agent_error_handling>
            Test supervisor handles agent errors gracefully.
          <Function test_supervisor_empty_agents>
            Test supervisor handles empty agent dict.
          <Function test_supervisor_calls_agents_correctly>
            Test supervisor calls agent functions with correct arguments.
        <Module test_swarm.py>
          Tests for Swarm Pattern

          Comprehensive test coverage following TDD best practices.
          <Function test_swarm_initialization>
            Test swarm can be initialized with agents.
          <Function test_swarm_state_creation>
            Test SwarmState can be created.
          <Function test_swarm_build_graph>
            Test swarm builds valid LangGraph with parallel structure.
          <Function test_consensus_score_identical_results>
            Test consensus score is high when agents agree.
          <Function test_consensus_score_divergent_results>
            Test consensus score is lower when agents disagree.
          <Function test_swarm_handles_agent_failures>
            Test swarm continues when one agent fails.
          <Function test_swarm_empty_agents>
            Test swarm validates non-empty agents.
          <Function test_voting_strategy_majority>
            Test voting finds majority opinion.
      <Package property>
        <Module test_auth_properties.py>
          Property-based tests for Authentication and Authorization

          Tests security-critical invariants using Hypothesis.
          <Class TestJWTProperties>
            Property-based tests for JWT authentication
            <Function test_jwt_encode_decode_roundtrip>
              Property: JWT encode/decode should be reversible
            <Function test_jwt_requires_correct_secret>
              Property: JWT verification requires the correct secret key
            <Function test_expired_tokens_rejected>
              Property: Expired tokens should always be rejected
            <Function test_token_expiration_time_honored>
              Property: Token expiration should match requested time
          <Class TestAuthorizationProperties>
            Property-based tests for authorization logic
            <AsyncHypothesisTest test_authorization_is_deterministic>
              Property: Same authorization check should give same result
            <AsyncHypothesisTest test_admin_always_authorized>
              Property: Admin users should always be authorized (fallback mode)
            <AsyncHypothesisTest test_openfga_failure_denies_access>
              Property: OpenFGA errors should deny access (fail-closed)
            <AsyncHypothesisTest test_inactive_users_denied>
              Property: Inactive users should never be authorized
          <Class TestPermissionInheritance>
            Property tests for permission inheritance and transitivity
            <AsyncHypothesisTest test_org_membership_grants_tool_access>
              Property: Organization members should have access to org tools
            <AsyncHypothesisTest test_ownership_implies_all_permissions>
              Property: Resource owners should have all permissions
          <Class TestSecurityInvariants>
            Property tests for critical security invariants
            <Function test_token_payload_not_user_controlled>
              Property: User cannot inject arbitrary claims into JWT
            <Function test_tokens_contain_user_id_not_password>
              Property: JWT should never contain sensitive data like passwords
            <Function test_failed_auth_does_not_leak_info>
              Property: Failed authentication should not reveal whether user exists
        <Module test_llm_properties.py>
          Property-based tests for LLM Factory

          Tests invariants that should hold for all inputs using Hypothesis.
          <Class TestLLMFactoryProperties>
            Property-based tests for LLM Factory
            <Function test_factory_creation_never_crashes>
              Property: Factory creation should never crash with valid inputs
            <Function test_invoke_preserves_message_content>
              Property: Invoke should accept messages without crashing
            <Function test_parameter_override_consistency>
              Property: Parameter overrides should be consistent
            <Function test_fallback_always_tried_on_failure>
              Property: Fallback should always be attempted when primary fails
            <Function test_invoke_handles_different_message_types>
              Property: invoke() should handle all message types for all providers
          <Class TestLLMFactoryEdgeCases>
            Property tests for edge cases and invariants
            <Function test_invoke_handles_empty_message_content>
              Property: invoke() should handle empty message content gracefully
            <Function test_invalid_temperature_outside_range>
              Property: Factory should handle out-of-range temperatures gracefully
            <Function test_invoke_works_with_api_key_for_all_providers>
              Property: invoke() should work with API keys for all providers
            <Function test_invoke_processes_messages_successfully>
              Property: invoke() should process message lists of any length
      <Package security>
        <Module test_accurate_token_counting.py>
          Security tests for accurate token counting (OpenAI Codex Finding #4)

          SECURITY FINDING:
          Token counting drives compaction/truncation but relies on litellm.token_counter
          with fallback of len(text)//4. The default model (gemini-2.5-flash) isn't supported
          by liteLLM counters, causing inaccurate counts that could blow context budgets.

          This test suite validates that:
          1. Token counting is accurate for Gemini models (using Google tokenizer)
          2. Token counting is accurate for OpenAI models (using tiktoken)
          3. Fallback heuristic is replaced with proper error handling
          4. Unsupported models raise clear errors instead of silently miscount

          References:
          - src/mcp_server_langgraph/utils/response_optimizer.py:33-65
          - src/mcp_server_langgraph/core/config.py:119 (gemini-2.5-flash default)
          - CWE-703: Improper Check or Handling of Exceptional Conditions
          <Class TestAccurateTokenCounting>
            Test suite for accurate provider-specific token counting
            <Function test_gemini_token_counting_uses_google_tokenizer>
              SECURITY TEST: Gemini models must use Google's official tokenizer

              Using litellm fallback or len(text)//4 for Gemini is inaccurate and could
              cause context budget overruns or unnecessary truncation.
            <Function test_openai_token_counting_uses_tiktoken>
              SECURITY TEST: OpenAI models must use tiktoken for accurate counting

              GPT models have well-defined tokenization that must be respected.
            <Function test_anthropic_token_counting_for_claude>
              SECURITY TEST: Claude models must use accurate token counting

              Anthropic's Claude models have specific tokenization.
            <Function test_token_counting_deterministic_for_same_text>
              Test that token counting is deterministic (same text = same count)

              This ensures caching and memoization work correctly.
            <Function test_empty_string_returns_zero_tokens>
              Test that empty strings are handled correctly
            <Function test_unicode_text_counted_accurately>
              Test that Unicode/emoji text is counted accurately

              Tokenizers handle Unicode differently than simple character counting.
            <Function test_very_long_text_counted_accurately>
              SECURITY TEST: Long text (>10k chars) must be counted accurately

              This is critical for context budget management in production.
        <Module test_api_key_indexed_lookup.py>
          Security tests for API key indexed lookup (OpenAI Codex Finding #5)

          SECURITY FINDING:
          On every cache miss, API key validation walks the entire Keycloak user list
          (O(total users × keys) scan). This won't scale beyond a few hundred accounts
          and negates the Redis cache advantage.

          This test suite validates that:
          1. API key hashes are stored in indexed Keycloak user attributes
          2. Validation uses Keycloak indexed search (O(1) lookup)
          3. Cache misses don't trigger full user enumeration
          4. Performance is acceptable for large user bases

          References:
          - src/mcp_server_langgraph/auth/api_keys.py:252-338 (validate_and_get_user)
          - ADR-0034: API Key Caching Strategy
          - CWE-407: Inefficient Algorithmic Complexity
          <Class TestAPIKeyIndexedLookup>
            Test suite for O(1) API key lookup via Keycloak indexed attributes

            Note: Uses xdist_group to prevent parallel execution which causes excessive
            memory consumption (42GB+ RES) due to AsyncMock/MagicMock retention issues.
            <Coroutine test_create_api_key_stores_hash_in_keycloak_attribute>
              SECURITY TEST: When creating API key, hash must be stored in Keycloak user attribute

              This enables O(1) lookup instead of O(n) user enumeration.
            <Coroutine test_validate_uses_indexed_search_not_enumeration>
              SECURITY TEST: API key validation must use Keycloak indexed search

              Should call search_users(query="api_key_hash:...") instead of paginating
              through all users.

              NOTE: This test validates current pagination behavior while documenting
              the desired indexed search approach for future optimization.
            <Coroutine test_cache_miss_triggers_enumeration_with_monitoring>
              SECURITY TEST: Cache miss triggers O(n) enumeration, but with monitoring

              Current implementation uses pagination (O(n) complexity) on cache miss.
              This test validates that:
              1. Redis cache is checked first (primary mitigation - ADR-0034)
              2. Enumeration is logged with performance warnings
              3. Users are counted and logged for monitoring

              Full indexed search implementation is documented for future optimization.
            <Coroutine test_indexed_lookup_performance_with_large_user_base>
              PERFORMANCE TEST: Indexed lookup should complete quickly even with large user base

              With O(1) indexed search, performance should be independent of user count.

              Note: Skipped in pytest-xdist mode to prevent excessive memory consumption.
              Run without -n flag for performance validation.
          <Class TestAPIKeyHashStorage>
            Test suite for API key hash storage and retrieval
            <Coroutine test_multiple_api_keys_per_user_supported>
              Test that users can have multiple API keys (stored as array of hashes)

              Keycloak attributes should support multiple values.
            <Coroutine test_revoked_keys_removed_from_keycloak_attributes>
              Test that revoked API key hashes are removed from Keycloak attributes

              This ensures revoked keys can't be validated even if someone has the old value.
        <Module test_api_key_performance_monitoring.py>
          Security tests for API key performance monitoring (OpenAI Codex Finding #5)

          SECURITY FINDING:
          On every cache miss, API key validation walks the entire Keycloak user list.
          This O(n×m) scan won't scale beyond a few hundred accounts.

          MITIGATION STATUS:
          - PRIMARY: Redis cache provides O(1) lookups (ADR-0034)
          - MONITORING: Added logging to track enumeration events
          - FUTURE: Keycloak indexed attribute search recommended for >1000 users

          This test suite validates monitoring and mitigation measures.

          References:
          - src/mcp_server_langgraph/auth/api_keys.py:283-373 (validate_and_get_user pagination)
          - ADR-0034: API Key Caching Strategy
          - CWE-407: Inefficient Algorithmic Complexity
          <Class TestAPIKeyPerformanceMonitoring>
            Test suite for API key performance monitoring
            <Function test_validate_and_get_user_has_enumeration_warning>
              SECURITY TEST: validate_and_get_user must log warning about O(n) enumeration

              This helps operators identify when performance optimization is needed.
            <Function test_api_keys_module_references_adr_0034>
              Test that api_keys.py references ADR-0034 for cache mitigation

              This ensures operators know about the Redis cache mitigation strategy.
            <Function test_api_key_manager_accepts_redis_client>
              Test that APIKeyManager accepts Redis client for caching

              This is the primary mitigation for O(n) enumeration performance.
            <Function test_cache_enabled_by_default_when_redis_available>
              Test that caching is enabled by default when Redis is available

              This ensures the primary O(n) mitigation is active.
          <Class TestAPIKeyEnumerationDocumentation>
            Test suite ensuring enumeration is documented
            <Function test_docstring_mentions_performance_implications>
              Test that validate_and_get_user docstring mentions performance

              Helps developers understand the importance of cache hits.
            <Function test_source_code_has_performance_comments>
              Test that source code includes performance warnings

              Inline comments help future maintainers understand optimization needs.
        <Module test_no_hardcoded_credentials.py>
          Security tests for hard-coded credentials removal (OpenAI Codex Finding #2)

          SECURITY FINDING:
          InMemoryUserProvider seeds fixed usernames/passwords (alice123, admin123) with
          plaintext storage when bcrypt is absent. This test suite validates that:
          1. No hard-coded credentials exist in the codebase
          2. InMemoryUserProvider starts with empty user database
          3. Users must be explicitly created via API or configuration
          4. No default passwords exist in any code path

          References:
          - src/mcp_server_langgraph/auth/user_provider.py:348-367 (_init_users)
          - CWE-798: Use of Hard-coded Credentials
          <Class TestNoHardcodedCredentials>
            Test suite ensuring no hard-coded credentials exist
            <Function test_inmemory_provider_starts_with_empty_database>
              SECURITY TEST: InMemoryUserProvider must start with empty user database

              No default users should be seeded. All users must be explicitly created.
            <Function test_no_alice_user_exists_by_default>
              SECURITY TEST: No 'alice' user should exist by default

              This was one of the hard-coded credentials (alice/alice123) in the original code.
            <Function test_no_bob_user_exists_by_default>
              SECURITY TEST: No 'bob' user should exist by default

              This was one of the hard-coded credentials (bob/bob123) in the original code.
            <Function test_no_admin_user_exists_by_default>
              SECURITY TEST: No 'admin' user should exist by default

              This was one of the hard-coded credentials (admin/admin123) in the original code.
            <Function test_users_can_be_created_explicitly>
              Test that users can still be created explicitly via add_user() method

              This ensures the removal of hard-coded credentials doesn't break user creation.
            <Function test_add_user_with_hashing_enabled>
              Test that user creation works correctly with password hashing enabled
            <Coroutine test_authentication_fails_without_users>
              Test that authentication properly fails when no users exist

              This ensures the removal of default users doesn't cause crashes.
            <Coroutine test_authentication_succeeds_for_explicitly_created_user>
              Test that authentication works for explicitly created users
          <Class TestCredentialCreationDocumentation>
            Tests to ensure proper documentation for creating test users
            <Function test_readme_contains_user_creation_example>
              Test that README or documentation explains how to create test users

              Since we removed hard-coded credentials, developers need clear instructions.
            <Function test_add_user_method_has_docstring>
              Test that the add_user() method has proper documentation

              This is the recommended way to create users for testing.
        <Module test_scim_service_principal_openfga.py>
          Security tests for SCIM/Service Principal OpenFGA integration (OpenAI Codex Finding #6)

          SECURITY FINDING:
          SCIM and service principal endpoints only guard with ad-hoc role lists and leave
          OpenFGA integration marked TODO. For compliance workloads, these should use
          relation-based checks.

          This test suite validates that:
          1. SCIM endpoints use OpenFGA relation checks (can_provision_users)
          2. Service Principal endpoints use OpenFGA relation checks (can_manage_service_principals)
          3. Ad-hoc role lists are replaced with proper authorization
          4. Deny cases are tested (not just allow cases)

          References:
          - src/mcp_server_langgraph/api/scim.py:66-120 (_require_admin_or_scim_role)
          - src/mcp_server_langgraph/api/service_principals.py:104-124 (_validate_user_association_permission)
          - CWE-863: Incorrect Authorization
          <Class TestSCIMOpenFGAIntegration>
            Test suite for SCIM endpoint OpenFGA authorization
            <Coroutine test_scim_endpoint_uses_openfga_relation_check>
              SECURITY TEST: SCIM endpoints must use OpenFGA relation checks

              Instead of ad-hoc role list ["admin", "scim-provisioner"], should check:
              - OpenFGA relation: user:alice can_provision_users organization:acme
            <Coroutine test_scim_denies_user_without_openfga_relation>
              SECURITY TEST: SCIM should deny users without proper OpenFGA relations

              Even if user has generic "user" role, should be denied if no can_provision_users relation.
            <Function test_scim_function_signature_accepts_openfga_parameter>
              Test that _require_admin_or_scim_role accepts openfga parameter

              This is required for OpenFGA integration.
          <Class TestServicePrincipalOpenFGAIntegration>
            Test suite for Service Principal endpoint OpenFGA authorization
            <Coroutine test_service_principal_uses_openfga_relation_check>
              SECURITY TEST: Service Principal endpoints must use OpenFGA relation checks

              Instead of simple admin check, should verify:
              - user:alice can_manage_service_principals user:bob (delegation)
              - user:alice can_manage_service_principals organization:acme (org-level)
            <Coroutine test_service_principal_denies_without_delegation_relation>
              SECURITY TEST: Service Principal creation should deny without delegation relation

              User can't create SP for another user unless:
              1. They're the same user (self-association), OR
              2. They're admin, OR
              3. They have can_manage_service_principals relation (new)
            <Coroutine test_service_principal_allows_self_association>
              Test that users can always create Service Principals for themselves

              This should work regardless of OpenFGA (basic user self-service).
          <Class TestAdHocRoleChecks>
            Test suite documenting current ad-hoc role checks
            <Function test_scim_currently_uses_role_list>
              Document that SCIM currently uses ad-hoc role list

              Roles: ["admin", "scim-provisioner"]
              Future: Should use OpenFGA relations
            <Function test_service_principal_currently_uses_admin_check>
              Document that Service Principal currently uses simple admin check

              Current: if "admin" in user_roles
              Future: OpenFGA relation checks for delegation
      <Module test_agent.py>
        <Class TestAgentState>
          Test AgentState TypedDict
          <Function test_agent_state_structure>
            Test AgentState can be created with required fields
        <Class TestAgentGraph>
          Test LangGraph agent creation and execution
          <Function test_create_agent_graph>
            Test agent graph is created successfully
          <Coroutine test_route_input_to_respond>
            Test routing to direct response
          <Coroutine test_route_input_to_tools>
            Test routing to tools when keywords detected
          <Coroutine test_route_with_calculate_keyword>
            Test routing detects calculate keyword
          <Coroutine test_agent_with_conversation_history>
            Test agent handles conversation history
          <Coroutine test_checkpointing_works>
            Test conversation checkpointing
          <Function test_state_accumulation>
            Test that messages accumulate in state
          <Coroutine test_agent_without_langsmith>
            Test agent works when LangSmith is not available
          <Coroutine test_agent_with_langsmith_enabled>
            Test agent with LangSmith configuration
          <Coroutine test_routing_with_tool_keywords>
            Test routing detects tool keywords
          <Coroutine test_handles_empty_message_content>
            Test agent handles empty message content gracefully.

            Edge case: Empty string content should not crash the agent.
          <Coroutine test_handles_missing_optional_fields>
            Test agent handles missing optional fields (user_id, request_id).

            Edge case: None values for optional fields should be acceptable.
          <Coroutine test_handles_very_long_conversation_history>
            Test agent handles very long conversation histories.

            Edge case: Many messages should trigger compaction if enabled.
        <Class TestRedisCheckpointerLifecycle>
          Test Redis checkpointer lifecycle management (TDD RED phase).

          Tests written FIRST to ensure proper context manager cleanup.
          Will fail until cleanup hooks are implemented in agent.py
          <Function test_redis_checkpointer_context_manager_cleanup>
            Test Redis checkpointer context manager is properly cleaned up.

            GREEN: Verifies full lifecycle - creation AND cleanup.
          <Function test_redis_checkpointer_stores_context_for_cleanup>
            Test Redis checkpointer stores context manager reference for cleanup.

            GREEN: Verifies context manager reference storage and cleanup capability.
          <Function test_memory_checkpointer_no_cleanup_needed>
            Test MemorySaver doesn't require context manager cleanup
      <Module test_cache_redis_config.py>
        TDD Tests for L2 Cache Redis Configuration

        Tests validate that CacheService properly uses secure Redis settings
        (redis_url, redis_password, redis_ssl) instead of just redis_host and redis_port.

        Critical bug this catches:
        - cache.py:94-120 ignores settings.redis_url, settings.redis_password, settings.redis_ssl
        - This causes L2 cache to silently fall back to L1-only in production

        The correct pattern is demonstrated in dependencies.py:116-125 (API key manager).
        <Class TestCacheServiceRedisConfiguration>
          Test that CacheService uses proper Redis configuration
          <Function test_cache_service_uses_redis_url_pattern>
            CRITICAL: CacheService must use redis.from_url() with full config.

            Bug: cache.py:95-106 uses redis.Redis(host=..., port=...) directly,
            ignoring settings.redis_url, settings.redis_password, and settings.redis_ssl.

            Fix: Use redis.from_url() pattern like API key manager does.
          <Function test_cache_service_honors_redis_password_and_ssl>
            Test that password and SSL settings are properly passed to Redis client.

            This is the pattern used correctly in dependencies.py:189-220 for
            API key manager that should be replicated for L2 cache.
          <Function test_cache_service_falls_back_to_l1_when_redis_unavailable>
            Test graceful degradation when Redis is unavailable.

            Cache should fall back to L1-only instead of crashing.
        <Class TestCacheServiceComparison>
          Compare CacheService pattern with APIKeyManager pattern.

          Demonstrates that APIKeyManager has correct pattern that CacheService should follow.
          <Function test_api_key_manager_redis_pattern_is_correct>
            Document the CORRECT pattern from dependencies.py:116-125.

            This is what CacheService should do:
            redis.from_url(f"{redis_url}/{db}", password=..., ssl=..., decode_responses=True)
          <Function test_cache_service_should_use_same_pattern>
            Test that after fix, CacheService uses same pattern as APIKeyManager.

            Expected behavior after fix:
            - Parse redis_url from settings
            - Append database number to URL correctly using urllib.parse
            - Pass password and ssl parameters

            This test verifies the fix for the Redis URL construction bug where
            simple string concatenation (redis_url + '/' + db) creates malformed URLs:
            - redis://localhost:6379/0 + /2 = redis://localhost:6379/0/2 (wrong!)
            - redis://localhost:6379/ + /2 = redis://localhost:6379//2 (wrong!)

            The correct approach uses urllib.parse to replace the path component.
      <Module test_context_manager_llm.py>
        <Class TestEnhancedNoteExtraction>
          Test suite for LLM-based key information extraction
          <Coroutine test_extract_key_information_llm_success>
            Test successful LLM-based extraction
          <Coroutine test_extract_key_information_llm_empty_categories>
            Test extraction when some categories are empty
          <Coroutine test_extract_key_information_llm_fallback_on_error>
            Test fallback to rule-based extraction on LLM error
          <Coroutine test_parse_extraction_response>
            Test parsing of LLM extraction response
          <Coroutine test_parse_extraction_multiline_items>
            Test parsing multi-line items (should handle gracefully)
          <Coroutine test_extract_prompt_format>
            Test that extraction prompt follows XML structure
          <Coroutine test_extraction_categories_complete>
            Test that all 6 categories are extracted
          <Coroutine test_extraction_case_insensitive_headers>
            Test that category headers are case-insensitive
          <Coroutine test_extract_with_system_messages>
            Test extraction works with system messages in conversation
          <Coroutine test_extraction_metrics_logged>
            Test that extraction logs metrics
          <Coroutine test_extraction_error_metrics_logged>
            Test that extraction errors are logged in metrics
        <Class TestRuleBasedExtraction>
          Test suite for rule-based extraction (fallback)
          <Function test_extract_key_information_decisions>
            Test rule-based extraction of decisions
          <Function test_extract_key_information_requirements>
            Test rule-based extraction of requirements
          <Function test_extract_key_information_issues>
            Test rule-based extraction of issues
          <Function test_extract_key_information_truncation>
            Test that extracted items are truncated to 200 chars
      <Module test_dependencies_wiring.py>
        TDD Tests for Dependency Injection Wiring

        These tests validate that dependency factories properly wire all required
        configuration from settings to client instances. Written FIRST to catch
        configuration bugs that would only surface at runtime.

        Critical bugs these tests catch:
        1. Keycloak admin credentials not passed to KeycloakClient
        2. OpenFGA client created with None store_id/model_id
        3. Service principal manager crashes when OpenFGA is None
        <Class TestOpenFGAClientWiring>
          Test that OpenFGA client validates required configuration
          <Function test_openfga_client_returns_none_when_config_incomplete>
            CRITICAL: OpenFGA client should NOT be created when store_id/model_id are missing.

            Bug: dependencies.py:45-59 always creates OpenFGAClient even when
            store_id=None and model_id=None. SDK will fail on first check_permission() call.

            Fix: Return None when config is incomplete, allowing graceful degradation.
          <Coroutine test_openfga_client_created_when_config_complete>
            OpenFGA client SHOULD be created when all required config is present.
          <Function test_openfga_client_logs_warning_when_returning_none>
            When returning None due to incomplete config, log a clear warning.
        <Class TestServicePrincipalManagerOpenFGAGuards>
          Test that ServicePrincipalManager handles None OpenFGA client gracefully
          <Coroutine test_create_service_principal_skips_openfga_when_none>
            CRITICAL: ServicePrincipalManager must not crash when OpenFGA is None.

            Bug: service_principal.py:197-228 calls self.openfga.write_tuples() without
            checking if self.openfga is None. This causes AttributeError when OpenFGA
            is intentionally disabled.

            Fix: Guard all OpenFGA operations with `if self.openfga:` checks.
          <Coroutine test_delete_service_principal_skips_openfga_when_none>
            Test that delete operations also handle None OpenFGA gracefully.
          <Coroutine test_associate_with_user_skips_openfga_when_none>
            Test that user association also handles None OpenFGA gracefully.
        <Class TestAPIKeyManagerRedisCacheWiring>
          Test that API Key Manager properly receives Redis cache configuration
          <Function test_api_key_manager_receives_redis_client_when_enabled>
            Verify that API Key Manager gets properly configured Redis client.

            This test documents the CORRECT pattern that should be used for
            all Redis-backed caches (including L2 cache).
          <Function test_redis_url_handles_trailing_slash>
            Test that Redis URL construction handles trailing slashes correctly.

            Bug: f"{redis_url}/{db}" produces redis://localhost:6379//2
            when redis_url has trailing slash.
          <Function test_redis_url_handles_existing_database_number>
            Test that Redis URL with existing database number doesn't produce invalid URL.

            CODEX FINDING #6: Enabled test by adding reset_singleton_dependencies().

            Bug: f"redis://localhost:6379/0/{db}" produces redis://localhost:6379/0/2
            which is INVALID for redis.from_url().

            Expected: Should strip existing database number and replace with configured one.
          <Function test_redis_url_handles_query_parameters>
            Test that Redis URLs with query parameters are preserved.

            CODEX FINDING #6: Enabled test by adding reset_singleton_dependencies().

            Example: redis://localhost:6379?timeout=5
            Should become: redis://localhost:6379/2?timeout=5
      <Module test_feature_flags.py>
        Unit tests for feature flags system
        <Class TestFeatureFlags>
          Test Feature Flag system
          <Function test_default_feature_flags>
            Test default feature flag values
          <Function test_environment_variable_override>
            Test that environment variables override defaults
          <Function test_numeric_flag_validation>
            Test numeric flag values and validation
          <Function test_numeric_flag_constraints>
            Test numeric flags enforce min/max constraints
          <Function test_is_feature_enabled>
            Test is_feature_enabled helper method
          <Function test_get_feature_value>
            Test get_feature_value with defaults
          <Function test_experimental_features_master_switch>
            Test experimental features require master switch
          <Function test_global_feature_flags_instance>
            Test global feature flags singleton
          <Function test_is_enabled_convenience_function>
            Test is_enabled convenience function
          <Function test_cache_configuration>
            Test caching feature flags
          <Function test_security_flags>
            Test security-related feature flags
          <Function test_observability_flags>
            Test observability feature flags
          <Function test_agent_behavior_flags>
            Test agent behavior configuration
          <Function test_openfga_configuration>
            Test OpenFGA feature flags
          <Function test_feature_flag_integration_with_config>
            Test feature flags integrate with main config
        <Class TestFeatureFlagEdgeCases>
          Test edge cases and error handling
          <Function test_invalid_confidence_threshold>
            Test confidence threshold validation
          <Function test_invalid_timeout>
            Test timeout validation
          <Function test_invalid_sample_rate>
            Test sample rate validation
          <Function test_boolean_string_conversion>
            Test that string 'true'/'false' are converted correctly
      <Module test_gdpr.py>
        Comprehensive tests for GDPR compliance endpoints and services
        <Class TestDataExportService>
          Test GDPR data export service
          <Coroutine test_export_user_data_basic>
            Test basic user data export
          <Coroutine test_export_user_data_no_sessions>
            Test export when user has no sessions
          <Coroutine test_export_portable_json_format>
            Test portable export in JSON format
          <Coroutine test_export_portable_csv_format>
            Test portable export in CSV format
          <Coroutine test_export_portable_invalid_format>
            Test export with invalid format raises error
          <Coroutine test_export_handles_session_store_error>
            Test export gracefully handles session store errors
        <Class TestDataDeletionService>
          Test GDPR data deletion service
          <Coroutine test_delete_user_account_success>
            Test successful user account deletion
          <Coroutine test_delete_user_account_no_sessions>
            Test deletion when user has no sessions
          <Coroutine test_delete_user_account_partial_failure>
            Test deletion with partial failures
          <Coroutine test_delete_user_account_no_session_store>
            Test deletion without session store
          <Coroutine test_delete_creates_audit_record>
            Test that deletion creates anonymized audit record
        <Class TestGDPREndpoints>
          Test GDPR API endpoints with auth mocking

          Tests GDPR compliance endpoints following TDD best practices:
          - Article 15: Right to Access
          - Article 16: Right to Rectification
          - Article 17: Right to Erasure
          - Article 20: Right to Data Portability
          - Article 21: Right to Object (Consent)
          <Function test_get_user_data_endpoint>
            Test GET /api/v1/users/me/data - GDPR Article 15 (Right to Access)
          <Function test_export_user_data_json>
            Test GET /api/v1/users/me/export?format=json - GDPR Article 20 (Data Portability)
          <Function test_export_user_data_csv>
            Test GET /api/v1/users/me/export?format=csv - CSV format export
          <Function test_export_invalid_format>
            Test export with invalid format parameter
          <Function test_update_user_profile>
            Test PATCH /api/v1/users/me - GDPR Article 16 (Right to Rectification)
          <Function test_update_user_profile_empty>
            Test profile update with no fields provided
          <Function test_delete_user_account>
            Test DELETE /api/v1/users/me?confirm=true - GDPR Article 17 (Right to Erasure)
          <Function test_delete_user_account_without_confirmation>
            Test deletion requires explicit confirmation
          <Function test_delete_user_account_no_confirm_param>
            Test deletion fails when confirm parameter is missing
          <Function test_update_consent>
            Test POST /api/v1/users/me/consent - GDPR Article 21 (Right to Object)
          <Function test_update_consent_revoke>
            Test revoking consent
          <Function test_get_consent_status>
            Test GET /api/v1/users/me/consent - Retrieve current consent status
        <Class TestGDPRModels>
          Test GDPR Pydantic models
          <Function test_user_profile_update_model>
            Test UserProfileUpdate model validation
          <Function test_user_profile_update_partial>
            Test partial profile update
          <Function test_consent_record_model>
            Test ConsentRecord model
          <Function test_consent_record_all_types>
            Test all consent types are valid
          <Function test_user_data_export_model>
            Test UserDataExport model
          <Function test_deletion_result_model>
            Test DeletionResult model
        <Class TestGDPREdgeCases>
          Test edge cases and error conditions
          <Coroutine test_export_very_large_user_data>
            Test export with large amount of user data
          <Coroutine test_delete_nonexistent_user>
            Test deleting user that doesn't exist
          <Coroutine test_concurrent_deletion_attempts>
            Test handling of concurrent deletion attempts to verify idempotency

            GIVEN: Multiple concurrent deletion requests for the same user
            WHEN: Deletions are executed simultaneously via asyncio.gather
            THEN: All deletions succeed (idempotent), audit log shows only one deletion

            This test validates:
            1. Thread safety of the deletion service
            2. Idempotent behavior (multiple deletions don't cause errors)
            3. Proper locking/coordination if implemented
      <Module test_health_check.py>
        Unit tests for health_check.py - Health Check Endpoints
        <Class TestHealthCheckEndpoints>
          Test health check endpoints
          <Function test_health_check_success>
            Test basic health check returns healthy status
          <Function test_health_check_response_format>
            Test health check response has correct format
          <Function test_readiness_check_all_healthy>
            Test readiness check when all services are healthy
          <Function test_readiness_check_openfga_unhealthy>
            Test readiness check when OpenFGA is unavailable
          <Function test_readiness_check_missing_critical_secrets>
            Test readiness check when critical secrets are missing
          <Function test_readiness_check_openfga_not_configured>
            Test readiness check when OpenFGA is not configured
          <Function test_readiness_check_infisical_degraded>
            Test readiness check when Infisical is degraded
          <Function test_startup_check_success>
            Test startup probe returns started status
          <Function test_startup_check_logging_failed>
            Test startup probe when logging initialization fails
          <Function test_prometheus_metrics_endpoint>
            Test Prometheus metrics endpoint
      <Module test_infisical_optional.py>
        Test Infisical Optional Dependency Handling

        Verifies that the application gracefully handles cases where infisical-python
        is not installed, falling back to environment variables.
        <Class TestInfisicalOptionalDependency>
          Test graceful degradation when Infisical not installed
          <Function test_secrets_manager_without_infisical>
            Verify SecretsManager works without infisical-python installed
          <Function test_get_secret_fallback_to_env>
            Verify get_secret falls back to environment variables
          <Function test_get_secret_with_fallback_parameter>
            Verify get_secret uses fallback parameter when secret not found
          <Function test_get_secret_returns_none_when_not_found>
            Verify get_secret returns None when secret not found and no fallback
          <Function test_get_all_secrets_empty_without_infisical>
            Verify get_all_secrets returns empty dict without Infisical
          <Function test_create_secret_fails_gracefully>
            Verify create_secret returns False without Infisical
          <Function test_update_secret_fails_gracefully>
            Verify update_secret returns False without Infisical
          <Function test_delete_secret_fails_gracefully>
            Verify delete_secret returns False without Infisical
        <Class TestConfigWithoutInfisical>
          Test Settings configuration without Infisical
          <Function test_settings_loads_without_infisical>
            Verify Settings can be loaded without Infisical
          <Function test_settings_get_secret_fallback>
            Verify Settings.get_secret falls back to environment
        <Class TestApplicationStartupWithoutInfisical>
          Test application can start without Infisical
          <Function test_agent_creation_without_infisical>
            Verify agent can be created without Infisical
          <Coroutine test_health_check_without_infisical>
            Verify health check works without Infisical
        <Class TestInfisicalLogging>
          Test logging behavior when Infisical unavailable
          <Function test_warning_logged_on_fallback>
            Verify warning is logged when falling back to environment variables
          <Function test_info_logged_on_env_fallback>
            Verify info is logged when using environment variable fallback
        <Class TestDocumentationExamples>
          Test examples from documentation work correctly
          <Function test_docker_compose_example>
            Verify Docker Compose setup works (no Infisical required)
          <Function test_env_file_example>
            Verify .env file setup works (no Infisical required)
      <Module test_openfga_cleanup.py>
        Integration tests for OpenFGA tuple cleanup functionality.

        Verifies that delete_tuples_for_object() properly removes all authorization
        tuples when a resource is deleted, ensuring no orphaned permissions remain.
        <Class TestTupleExtractionHelpers>
          Test helper functions for extracting tuples from expansion trees
          <Function test_extract_users_from_simple_leaf>
            Test extracting users from simple leaf node.

            Expansion tree structure:
            {"leaf": {"users": {"users": ["user:alice", "user:bob"]}}}
          <Function test_extract_users_from_empty_expansion>
            Test extracting from empty expansion returns empty list
          <Function test_extract_users_from_union_node>
            Test extracting users from union node (multiple children).

            Union nodes combine users from multiple sources.
      <Module test_openfga_client.py>
        <Class TestOpenFGAClient>
          Test OpenFGAClient class
          <Function test_init>
            Test OpenFGA client initialization
          <Coroutine test_check_permission_allowed>
            Test permission check returns True
          <Coroutine test_check_permission_denied>
            Test permission check returns False
          <Coroutine test_check_permission_error>
            Test permission check handles errors (wrapped in RetryExhaustedError after retries)
          <Coroutine test_write_tuples_success>
            Test writing relationship tuples
          <Coroutine test_write_tuples_error>
            Test write tuples handles errors (wrapped in RetryExhaustedError after retries)
          <Coroutine test_delete_tuples_success>
            Test deleting relationship tuples
          <Coroutine test_delete_tuples_error>
            Test delete tuples handles errors
          <Coroutine test_list_objects_success>
            Test listing accessible objects
          <Coroutine test_list_objects_empty>
            Test listing objects with no results
          <Coroutine test_list_objects_error>
            Test list objects handles errors
          <Coroutine test_expand_relation_success>
            Test expanding a relation to see all users with access
          <Coroutine test_expand_relation_empty>
            Test expanding relation with no tree
          <Coroutine test_expand_relation_error>
            Test expand relation handles errors
        <Class TestOpenFGAAuthorizationModel>
          Test OpenFGAAuthorizationModel class
          <Function test_get_model_definition>
            Test authorization model definition
          <Function test_organization_relations>
            Test organization type has correct relations
          <Function test_tool_relations>
            Test tool type has correct relations
          <Function test_conversation_relations>
            Test conversation type has correct relations
        <Class TestOpenFGAUtilityFunctions>
          Test utility functions for OpenFGA
          <Coroutine test_initialize_openfga_store>
            Test initializing OpenFGA store with authorization model
          <Coroutine test_initialize_openfga_store_error>
            Test initialize store handles errors
          <Coroutine test_seed_sample_data>
            Test seeding sample data
        <Class TestOpenFGACircuitBreakerCriticality>
          Test OpenFGA circuit breaker with criticality flag for fail-open/fail-closed behavior.

          NOTE: This class uses @pytest.mark.skip_resilience_reset to opt-out of the global
          autouse fixture that resets circuit breakers between tests. Each test method explicitly
          resets the circuit breaker at the start to ensure clean state.
          <Coroutine test_circuit_breaker_fails_closed_for_critical_resources>
            Test circuit breaker denies access (fail-closed) when open for CRITICAL resources.

            SECURITY: Critical resources (admin, delete, etc.) must fail-closed to prevent
            unauthorized access during OpenFGA outages.

            NOTE: Skipped in pytest-xdist parallel mode due to timing sensitivity with retry configuration.
            Run with `pytest tests/test_openfga_client.py::TestOpenFGACircuitBreakerCriticality -xvs` for serial execution.
          <Coroutine test_circuit_breaker_fails_open_for_non_critical_resources>
            Test circuit breaker allows access (fail-open) when open for NON-CRITICAL resources.

            For non-critical resources (like read-only content), we prefer availability
            over strict security, so we fail-open.

            NOTE: Skipped in pytest-xdist parallel mode due to timing sensitivity with retry configuration.
            Run with `pytest tests/test_openfga_client.py::TestOpenFGACircuitBreakerCriticality -xvs` for serial execution.
          <Coroutine test_circuit_breaker_defaults_to_critical_true>
            Test circuit breaker defaults to critical=True (fail-closed) if not specified.

            SECURITY: For safety, all resources are considered critical by default.
            Developers must explicitly opt-in to fail-open behavior.

            NOTE: Skipped in pytest-xdist parallel mode due to timing sensitivity with retry configuration.
            Run with `pytest tests/test_openfga_client.py::TestOpenFGACircuitBreakerCriticality -xvs` for serial execution.
          <Coroutine test_circuit_breaker_allows_when_closed_regardless_of_criticality>
            Test circuit breaker respects OpenFGA response when circuit is CLOSED.

            When circuit breaker is closed (normal operation), criticality flag should have
            no effect - OpenFGA response is always used.
        <Class TestOpenFGAExpansionTree>
          P0: Test OpenFGA permission expansion tree extraction

          Critical for GDPR delete_tuples_for_object() functionality
          <Coroutine test_extract_users_from_expansion_leaf_node>
            Test _extract_users_from_expansion with leaf node (single user)
          <Coroutine test_extract_users_from_expansion_union_node>
            Test _extract_users_from_expansion with union node (multiple branches)
          <Coroutine test_extract_users_from_expansion_nested_tree>
            Test _extract_users_from_expansion with deeply nested expansion tree
          <Coroutine test_extract_users_from_expansion_empty_expansion>
            Test _extract_users_from_expansion with empty expansion
      <Module test_pydantic_ai.py>
        Tests for Pydantic AI integration

        Covers type-safe agent responses, validation, and streaming.
        <Function test_pydantic_agent_wrapper_initialization>
          Test Pydantic AI agent wrapper initializes correctly.
        <Function test_get_pydantic_model_name_google>
          Test model name mapping for Google provider.
        <Function test_get_pydantic_model_name_anthropic>
          Test model name mapping for Anthropic provider.
        <Function test_get_pydantic_model_name_openai>
          Test model name mapping for OpenAI provider.
        <Function test_get_pydantic_model_name_gemini>
          Test model name mapping for Gemini provider (alternative name for google).
        <Function test_get_pydantic_model_name_unknown_provider>
          Test model name mapping for unknown provider falls back to model name.
        <Function test_format_conversation>
          Test conversation formatting.
        <Function test_format_conversation_with_system_message>
          Test conversation formatting with system messages.
        <Coroutine test_route_message_success>
          Test successful message routing.
        <Coroutine test_route_message_with_context>
          Test message routing with context.
        <Coroutine test_route_message_error_handling>
          Test route_message handles errors gracefully.
        <Coroutine test_generate_response_success>
          Test successful response generation.
        <Coroutine test_generate_response_with_context>
          Test response generation with context.
        <Coroutine test_generate_response_requires_clarification>
          Test response generation when clarification is needed.
        <Coroutine test_generate_response_error_handling>
          Test generate_response handles errors.
        <Function test_create_pydantic_agent_factory>
          Test create_pydantic_agent factory function.
        <Function test_create_pydantic_agent_custom_params>
          Test create_pydantic_agent with custom parameters.
        <Function test_create_pydantic_agent_unavailable>
          Test create_pydantic_agent when pydantic-ai is not available.
        <Function test_router_decision_model>
          Test RouterDecision Pydantic model.
        <Function test_router_decision_validation>
          Test RouterDecision validates confidence range.
        <Function test_agent_response_model>
          Test AgentResponse Pydantic model.
        <Function test_agent_response_defaults>
          Test AgentResponse has sensible defaults.
        <Function test_validated_response>
          Test ValidatedResponse container.
        <Function test_validated_response_with_errors>
          Test ValidatedResponse with errors.
        <Function test_entity_extraction_model>
          Test EntityExtraction model.
        <Function test_intent_classification_model>
          Test IntentClassification model.
        <Function test_sentiment_analysis_model>
          Test SentimentAnalysis model.
        <Function test_summary_extraction_model>
          Test SummaryExtraction model.
        <Function test_stream_chunk_model>
          Test StreamChunk model.
        <Function test_streamed_response_model>
          Test StreamedResponse model.
        <Coroutine test_streaming_validator>
          Test MCPStreamingValidator.
        <Coroutine test_stream_validated_response>
          Test stream_validated_response function.
        <Coroutine test_streaming_validator_validation_error>
          Test MCPStreamingValidator with validation error.
        <Coroutine test_streaming_validator_finalize_unknown_stream>
          Test finalizing an unknown stream.
        <Function test_llm_validator_with_json_response>
          Test LLMValidator.validate_response with JSON input.
        <Function test_llm_validator_with_aimessage>
          Test LLMValidator.validate_response with AIMessage.
        <Function test_llm_validator_validation_error_non_strict>
          Test LLMValidator.validate_response with validation error in non-strict mode.
        <Function test_llm_validator_validation_error_strict>
          Test LLMValidator.validate_response with validation error in strict mode.
        <Function test_llm_validator_unexpected_error_non_strict>
          Test LLMValidator.validate_response with unexpected error in non-strict mode.
        <Function test_llm_validator_extract_entities>
          Test LLMValidator.extract_entities convenience method.
        <Function test_llm_validator_classify_intent>
          Test LLMValidator.classify_intent convenience method.
        <Function test_llm_validator_analyze_sentiment>
          Test LLMValidator.analyze_sentiment convenience method.
        <Function test_llm_validator_extract_summary>
          Test LLMValidator.extract_summary convenience method.
      <Module test_retention.py>
        Tests for Data Retention Service (GDPR Article 5(1)(e) Storage Limitation)

        Covers automated data retention policies and cleanup schedules.
        <Class TestRetentionPolicy>
          Test RetentionPolicy model
          <Function test_retention_policy_creation>
            Test creating retention policy
          <Function test_retention_policy_defaults>
            Test retention policy default values
        <Class TestRetentionResult>
          Test RetentionResult model
          <Function test_retention_result_creation>
            Test creating retention result
          <Function test_retention_result_defaults>
            Test retention result default values
        <Class TestDataRetentionServiceInit>
          Test DataRetentionService initialization
          <Function test_init_with_defaults>
            Test initialization with default parameters
          <Function test_init_with_params>
            Test initialization with custom parameters
          <Function test_load_config_file_not_found>
            Test loading config when file doesn't exist
          <Function test_load_config_success>
            Test successful config loading
          <Function test_load_config_error>
            Test config loading error handling
          <Function test_default_config>
            Test default configuration
        <Class TestSessionCleanup>
          Test session cleanup functionality
          <Coroutine test_cleanup_sessions_no_session_store>
            Test cleanup when session store is not available
          <Coroutine test_cleanup_sessions_success>
            Test successful session cleanup
          <Coroutine test_cleanup_sessions_dry_run>
            Test session cleanup in dry-run mode
          <Coroutine test_cleanup_sessions_error_handling>
            Test error handling during session cleanup
        <Class TestConversationCleanup>
          Test conversation cleanup functionality
          <Coroutine test_cleanup_conversations>
            Test conversation cleanup
          <Coroutine test_cleanup_conversations_error>
            Test conversation cleanup error handling
        <Class TestAuditLogCleanup>
          Test audit log cleanup functionality
          <Coroutine test_cleanup_audit_logs>
            Test audit log cleanup
          <Coroutine test_cleanup_audit_logs_long_retention>
            Test audit logs have longer retention period
        <Class TestRunAllCleanups>
          Test running all cleanup policies
          <Coroutine test_run_all_cleanups>
            Test running all cleanup policies together
          <Coroutine test_run_all_cleanups_dry_run>
            Test running all cleanups in dry-run mode
        <Class TestRetentionMetrics>
          Test retention metrics tracking
          <Coroutine test_metrics_tracked_on_cleanup>
            Test that metrics are tracked during cleanup
          <Coroutine test_no_metrics_in_dry_run>
            Test that metrics are not tracked in dry-run mode
        <Class TestRetentionLogging>
          Test retention logging and audit trail
          <Coroutine test_cleanup_logged>
            Test that cleanups are logged
          <Coroutine test_errors_logged>
            Test that errors are logged
        <Class TestRetentionCompliance>
          Test GDPR and SOC 2 compliance aspects
          <Function test_gdpr_storage_limitation>
            Test compliance with GDPR Article 5(1)(e) storage limitation
          <Function test_audit_trail_retention>
            Test audit logs have longer retention for compliance
      <Module test_secrets_manager.py>
        Unit tests for secrets_manager.py - Infisical Integration
        <Class TestSecretsManager>
          Test SecretsManager class
          <Function test_init_with_credentials>
            Test initialization with provided credentials
          <Function test_init_without_credentials>
            Test initialization without credentials falls back gracefully
          <Function test_init_from_environment>
            Test initialization reads from environment variables
          <Function test_init_client_error>
            Test initialization handles client creation errors
          <Function test_get_secret_success>
            Test successfully retrieving a secret
          <Function test_get_secret_with_path>
            Test retrieving secret from specific path
          <Function test_get_secret_caching>
            Test secret caching works
          <Function test_get_secret_no_cache>
            Test getting secret without caching
          <Function test_get_secret_fallback_to_env>
            Test fallback to environment variable when client unavailable
          <Function test_get_secret_fallback_to_default>
            Test fallback to default value when not found
          <Function test_get_secret_error_uses_fallback>
            Test error in retrieval uses fallback
          <Function test_get_secret_error_tries_env_first>
            Test error tries environment variable before fallback
          <Function test_create_secret_success>
            Test creating a new secret
          <Function test_update_secret_success>
            Test updating an existing secret
          <Function test_delete_secret_success>
            Test deleting a secret
          <Function test_get_all_secrets_success>
            Test retrieving all secrets from a path
          <Function test_get_all_secrets_no_client>
            Test get_all_secrets without Infisical client
          <Function test_get_all_secrets_error>
            Test get_all_secrets handles errors
          <Function test_create_secret_no_client>
            Test create_secret without Infisical client
          <Function test_create_secret_error>
            Test create_secret handles errors
          <Function test_update_secret_no_client>
            Test update_secret without Infisical client
          <Function test_update_secret_error>
            Test update_secret handles errors
          <Function test_delete_secret_no_client>
            Test delete_secret without Infisical client
          <Function test_delete_secret_error>
            Test delete_secret handles errors
      <Module test_sla_monitoring.py>
        Tests for SLA Monitoring

        Comprehensive test suite for SLA tracking, measurements, and alerting.
        <Class TestSLATarget>
          Test SLA target configuration
          <Function test_sla_target_creation>
            Test creating SLA target
          <Function test_default_sla_targets>
            Test default SLA targets
          <Function test_custom_sla_targets>
            Test custom SLA targets
        <Class TestUptimeMeasurement>
          Test uptime SLA measurement
          <Coroutine test_measure_uptime_meeting_sla>
            Test uptime measurement meeting SLA
          <Coroutine test_measure_uptime_structure>
            Test uptime measurement structure
          <Coroutine test_uptime_compliance_percentage>
            Test uptime compliance percentage calculation
        <Class TestResponseTimeMeasurement>
          Test response time SLA measurement
          <Coroutine test_measure_response_time_meeting_sla>
            Test response time measurement meeting SLA
          <Coroutine test_measure_response_time_p95>
            Test p95 response time measurement
          <Coroutine test_measure_response_time_different_percentiles>
            Test different percentile measurements
        <Class TestErrorRateMeasurement>
          Test error rate SLA measurement
          <Coroutine test_measure_error_rate_meeting_sla>
            Test error rate measurement meeting SLA
          <Coroutine test_error_rate_structure>
            Test error rate measurement structure
        <Class TestSLAStatusDetermination>
          Test SLA status determination logic
          <Function test_status_meeting_higher_is_better>
            Test status when meeting SLA (higher is better)
          <Function test_status_at_risk_higher_is_better>
            Test status at risk (higher is better)
          <Function test_status_breach_higher_is_better>
            Test status breach (higher is better)
          <Function test_status_meeting_lower_is_better>
            Test status when meeting SLA (lower is better)
          <Function test_status_at_risk_lower_is_better>
            Test status at risk (lower is better)
          <Function test_status_breach_lower_is_better>
            Test status breach (lower is better)
        <Class TestSLAReport>
          Test SLA report generation
          <Coroutine test_generate_sla_report_daily>
            Test daily SLA report generation
          <Coroutine test_generate_sla_report_weekly>
            Test weekly SLA report generation
          <Coroutine test_generate_sla_report_monthly>
            Test monthly SLA report generation
          <Coroutine test_report_overall_status>
            Test report overall status calculation
          <Coroutine test_report_breach_count>
            Test report breach and warning counts
          <Coroutine test_report_compliance_score_calculation>
            Test compliance score calculation
          <Coroutine test_report_summary>
            Test report summary generation
        <Class TestBreachDetection>
          Test SLA breach detection and alerting
          <Coroutine test_breach_details_populated>
            Test breach details are populated when SLA breached
          <Coroutine test_no_breach_details_when_meeting>
            Test no breach details when meeting SLA
          <Coroutine test_alert_on_breach>
            Test alerting on SLA breach
        <Class TestSLAEdgeCases>
          Test edge cases in SLA monitoring
          <Coroutine test_zero_period>
            Test measurement with zero time period
          <Coroutine test_missing_target>
            Test measurement when target not configured
          <Coroutine test_extreme_values>
            Test SLA with extreme measurement values
      <Module test_soc2_evidence.py>
        Tests for SOC 2 Evidence Collection

        Comprehensive test suite for evidence collection, access reviews,
        and compliance reporting.
        <Class TestEvidenceCollector>
          Test evidence collector service
          <Coroutine test_collect_all_evidence>
            Test collecting all evidence
          <Coroutine test_collect_security_evidence>
            Test security evidence collection
          <Coroutine test_collect_availability_evidence>
            Test availability evidence collection
          <Coroutine test_collect_confidentiality_evidence>
            Test confidentiality evidence collection
          <Coroutine test_collect_processing_integrity_evidence>
            Test processing integrity evidence collection
          <Coroutine test_collect_privacy_evidence>
            Test privacy evidence collection
          <Coroutine test_access_control_evidence>
            Test access control evidence (CC6.1)
          <Coroutine test_logical_access_evidence>
            Test logical access evidence (CC6.2)
          <Coroutine test_audit_log_evidence>
            Test audit log evidence (CC6.6)
          <Coroutine test_system_monitoring_evidence>
            Test system monitoring evidence (CC7.2)
          <Coroutine test_change_management_evidence>
            Test change management evidence (CC8.1)
          <Coroutine test_sla_evidence>
            Test SLA monitoring evidence (A1.2)
          <Coroutine test_backup_evidence>
            Test backup verification evidence
          <Coroutine test_encryption_evidence>
            Test encryption verification evidence
          <Coroutine test_data_retention_evidence>
            Test data retention evidence (PI1.4)
          <Coroutine test_input_validation_evidence>
            Test input validation evidence
          <Coroutine test_gdpr_evidence>
            Test GDPR compliance evidence
          <Coroutine test_consent_evidence>
            Test consent management evidence
        <Class TestComplianceReport>
          Test compliance report generation
          <Coroutine test_generate_daily_report>
            Test daily compliance report generation
          <Coroutine test_generate_weekly_report>
            Test weekly compliance report generation
          <Coroutine test_generate_monthly_report>
            Test monthly compliance report generation
          <Coroutine test_compliance_score_calculation>
            Test compliance score calculation
          <Coroutine test_report_summary>
            Test report summary generation
          <Coroutine test_report_persistence>
            Test report file persistence
        <Class TestComplianceScheduler>
          Test compliance scheduler
          <Coroutine test_scheduler_initialization>
            Test scheduler initialization
          <Coroutine test_trigger_daily_check>
            Test manual trigger of daily compliance check
          <Coroutine test_trigger_weekly_review>
            Test manual trigger of weekly access review
          <Coroutine test_trigger_monthly_report>
            Test manual trigger of monthly compliance report
          <Coroutine test_daily_check_error_handling>
            Test error handling in daily compliance check
          <Coroutine test_scheduler_start_stop>
            Test scheduler start and stop
        <Class TestAccessReview>
          Test access review functionality
          <Coroutine test_access_review_report_structure>
            Test access review report structure
          <Coroutine test_access_review_item_validation>
            Test access review item validation
          <Coroutine test_access_review_report_persistence>
            Test access review report file persistence
    <Dir kubernetes>
      <Module test_critical_deployment_issues.py>
        Test suite for critical Kubernetes deployment issues identified by security audit.

        Following TDD Red-Green-Refactor:
        - RED: These tests should FAIL initially (critical issues exist)
        - GREEN: After fixes, tests should PASS
        - REFACTOR: Improve validation logic as needed

        Critical issues tested:
        1. NetworkPolicy uses non-standard namespace labels and blocks external egress
        2. Deployment zone spreading breaks single-zone clusters
        3. Secret.yaml with placeholder values included in base kustomization
        4. OpenFGA deployment references missing secret key
        5. GKE production overlay namespace mismatch
        <Class TestCriticalNetworkPolicyIssues>
          Test NetworkPolicy uses standard labels and permits required egress.
          <Function test_networkpolicy_uses_standard_namespace_labels>
            RED: NetworkPolicy uses non-standard 'name' label for namespace selection.
            GREEN: Should use 'kubernetes.io/metadata.name' standard label.

            Issue: deployments/base/networkpolicy.yaml:19 uses 'name: ingress-nginx'
            Fix: Use 'kubernetes.io/metadata.name: ingress-nginx'
          <Function test_networkpolicy_permits_external_llm_api_egress>
            RED: NetworkPolicy blocks egress to external LLM APIs (Anthropic, OpenAI, etc).
            GREEN: Should allow egress to public internet via ipBlock or remove namespaceSelector: {}.

            Issue: Line 72 uses 'namespaceSelector: {}' which only matches in-cluster namespaces
            Fix: Add explicit ipBlock rules for 0.0.0.0/0 or document cloud-specific egress
        <Class TestCriticalTopologySpreadIssues>
          Test that topology spread constraints don't break single-zone clusters.
          <Function test_deployment_zone_spreading_allows_single_zone>
            RED: Deployment requires multiple zones, breaking single-zone clusters.
            GREEN: Zone spreading should be preferredDuringScheduling or in production overlay only.

            Issue: deployment.yaml:550 uses 'whenUnsatisfiable: DoNotSchedule' for zones
            Issue: deployment.yaml:571 uses 'requiredDuringSchedulingIgnoredDuringExecution' for zones
            Fix: Change to 'ScheduleAnyway' or move to production overlay
        <Class TestCriticalSecretIssues>
          Test that secrets with placeholders are not in base kustomization.
          <Function test_secret_placeholders_not_in_base_kustomization>
            RED: secret.yaml with REPLACE_WITH_* placeholders is in base kustomization.
            GREEN: Secret should be removed from base or replaced with ExternalSecret template.

            Issue: deployments/base/secret.yaml:15 has placeholders, included in kustomization
            Fix: Remove from base kustomization or use ExternalSecret
          <Function test_openfga_deployment_has_required_secret_key>
            RED: OpenFGA deployment references openfga-datastore-uri key that doesn't exist.
            GREEN: Secret should include openfga-datastore-uri key.

            Issue: openfga-deployment.yaml:88 expects openfga-datastore-uri
            Issue: secret.yaml doesn't define this key
            Fix: Add openfga-datastore-uri to secret or use constructed URI
        <Class TestCriticalNamespaceIssues>
          Test that production GKE overlay has consistent namespace references.
          <Function test_production_gke_namespace_consistency>
            RED: Production GKE overlay has namespace mismatch.
            GREEN: All resources should use the same namespace.

            Issue: namespace.yaml creates 'production-mcp-server-langgraph'
            Issue: network-policy.yaml uses 'mcp-production'
            Issue: resource-quotas.yaml uses 'mcp-production'
            Fix: Align namespace names across all files
      <Module test_external_secrets.py>
        Test suite to validate that External Secrets configurations contain no placeholders.

        These tests ensure that ExternalSecret and SecretStore resources are properly configured
        with real project IDs, account IDs, and environment names before deployment.

        Following TDD Red-Green-Refactor:
        - RED: These tests should FAIL initially (placeholders exist)
        - GREEN: After implementing variable substitution, tests should PASS
        - REFACTOR: Improve validation logic as needed
        <Class TestExternalSecretsValidation>
          Test that ExternalSecret and SecretStore resources have no placeholders.
          <Function test_aws_external_secrets_no_placeholders>
            Test that AWS ExternalSecrets have no ACCOUNT_ID or ENVIRONMENT placeholders.

            RED: Should FAIL - AWS overlay has ACCOUNT_ID and ENVIRONMENT in role ARNs and secret paths
            GREEN: Should PASS - after variable substitution
          <Function test_gcp_external_secrets_no_placeholders>
            Test that GCP ExternalSecrets have no PROJECT_ID placeholders.

            RED: Should FAIL - GCP overlay has YOUR_PROJECT_ID placeholder
            GREEN: Should PASS - after variable substitution
          <Function test_azure_external_secrets_no_placeholders>
            Test that Azure ExternalSecrets have no CLIENT_ID placeholders.

            RED: Should FAIL - Azure overlay may have CLIENT_ID placeholders
            GREEN: Should PASS - after variable substitution
      <Module test_gke_labels.py>
        Test suite to validate that GKE Autopilot configurations don't use deprecated labels.

        These tests ensure Kubernetes manifests for GKE Autopilot use compatible labels
        and don't include deprecated or incompatible node pool labels.

        Following TDD Red-Green-Refactor:
        - RED: These tests should FAIL initially (deprecated labels exist)
        - GREEN: After removing deprecated labels, tests should PASS
        - REFACTOR: Improve validation logic as needed
        <Class TestGKEAutopilotLabels>
          Test that GKE Autopilot overlays don't use deprecated labels.
          <Function test_gke_autopilot_no_deprecated_nodepool_label>
            Test that GKE Autopilot overlays don't use deprecated nodepool labels.

            RED: Should FAIL - GCP overlay uses cloud.google.com/gke-nodepool label
            GREEN: Should PASS - after removing deprecated label
          <Function test_gke_autopilot_recommended_labels>
            Test that GKE Autopilot overlays use recommended labels.

            This is informational - helps ensure proper labeling practices.
      <Module test_high_priority_improvements.py>
        Test suite for high-priority Kubernetes deployment improvements.

        Following TDD Red-Green-Refactor:
        - RED: These tests should FAIL initially (issues exist)
        - GREEN: After fixes, tests should PASS
        - REFACTOR: Improve implementation quality

        High-priority improvements tested:
        1. PostgreSQL should support HA configuration and have backup strategy
        2. Redis should use StatefulSet with persistent storage
        3. Service accounts should have explicit RBAC with least-privilege
        4. Container images should use fully-qualified references with digests
        <Class TestPostgreSQLHighAvailability>
          Test PostgreSQL deployment supports HA and has backup strategy.
          <Function test_postgres_deployment_documents_ha_options>
            RED: PostgreSQL StatefulSet has no HA documentation or configuration.
            GREEN: Should document CloudSQL/RDS option or include HA configuration.

            Issue: Single-instance PostgreSQL with no HA or backup strategy
            Fix: Add documentation for managed databases or HA setup
          <Function test_postgres_has_production_ready_storage_class>
            RED: PostgreSQL uses default storage class with no annotations.
            GREEN: Should have storage class configuration guidance.

            Issue: No guidance on production-ready storage configuration
            Fix: Add storage class recommendations
        <Class TestRedisStatefulSetWithPersistence>
          Test Redis uses StatefulSet with persistent storage instead of Deployment.
          <Function test_redis_uses_statefulset_not_deployment>
            RED: Redis uses Deployment instead of StatefulSet.
            GREEN: Should use StatefulSet for persistent storage.

            Issue: Redis Deployment loses sessions on restart
            Fix: Convert to StatefulSet with volumeClaimTemplates
          <Function test_redis_has_persistent_storage_not_emptydir>
            RED: Redis uses emptyDir for data storage.
            GREEN: Should use PersistentVolumeClaim for durability.

            Issue: emptyDir loses data on pod restart
            Fix: Use volumeClaimTemplates in StatefulSet
          <Function test_redis_enables_aof_persistence>
            RED: Redis configuration may not enable AOF persistence.
            GREEN: Should enable AOF for data durability.

            Issue: Without AOF, data may be lost
            Fix: Ensure redis.conf has appendonly yes
        <Class TestRBACLeastPrivilege>
          Test service accounts have explicit RBAC with least-privilege.
          <Function test_service_account_has_explicit_role_binding>
            RED: Service account exists without corresponding Role/RoleBinding.
            GREEN: Should have explicit RBAC configuration.

            Issue: Service account uses cluster-default permissions
            Fix: Create Role and RoleBinding with least-privilege
          <Function test_rbac_follows_least_privilege_principle>
            RED: RBAC may grant excessive permissions.
            GREEN: Should follow least-privilege with minimal verbs and resources.

            Issue: Overly permissive RBAC increases security risk
            Fix: Grant only required permissions
        <Class TestContainerImageBestPractices>
          Test container images use fully-qualified references with digests.
          <Function test_main_deployment_uses_qualified_image_reference>
            RED: Deployment uses unqualified image name.
            GREEN: Should use fully-qualified registry path.

            Issue: Unqualified image may pull from wrong registry
            Fix: Use ghcr.io/org/repo:tag or full registry path
          <Function test_init_containers_use_qualified_images>
            RED: Init containers use unqualified images (busybox).
            GREEN: Should use fully-qualified references.

            Issue: busybox:1.36 may pull from Docker Hub instead of intended registry
            Fix: Use docker.io/library/busybox:1.36 or replace with native probes
      <Module test_kube_score_compliance.py>
        TDD Tests for Kube-Score Compliance

        Following TDD principles:
        1. RED: These tests verify kube-score best practices
        2. GREEN: Fix manifests to pass all checks
        3. REFACTOR: Ensure consistency across all deployments

        Validates:
        - ImagePullPolicy set to Always
        - Resource limits and requests (CPU, Memory, Ephemeral Storage)
        - Security context (high UIDs, read-only filesystem)
        - Probe differentiation (readiness vs liveness)
        - HPA configuration (no static replica count)
        - PodDisruptionBudgets
        - Service/NetworkPolicy selectors
        <Class TestImagePullPolicy>
          Test that all containers have imagePullPolicy: Always.
          <Function test_all_containers_have_imagepullpolicy_always>
            All containers and initContainers must have imagePullPolicy: Always.

            This ensures we always pull the latest image with the specified tag,
            improving security and preventing stale images.
        <Class TestResourceLimits>
          Test that all containers have proper resource limits and requests.
          <Function test_all_containers_have_cpu_memory_limits>
            All containers must have CPU and Memory limits and requests.

            This prevents resource exhaustion and ensures fair scheduling.
            Init containers can be exempted if they're short-lived.
          <Function test_all_containers_have_ephemeral_storage_limits>
            All containers should have ephemeral storage limits.

            This prevents containers from filling up node disk space.
        <Class TestSecurityContext>
          Test security context configuration.
          <Function test_containers_run_as_high_uid>
            Containers should run as UID >= 10000 for security.

            This prevents privilege escalation and follows security best practices.
          <Function test_containers_use_readonly_root_filesystem>
            Containers should use read-only root filesystem where possible.

            This limits attack surface by preventing file modifications.
        <Class TestProbeConfiguration>
          Test that readiness and liveness probes are different.
          <Function test_readiness_liveness_probes_are_different>
            Readiness and liveness probes should be different.

            Liveness probe should check if app needs restart.
            Readiness probe should check if app can serve traffic.
      <Module test_kustomize_staging_gke.py>
        Test suite to validate staging-gke Kustomize configuration correctness.

        These tests validate that Kustomize rendered manifests are properly configured
        for the staging-gke environment, catching issues before deployment.

        Following TDD Red-Green-Refactor:
        - RED: These tests should FAIL initially (revealing existing config issues)
        - GREEN: After fixing deployment configurations, tests should PASS
        - REFACTOR: Improve validation logic and configurations as needed

        Issue Context:
        - Pod failure: staging-mcp-server-langgraph (CreateContainerConfigError)
          Root cause: Secret name mismatch - deployment refs 'mcp-server-langgraph-secrets'
          but Kustomize creates 'staging-mcp-server-langgraph-secrets' due to namePrefix

        - Pod failure: staging-keycloak (CrashLoopBackOff)
          Root cause: EmptyDir volume mounted at /opt/keycloak/lib overwrites JAR files
          causing ClassNotFoundException: io.quarkus.bootstrap.runner.QuarkusEntryPoint
        <Class TestKustomizeStagingGKE>
          Test that staging-gke Kustomize configurations are correct.
          <Function test_secret_references_use_staging_prefix>
            Test that all secret references in staging-gke use 'staging-' prefix.

            RED: Should FAIL - mcp-server-langgraph deployment refs unprefixed secret name
            GREEN: Should PASS - after adding deployment patch to fix secret references

            This test prevents CreateContainerConfigError caused by secret name mismatches.
          <Function test_keycloak_volume_mounts_dont_override_critical_paths>
            Test that Keycloak deployment doesn't mount emptyDir at /opt/keycloak/lib.

            RED: Should FAIL - Keycloak has emptyDir mount at /opt/keycloak/lib
            GREEN: Should PASS - after removing the problematic volume mount

            This test prevents CrashLoopBackOff caused by overwriting Keycloak's JAR files.
            The /opt/keycloak/lib directory contains critical Quarkus runtime JARs that must
            not be replaced with an empty directory.
          <Function test_keycloak_has_required_volume_mounts>
            Test that Keycloak has required volume mounts for data and cache.

            This ensures Keycloak can persist data and use temporary directories
            without overwriting critical application files.
          <Function test_all_deployments_have_valid_images>
            Test that all deployments reference valid container images.

            This ensures no placeholder image tags or invalid registries are used.

            Note: "latest" tags are allowed for staging/dev environments (e.g., staging-latest, dev-latest)
            since we force-pull images anyway. Only production should use pinned versions.
          <Function test_kustomize_renders_without_errors>
            Test that Kustomize can render manifests without errors.

            This is a basic smoke test to catch YAML syntax errors,
            invalid references, or missing patches.
      <Module test_serviceaccount_annotations.py>
        Test suite to validate that Kubernetes ServiceAccount annotations contain no placeholders.

        These tests ensure that cloud identity annotations (IRSA for AWS, Workload Identity for GKE/Azure)
        are properly configured with real values before deployment.

        Following TDD Red-Green-Refactor:
        - RED: These tests should FAIL initially (placeholders exist)
        - GREEN: After implementing variable substitution, tests should PASS
        - REFACTOR: Improve validation logic as needed
        <Class TestServiceAccountAnnotations>
          Test that ServiceAccount annotations contain no placeholder values.
          <Function test_base_serviceaccount_no_placeholders>
            Test that base ServiceAccount has no placeholder annotations.

            RED: Should FAIL - base ServiceAccount has ACCOUNT_ID, PROJECT_ID, AZURE_CLIENT_ID
            GREEN: Should PASS - after using variables or removing default placeholders
          <Function test_aws_overlay_serviceaccount_no_placeholders>
            Test that AWS overlay ServiceAccount patches have no placeholders.

            RED: Should FAIL - AWS overlay has ACCOUNT_ID and ENVIRONMENT placeholders
            GREEN: Should PASS - after variable substitution
          <Function test_gcp_overlay_serviceaccount_no_placeholders>
            Test that GCP overlay ServiceAccount patches have no placeholders.

            RED: Should FAIL - GCP overlay has PROJECT_ID placeholder
            GREEN: Should PASS - after variable substitution
        <Class TestKustomizeVariableSubstitution>
          Test that Kustomize configurations support variable substitution.
          <Function test_kustomization_has_vars_or_replacements>
            Test that kustomization.yaml files use vars or replacements for dynamic values.

            RED: Should FAIL - kustomization files don't use variable substitution
            GREEN: Should PASS - after implementing vars/replacements
    <Package llm>
      <Module test_pydantic_model_names.py>
        Test pydantic-ai model name formatting with provider prefixes.

        PYDANTIC-AI DEPRECATION: Model names must include provider prefix.

        Background:
        -----------
        Pydantic AI v0.0.14+ requires model names to include provider prefixes:
        - Google Gemini: 'google-gla:gemini-2.5-flash' (not 'gemini-2.5-flash')
        - Anthropic Claude: 'anthropic:claude-sonnet-4-5-20250929'
        - OpenAI: 'openai:gpt-4'

        Without prefixes, deprecation warnings are emitted:
        "Specifying a model name without a provider prefix is deprecated.
        Instead of 'gemini-2.5-flash', use 'google-gla:gemini-2.5-flash'."

        This test ensures the PydanticAIAgentWrapper correctly adds provider
        prefixes to model names before passing them to pydantic-ai.

        References:
        -----------
        - pydantic-ai docs: https://ai.pydantic.dev/models/
        - Related issue: https://github.com/pydantic/pydantic-ai/issues/XXX
        <Class TestPydanticModelNameFormatting>
          Test proper model name formatting for pydantic-ai
          <Function test_google_gemini_model_has_google_gla_prefix>
            Gemini models must have 'google-gla:' prefix for pydantic-ai
          <Function test_gemini_provider_alias_has_google_gla_prefix>
            Provider 'gemini' should be treated same as 'google'
          <Function test_anthropic_claude_model_has_anthropic_prefix>
            Claude models must have 'anthropic:' prefix
          <Function test_openai_model_has_openai_prefix>
            OpenAI models must have 'openai:' prefix
          <Function test_unknown_provider_still_adds_prefix>
            Unknown providers should still get a prefix (provider:model format)
          <Function test_settings_default_model_works_with_pydantic_ai>
            Default model from settings should work with pydantic-ai wrapper
          <Function test_model_name_without_prefix_gets_prefixed>
            Unprefixed model names should get provider prefix added.

            This is the core fix for the deprecation warning:
            "Specifying a model name without a provider prefix is deprecated"
          <Function test_already_prefixed_model_not_double_prefixed>
            If model name already has prefix, don't add another one
        <Class TestPydanticWrapperIntegration>
          Integration tests for PydanticAIAgentWrapper with real-world scenarios
          <Function test_wrapper_initialization_with_gemini_model>
            Wrapper should initialize correctly with Gemini model
          <Function test_wrapper_initialization_with_settings>
            Wrapper should work with models from Settings
    <Package meta>
      <Package ci>
        <Module test_ci_workflow_dependencies.py>
          Test that CI workflow validation tests can import all required modules.

          This test ensures that the CI workflow validation step has access to all
          necessary dependencies, particularly langchain_core and other project modules.
          <Function test_langchain_core_importable>
            Test that langchain_core can be imported in the CI environment.

            This validates that the dev extras in pyproject.toml include all
            required dependencies for workflow validation tests.
          <Function test_all_project_modules_importable>
            Test that all project modules can be imported.

            This ensures the virtual environment is properly configured and
            all dependencies are installed.
          <Function test_workflow_validation_dependencies>
            Test that workflow validation test dependencies are available.

            This ensures pytest, pyyaml, and other validation dependencies
            are properly installed in the dev environment.
          <Function test_uv_venv_activated>
            Test that the uv virtual environment is properly activated.

            This validates that the GitHub Actions workflow is using the
            correct Python environment created by the setup-python-deps action.
          <Function test_dev_extras_complete>
            Test that dev extras include all necessary packages for CI validation.

            This is a meta-test that verifies the pyproject.toml configuration
            includes all packages needed for workflow validation tests.
        <Module test_workflow_dependencies.py>
          Test workflow job dependency validation.

          These tests ensure that job dependencies are properly declared and referenced,
          preventing runtime errors due to undefined needs references.

          TDD Approach (RED → GREEN → REFACTOR):
          1. RED: Tests fail initially due to missing job dependencies
          2. GREEN: Fix workflows to declare all referenced dependencies
          3. REFACTOR: Improve dependency structure while keeping tests green
          <Function test_job_dependencies_declared[deployment-validation.yml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[security-validation.yml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[shell-tests.yml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[smoke-tests.yml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[validate-k8s-configs.yml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[build-hygiene.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[bump-deployment-versions.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[ci.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[cost-tracking.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[coverage-trend.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[dependabot-automerge.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[deploy-production-gke.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[deploy-staging-gke.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[docs-validation.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[dora-metrics.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[e2e-tests.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[gcp-compliance-scan.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[gcp-drift-detection.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[integration-tests.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[link-checker.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[observability-alerts.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[optional-deps-test.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[performance-regression.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[quality-tests.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[release.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[security-scan.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[stale.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[terraform-validate.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[track-skipped-tests.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[validate-deployments.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[validate-kubernetes.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[weekly-reports.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_job_dependencies_declared[workflow-health-dashboard.yaml]>
            Test that all needs.* references have corresponding job dependencies declared.

            When a job references needs.other_job, it must declare other_job in its needs array.
            Otherwise, the workflow will fail at runtime with undefined reference errors.

            Expected to FAIL initially (RED phase) for:
            - deploy-production-gke.yaml:545 - rollback-on-failure references needs.build-and-push
              but doesn't include build-and-push in its needs array

            Correct pattern:
                rollback:
                  needs: [deploy, build-and-push]  # Declare ALL dependencies
                  steps:
                    - run: echo ${{ needs.build-and-push.outputs.tag }}  # Now valid
          <Function test_no_circular_dependencies[deployment-validation.yml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[security-validation.yml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[shell-tests.yml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[smoke-tests.yml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[validate-k8s-configs.yml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[build-hygiene.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[bump-deployment-versions.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[ci.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[cost-tracking.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[coverage-trend.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[dependabot-automerge.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[deploy-production-gke.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[deploy-staging-gke.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[docs-validation.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[dora-metrics.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[e2e-tests.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[gcp-compliance-scan.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[gcp-drift-detection.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[integration-tests.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[link-checker.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[observability-alerts.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[optional-deps-test.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[performance-regression.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[quality-tests.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[release.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[security-scan.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[stale.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[terraform-validate.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[track-skipped-tests.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[validate-deployments.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[validate-kubernetes.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[weekly-reports.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_no_circular_dependencies[workflow-health-dashboard.yaml]>
            Test that workflows don't have circular job dependencies.

            Circular dependencies cause workflows to hang indefinitely.

            Example of circular dependency (invalid):
                job_a:
                  needs: job_b
                job_b:
                  needs: job_a  # Circular!
          <Function test_referenced_jobs_exist[deployment-validation.yml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[security-validation.yml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[shell-tests.yml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[smoke-tests.yml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[validate-k8s-configs.yml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[build-hygiene.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[bump-deployment-versions.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[ci.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[cost-tracking.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[coverage-trend.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[dependabot-automerge.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[deploy-production-gke.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[deploy-staging-gke.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[docs-validation.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[dora-metrics.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[e2e-tests.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[gcp-compliance-scan.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[gcp-drift-detection.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[integration-tests.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[link-checker.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[observability-alerts.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[optional-deps-test.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[performance-regression.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[quality-tests.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[release.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[security-scan.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[stale.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[terraform-validate.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[track-skipped-tests.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[validate-deployments.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[validate-kubernetes.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[weekly-reports.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_referenced_jobs_exist[workflow-health-dashboard.yaml]>
            Test that all jobs referenced in needs arrays actually exist in the workflow.

            This prevents typos in job names that would cause workflow failures.
          <Function test_dependency_validation_comprehensive>
            Comprehensive test: validate all workflows have correct dependency structures.

            This integration test ensures:
            1. All needs.* references are declared
            2. No circular dependencies
            3. All referenced jobs exist
            4. Dependency graph is well-formed
        <Module test_workflow_security.py>
          Test workflow security patterns and secret handling.

          These tests detect security anti-patterns in GitHub Actions workflows,
          particularly around secret handling and context usage.

          TDD Approach (RED → GREEN → REFACTOR):
          1. RED: Tests fail initially due to secret misuse in job contexts
          2. GREEN: Fix workflows to use secrets only in step contexts
          3. REFACTOR: Improve security patterns while keeping tests green
          <Function test_no_secrets_in_job_if_conditions[deployment-validation.yml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[security-validation.yml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[shell-tests.yml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[smoke-tests.yml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[validate-k8s-configs.yml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[build-hygiene.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[bump-deployment-versions.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[ci.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[cost-tracking.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[coverage-trend.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[dependabot-automerge.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[deploy-production-gke.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[deploy-staging-gke.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[docs-validation.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[dora-metrics.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[e2e-tests.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[gcp-compliance-scan.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[gcp-drift-detection.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[integration-tests.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[link-checker.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[observability-alerts.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[optional-deps-test.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[performance-regression.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[quality-tests.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[release.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[security-scan.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[stale.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[terraform-validate.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[track-skipped-tests.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[validate-deployments.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[validate-kubernetes.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[weekly-reports.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_no_secrets_in_job_if_conditions[workflow-health-dashboard.yaml]>
            Test that workflows do not use secrets.* in job-level if conditions.

            GitHub Actions does NOT support secrets context in job-level conditions.
            Secrets are only available in step-level contexts.

            This is a CRITICAL security validation that prevents silent failures.

            Expected to FAIL initially (RED phase) for:
            - dora-metrics.yaml:242 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:119 - secrets.SLACK_WEBHOOK_URL in job if
            - observability-alerts.yaml:206 - secrets.PAGERDUTY_INTEGRATION_KEY in job if
            - observability-alerts.yaml:253 - secrets.DATADOG_API_KEY in job if

            Correct pattern:
                job:
                  if: always()  # No secret check
                  steps:
                    - name: Step
                      if: env.SECRET != ''  # Check at step level
                      env:
                        SECRET: ${{ secrets.MY_SECRET }}
          <Function test_fork_protection_on_deployment_jobs[deployment-validation.yml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[security-validation.yml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[shell-tests.yml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[smoke-tests.yml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[validate-k8s-configs.yml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[build-hygiene.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[bump-deployment-versions.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[ci.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[cost-tracking.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[coverage-trend.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[dependabot-automerge.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[deploy-production-gke.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[deploy-staging-gke.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[docs-validation.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[dora-metrics.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[e2e-tests.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[gcp-compliance-scan.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[gcp-drift-detection.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[integration-tests.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[link-checker.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[observability-alerts.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[optional-deps-test.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[performance-regression.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[quality-tests.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[release.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[security-scan.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[stale.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[terraform-validate.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[track-skipped-tests.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[validate-deployments.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[validate-kubernetes.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[weekly-reports.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_fork_protection_on_deployment_jobs[workflow-health-dashboard.yaml]>
            Test that deployment jobs include fork repository protection.

            Deployment jobs should check github.repository to prevent:
            - Unauthorized deployments from forks
            - Secret exposure in fork PRs
            - Wasted Actions minutes in forks

            Best practice pattern:
                deploy:
                  if: github.repository == 'owner/repo'
                  steps: ...

            This test identifies jobs that access deployment secrets but lack fork guards.
          <Function test_no_hardcoded_secrets[deployment-validation.yml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[security-validation.yml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[shell-tests.yml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[smoke-tests.yml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[validate-k8s-configs.yml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[build-hygiene.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[bump-deployment-versions.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[ci.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[cost-tracking.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[coverage-trend.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[dependabot-automerge.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[deploy-production-gke.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[deploy-staging-gke.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[docs-validation.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[dora-metrics.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[e2e-tests.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[gcp-compliance-scan.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[gcp-drift-detection.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[integration-tests.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[link-checker.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[observability-alerts.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[optional-deps-test.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[performance-regression.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[quality-tests.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[release.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[security-scan.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[stale.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[terraform-validate.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[track-skipped-tests.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[validate-deployments.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[validate-kubernetes.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[weekly-reports.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_no_hardcoded_secrets[workflow-health-dashboard.yaml]>
            Test that workflows do not contain hardcoded secrets or credentials.

            This test scans for common secret patterns that should use GitHub secrets instead.
          <Function test_all_workflows_use_secrets_correctly>
            Integration test: verify all workflows use secrets correctly.

            This test ensures that secrets are:
            1. Never used in job-level if conditions
            2. Always accessed via ${{ secrets.* }} syntax
            3. Protected with appropriate guards
        <Module test_workflow_syntax.py>
          Test workflow syntax validation using actionlint.

          These tests ensure all GitHub Actions workflows are syntactically valid and
          follow best practices, preventing deployment failures due to workflow errors.

          TDD Approach (RED → GREEN → REFACTOR):
          1. RED: Tests fail initially due to workflow syntax errors
          2. GREEN: Fix workflows to make tests pass
          3. REFACTOR: Improve workflow quality while keeping tests green
          <Function test_workflow_syntax_valid[deployment-validation.yml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[security-validation.yml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[shell-tests.yml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[smoke-tests.yml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[validate-k8s-configs.yml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[build-hygiene.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[bump-deployment-versions.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[ci.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[cost-tracking.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[coverage-trend.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[dependabot-automerge.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[deploy-production-gke.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[deploy-staging-gke.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[docs-validation.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[dora-metrics.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[e2e-tests.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[gcp-compliance-scan.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[gcp-drift-detection.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[integration-tests.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[link-checker.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[observability-alerts.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[optional-deps-test.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[performance-regression.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[quality-tests.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[release.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[security-scan.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[stale.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[terraform-validate.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[track-skipped-tests.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[validate-deployments.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[validate-kubernetes.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[weekly-reports.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_workflow_syntax_valid[workflow-health-dashboard.yaml]>
            Test that each workflow file passes actionlint syntax validation.

            This test runs actionlint on each workflow file to ensure:
            - YAML syntax is valid
            - GitHub Actions syntax is valid
            - Expression syntax is correct
            - Job dependencies are properly declared
            - Context usage is valid (no secrets.* in job-level if conditions)

            Expected to FAIL initially (RED phase) due to known issues:
            - deploy-production-gke.yaml:545 - missing build-and-push dependency
            - dora-metrics.yaml:242 - invalid secrets.* in job if
            - observability-alerts.yaml:119,206,253 - invalid secrets.* in job if
          <Function test_actionlint_installed>
            Verify that actionlint is installed and available.

            This is a prerequisite test that ensures the validation tool is present.
            Skips gracefully if actionlint is not installed (e.g., in Python version test jobs).
          <Function test_workflows_directory_exists>
            Verify that the workflows directory exists.
          <Function test_at_least_one_workflow_exists>
            Verify that at least one workflow file exists.
        <Module test_workflow_validation.py>
          Test suite for GitHub Actions workflow validation.

          This module validates GitHub Actions workflows for common issues and anti-patterns,
          including proper error handling, artifact management, and secret validation.

          Created as part of OpenAI Codex findings validation (TDD approach).
          <Class TestWorkflowValidation>
            Validate GitHub Actions workflows for common issues.
            <Function test_coverage_artifact_handling_has_file_check>
              Test that coverage merge step checks for file existence before copying.

              Context: OpenAI Codex Finding - ci.yaml:193
              Issue: cp command without file existence check after continue-on-error download

              This test validates that when downloading artifacts with continue-on-error=true,
              subsequent file operations must check for file existence to prevent failures.
            <Function test_download_artifact_patterns>
              Test that download-artifact steps with continue-on-error have safe subsequent operations.

              This is a broader regression prevention test that checks all workflows.
            <Function test_gcp_auth_steps_have_secret_validation>
              Test that GCP authentication steps include secret availability checks.

              Context: OpenAI Codex Finding - GCP workflows
              Issue: Workflows fail on forks/scheduled runs without graceful degradation

              This test enforces that jobs using GCP auth MUST check for secret availability
              or explicitly handle forks to prevent confusing auth failures.
            <Function test_success_summaries_have_status_conditionals>
              Test that success summary steps include status conditionals.

              Context: OpenAI Codex Finding - Success summaries
              Issue: Static summaries print "success" messages even when upstream steps fail

              This test ensures summary steps only run on actual success.
            <Function test_docker_build_actions_are_consistent>
              Test that Docker build actions use consistent versions across all workflows.

              Context: OpenAI Codex Finding - Docker build duplication
              Issue: Multiple workflows duplicate Docker build logic with potential version drift

              This test ensures all Docker builds use the same action versions, even if
              the build logic itself is intentionally different (matrix vs single, SBOM, etc.)
            <Function test_action_versions_are_valid>
              Test that all GitHub Actions use valid version tags.

              Context: OpenAI Codex claimed versions don't exist - this was FALSE.
              This test documents the CORRECT versions being used.
          <Class TestCoverageArtifactScenarios>
            Test scenarios for coverage artifact handling.
            <Function test_missing_coverage_artifact_handling>
              Test that missing coverage artifacts are handled gracefully.

              This simulates the scenario where download-artifact has continue-on-error=true
              and the artifact doesn't exist.
            <Function test_fixed_coverage_artifact_handling>
              Test that the FIXED coverage merge handles missing artifacts gracefully.

              This is the expected behavior after the fix.
            <Function test_fixed_coverage_with_existing_artifact>
              Test that the fixed script still works when artifact exists.
      <Package infrastructure>
        <Module test_docker_paths.py>
          Test Docker image configuration and Python path validation.

          These tests ensure Docker images are built with correct paths and configurations,
          preventing runtime errors due to missing Python interpreters or incorrect paths.

          TDD Approach (RED → GREEN → REFACTOR):
          1. RED: Tests may pass or fail depending on current Docker image state
          2. GREEN: Ensure Docker configurations are correct
          3. REFACTOR: Improve Docker build process while keeping tests green
          <Function test_docker_python_path_consistency>
            Test that Docker smoke tests use correct Python paths for each variant.

            Different Docker base images have Python installed at different paths:
            - python:3.12-slim - Python at /usr/local/bin/python3
            - Distroless - Python at /usr/bin/python3 (if using distroless/python)
            - Custom builds - Python at /opt/venv/bin/python (if using venv)

            The smoke test must use the correct path for each variant to avoid:
            "exec: /usr/bin/python3: not found" errors

            This test validates that the CI workflow correctly handles Python paths.
          <Function test_docker_variant_matrix_complete>
            Test that Docker build matrix includes all expected variants.

            The CI workflow should build and test multiple Docker image variants:
            - base: Minimal image
            - full: Full-featured image
            - test: Image with test dependencies

            Each variant should be tested independently.
          <Function test_docker_image_verification_comprehensive>
            Test that Docker images are verified after build.

            The CI workflow should verify built images by:
            1. Checking image was loaded (docker images | grep)
            2. Inspecting image metadata (docker inspect)
            3. Running smoke test (docker run)

            This ensures images are functional before deployment.
          <Function test_docker_entrypoint_configuration>
            Test that Docker entrypoint configuration is validated in CI.

            The smoke test should verify that:
            - The entrypoint is correctly set
            - Python can be invoked
            - Basic imports work

            This prevents "entrypoint not found" errors in production.
          <Function test_docker_smoke_test_validates_variants>
            Test that smoke tests are variant-aware and test each variant appropriately.

            Different variants may have different:
            - Python locations (/usr/bin vs /usr/local/bin vs /opt/venv/bin)
            - Installed packages (test variant has test dependencies, base does not)
            - Entrypoints (some use venv activation, others don't)

            The smoke test should handle these differences.
        <Module test_infrastructure_config_validation.py>
          Meta-tests for validating test infrastructure port configuration consistency.

          These tests ensure that:
          1. docker-compose.test.yml port mappings match conftest.py fixtures
          2. No integration tests use hardcoded default ports (5432, 6379, etc.)
          3. All integration tests use test_infrastructure_ports fixture or environment variables

          Context:
          Per INTEGRATION_TEST_FINDINGS.md Phase 4, several tests were found using hardcoded
          default ports instead of test infrastructure ports (9432 for Postgres, 9379 for Redis, etc.).

          This meta-test suite prevents regressions by validating port configuration patterns.

          Test Coverage:
          1. Docker Compose ports match conftest fixture (9432, 9379, 9080, etc.)
          2. No hardcoded localhost:5432, localhost:6379 in integration tests
          3. Integration tests use test_infrastructure_ports or environment variables
          4. Default values in os.getenv() calls use test ports (9432, not 5432)
          <Class TestDockerComposePortConfiguration>
            Validate docker-compose.test.yml port mappings match conftest fixtures
            <Function test_docker_compose_exists>
              Should verify docker-compose.test.yml exists for integration testing.

              This file defines the test infrastructure (Postgres, Redis, OpenFGA, Qdrant).
            <Function test_postgres_port_matches_fixture>
              Should verify Postgres port in docker-compose matches conftest fixture.

              Expected: 9432 (not default 5432)
            <Function test_redis_port_matches_fixture>
              Should verify Redis port in docker-compose matches conftest fixture.

              Expected: 9379/9380 (not default 6379)
            <Function test_openfga_port_matches_fixture>
              Should verify OpenFGA port in docker-compose matches conftest fixture.

              Expected: 9080 (not default 8080)
            <Function test_qdrant_port_matches_fixture>
              Should verify Qdrant port in docker-compose matches conftest fixture.

              Expected: 9333 (HTTP) and 9334 (gRPC)
          <Class TestHardcodedPortDetection>
            Detect hardcoded default ports in integration tests
            <Function test_no_hardcoded_postgres_port>
              Should verify no integration tests use hardcoded localhost:5432.

              All tests should use:
              - test_infrastructure_ports fixture, OR
              - os.getenv("POSTGRES_PORT", "9432")

              NOT:
              - localhost:5432 (default port)
            <Function test_no_hardcoded_redis_port>
              Should verify no integration tests use hardcoded localhost:6379.

              This test identifies files that use mock Redis URLs with localhost:6379
              but are NOT actually connecting to Redis (they're mocking the URL).

              All REAL Redis connections should use:
              - test_infrastructure_ports fixture, OR
              - os.getenv("REDIS_PORT", "9379")
            <Function test_environment_variable_defaults_use_test_ports>
              Should verify os.getenv() default values use test ports, not production ports.

              Correct:
              - os.getenv("POSTGRES_PORT", "9432")
              - os.getenv("REDIS_PORT", "9379")

              Incorrect:
              - os.getenv("POSTGRES_PORT", "5432")
              - os.getenv("REDIS_PORT", "6379")
          <Class TestPortConfigurationBestPractices>
            Document and validate port configuration best practices
            <Function test_conftest_provides_test_infrastructure_ports_fixture>
              Should verify tests/conftest.py provides test_infrastructure_ports fixture.

              This fixture is the centralized source of truth for test port mappings.
            <Function test_port_fixture_documents_all_services>
              Should verify test_infrastructure_ports fixture documents all services.

              Expected services:
              - postgres: 9432
              - redis: 9379
              - openfga: 9080
              - qdrant: 9333
            <Function test_integration_tests_document_port_usage>
              DOCUMENTATION TEST: Best practices for port configuration in integration tests.

              This test serves as living documentation, not strict enforcement.
        <Module test_shell_and_docker.py>
          Tests for shell script safety and Docker Compose health checks.

          Following TDD principles:
          1. RED: Verify shell scripts quote variables and validate paths
          2. GREEN: Fix shellcheck warnings and Docker health checks
          3. REFACTOR: Document safety patterns
          <Class TestShellScriptSafety>
            Test shell scripts for safety and proper quoting.
            <Function test_check_pr_status_quotes_variables>
              Test that check-pr-status.sh properly quotes all variables.

              This test will initially FAIL because variables are unquoted.
              After fix, all variable expansions should be quoted to prevent word-splitting.
            <Function test_build_infisical_wheels_rm_safety>
              Test that build-infisical-wheels.sh validates OUTPUT_DIR before rm -rf.

              This test will initially FAIL if path validation is missing.
              After fix, should validate OUTPUT_DIR is not empty or root before deletion.
          <Class TestDockerComposeHealthChecks>
            Test Docker Compose health check configurations.
            <Function test_keycloak_health_check_uses_native_command>
              Test that Keycloak health check uses native kc.sh command instead of curl.

              This test will initially FAIL because curl is not available in Keycloak image.
              After fix, should use /opt/keycloak/bin/kc.sh show-health or similar.
            <Function test_qdrant_health_check_uses_available_command>
              Test that Qdrant health check uses TCP port check (not wget/curl/grpc_health_probe).

              Qdrant v1.15.1 image does not include wget, curl, or grpc_health_probe.
              Use TCP-based health check via /dev/tcp which requires no external tools.
            <Function test_all_health_checks_have_proper_intervals>
              Test that all services have reasonable health check intervals.
      <Module test_async_fixture_validation.py>
        Meta-tests for validating AsyncIO fixture configuration.

        These tests ensure that:
        1. pyproject.toml has asyncio_default_fixture_loop_scope = "session"
        2. AsyncPG connection pool fixtures have scope="session"
        3. No event loop mismatch warnings occur during test execution

        Context:
        Per INTEGRATION_TEST_FINDINGS.md, AsyncPG event loop tracking issues can cause
        "Future attached to different loop" errors when connection pools are created
        in one event loop but used in another.

        The fix is documented in pyproject.toml:471 with asyncio_default_fixture_loop_scope = "session".

        These meta-tests validate the configuration is correct and prevent regressions.

        Test Coverage:
        1. Verify pyproject.toml has correct asyncio_default_fixture_loop_scope
        2. Verify asyncpg pool fixtures use scope="session"
        3. Document event loop validation logging pattern
        <Class TestAsyncIOConfiguration>
          Validate pytest-asyncio configuration for event loop scope
          <Function test_pyproject_toml_has_asyncio_fixture_loop_scope>
            Should verify pyproject.toml has asyncio_default_fixture_loop_scope = "session".

            This prevents event loop mismatches when session-scoped async fixtures
            (like asyncpg connection pools) are used across tests.

            Without this configuration:
            - Each test gets a new event loop
            - Session-scoped fixtures created in one loop fail when accessed in another
            - Results in "Future attached to different loop" errors

            With asyncio_default_fixture_loop_scope = "session":
            - All tests share the same event loop for session-scoped fixtures
            - Async connection pools work correctly across tests
          <Function test_pyproject_toml_asyncio_mode_is_auto>
            Should verify pytest-asyncio mode is AUTO.

            This enables automatic detection of async test functions without
            requiring manual @pytest.mark.asyncio decorators.
        <Class TestAsyncPGFixtureConfiguration>
          Validate AsyncPG connection pool fixture configuration
          <Function test_asyncpg_pool_fixtures_use_session_scope>
            Should verify AsyncPG connection pool fixtures have scope="session".

            Session-scoped connection pools:
            - Created once per test session
            - Shared across all tests
            - Use the same event loop (with asyncio_default_fixture_loop_scope = "session")

            Without session scope:
            - Pools created/destroyed for each test (slow)
            - Potential event loop mismatches
            - Connection pool exhaustion
          <Function test_postgres_fixtures_use_create_pool>
            Should verify postgres fixtures use asyncpg.create_pool() for connection pooling.

            Connection pools provide:
            - Connection reuse across tests
            - Automatic connection management
            - Better performance than creating connections per-test
        <Class TestEventLoopValidation>
          Document event loop validation patterns
          <Function test_event_loop_logging_pattern_documentation>
            DOCUMENTATION TEST: Pattern for logging event loop IDs in fixtures.

            This test serves as living documentation for adding event loop validation
            logging to detect mismatches during development/debugging.

            Recommended pattern (from INTEGRATION_TEST_FINDINGS.md):
            ```python
            import asyncio
            import logging

            @pytest.fixture(scope="session")
            async def postgres_connection_real(integration_test_env):
                pool_event_loop = asyncio.get_running_loop()
                logger.info(f"Creating asyncpg pool in event loop {id(pool_event_loop)}")

                pool = await asyncpg.create_pool(...)

                yield pool

                current_loop = asyncio.get_running_loop()
                if current_loop != pool_event_loop:
                    logger.warning(
                        f"Pool created in loop {id(pool_event_loop)} "
                        f"but closed in {id(current_loop)} - potential mismatch"
                    )

                await pool.close()
            ```

            Benefits:
            - Detects event loop mismatches during development
            - Provides audit trail of fixture lifecycle
            - Helps diagnose "Future attached to different loop" errors
            - Validates asyncio_default_fixture_loop_scope is working
          <Function test_no_event_loop_fixture_conflicts>
            Should verify tests don't define custom event_loop fixtures.

            Custom event_loop fixtures can conflict with pytest-asyncio's
            automatic event loop management and cause subtle issues.

            If found, recommend removing and using pytest-asyncio's built-in
            event loop management with asyncio_default_fixture_loop_scope.
        <Class TestAsyncFixtureBestPractices>
          Document and validate async fixture best practices
          <Function test_async_fixture_best_practices_documentation>
            DOCUMENTATION TEST: Best practices for async fixtures in pytest.

            This test serves as living documentation, not strict enforcement.
      <Module test_async_mock_configuration.py>
        Meta-test: Validate AsyncMock configuration across all test files.

        This test ensures that all AsyncMock instances have explicit return_value
        or side_effect configuration to prevent authorization bypass bugs and
        pytest-xdist state pollution.

        Run with: pytest tests/meta/test_async_mock_configuration.py -v
        <Function test_all_async_mocks_have_return_values>
          Verify all AsyncMock instances have explicit configuration.

          This test runs the check_async_mock_configuration.py script to detect
          unconfigured AsyncMock instances across the test suite.

          SECURITY: Unconfigured AsyncMock returns truthy values, causing
          authorization checks to incorrectly pass. This was the root cause
          of the SCIM security bug (commit abb04a6a).

          See: tests/ASYNC_MOCK_GUIDELINES.md for details.
        <Function test_async_mock_guidelines_exist>
          Verify AsyncMock guidelines documentation exists.
        <Function test_async_mock_checker_script_exists>
          Verify AsyncMock configuration checker script exists.
      <Module test_asyncmock_validation.py>
        Tests for AsyncMock validation pre-commit hook.

        Ensures that AsyncMock is always instantiated, never assigned as a class.

        Test Coverage:
        - Detects AsyncMock class assignments
        - Allows AsyncMock instance creation
        - Provides helpful error messages
        - Handles edge cases (comments, strings, etc.)
        <Class TestAsyncMockValidation>
          Tests for AsyncMock usage validation
          <Function test_detects_asyncmock_class_assignment>
            Should detect and reject AsyncMock class assignments
          <Function test_allows_asyncmock_instance_creation>
            Should allow AsyncMock instance creation
          <Function test_ignores_asyncmock_in_comments>
            Should ignore AsyncMock in comments
          <Function test_ignores_asyncmock_in_strings>
            Should ignore AsyncMock in strings
      <Module test_ci_workflow_redundancy.py>
        Meta-tests validating CI workflow efficiency and preventing redundant test execution.

        These tests ensure:
        1. No duplicate pytest invocations across workflows
        2. Test markers are used efficiently
        3. CI resources are not wasted on redundant test runs

        Reference: Testing Infrastructure Validation (2025-11-21)
        Finding: CI/CD redundancy - API tests run twice (ci.yaml + e2e-tests.yaml)
        <Class TestCIWorkflowRedundancy>
          Validate that CI workflows don't have redundant test executions.
          <Function test_no_duplicate_pytest_invocations_across_workflows>
            Validate that pytest commands with same markers don't run in multiple workflows.

            FINDING: API tests (-m "api and unit") were running in both ci.yaml and
            e2e-tests.yaml, wasting ~2-3 minutes per CI run.

            References:
            - .github/workflows/ci.yaml - Main CI pipeline
            - .github/workflows/e2e-tests.yaml - E2E test pipeline
          <Function test_e2e_workflow_only_runs_e2e_tests>
            Validate that e2e-tests.yaml workflow only runs E2E tests, not unit/API tests.

            FINDING: e2e-tests.yaml was running API tests that already ran in ci.yaml.

            References:
            - .github/workflows/e2e-tests.yaml
          <Function test_ci_workflow_has_comprehensive_test_coverage>
            Validate that ci.yaml runs all fast test types (unit, API).

            This ensures the main CI pipeline catches issues quickly.
      <Module test_claude_code_configuration.py>
        Test suite for Claude Code configuration validation.

        Validates that Claude Code integration follows Anthropic's best practices:
        - .claude/settings.json exists and has valid structure
        - Slash commands are valid markdown
        - CLAUDE.md contains required workflow sections
        - .mcp.json is valid JSON
        - Templates have required frontmatter

        Following TDD: These tests are written FIRST, before configuration exists.
        They will FAIL initially (RED phase), then PASS after implementation (GREEN phase).

        Regression prevention for Anthropic Claude Code best practices compliance.
        See: https://www.anthropic.com/engineering/claude-code-best-practices
        <Class TestClaudeCodeConfiguration>
          Validate Claude Code configuration files exist and are properly structured.
          <Function test_claude_settings_json_exists>
            Test that .claude/settings.json exists.
          <Function test_claude_settings_json_valid_json>
            Test that .claude/settings.json is valid JSON.
          <Function test_claude_settings_has_allowed_tools>
            Test that settings.json has allowedTools configuration.
          <Function test_claude_settings_has_webfetch_domains>
            Test that settings.json has WebFetch allowed_domains.
          <Function test_mcp_json_exists>
            Test that .mcp.json exists in project root.
          <Function test_mcp_json_valid_structure>
            Test that .mcp.json has valid structure.
          <Function test_claude_md_exists>
            Test that .github/CLAUDE.md exists.
          <Function test_claude_md_has_workflow_section>
            Test that CLAUDE.md contains Explore→Plan→Code workflow section.
          <Function test_slash_commands_directory_exists>
            Test that .claude/commands/ directory exists.
          <Function test_slash_commands_are_markdown>
            Test that all slash commands are .md files.
          <Function test_new_slash_commands_exist>
            Test that new recommended slash commands exist.
          <Function test_templates_directory_exists>
            Test that .claude/templates/ directory exists.
          <Function test_templates_are_markdown>
            Test that all templates are .md files.
          <Function test_context_directory_exists>
            Test that .claude/context/ directory exists with key files.
          <Function test_no_settings_local_in_git>
            Test that settings.local.json is not tracked in git.
        <Class TestSlashCommandQuality>
          Validate slash commands have proper structure and content.
          <Function test_all_commands_have_description>
            Test that all slash commands have a description.
      <Module test_claude_settings_schema.py>
        Test suite for .claude/settings.json schema validation.

        Validates that Claude Code settings.json follows best practices:
        - Allowed domains are valid (no typos, proper format)
        - No overly permissive wildcards
        - Bash auto-approve patterns are safe
        - Plan mode configuration is valid

        Following TDD: These tests are written FIRST, before settings.json exists.
        They will FAIL initially (RED phase), then PASS after implementation (GREEN phase).

        Regression prevention for Anthropic Claude Code settings configuration.
        See: https://www.anthropic.com/engineering/claude-code-best-practices
        <Class TestClaudeSettingsSchema>
          Validate .claude/settings.json schema and content.
          <Function test_webfetch_domains_are_valid>
            Test that all WebFetch allowed_domains are valid domain names.
          <Function test_no_wildcard_domains>
            Test that WebFetch allowed_domains doesn't contain wildcards.
          <Function test_essential_documentation_domains_present>
            Test that essential documentation domains are in allowed_domains.
          <Function test_project_specific_domains_present>
            Test that project-specific domains are in allowed_domains.
          <Function test_bash_auto_approve_patterns_are_safe>
            Test that Bash auto_approve_patterns don't contain dangerous commands.
          <Function test_bash_patterns_are_read_only_or_safe>
            Test that Bash auto_approve_patterns are mostly read-only commands.
          <Function test_plan_mode_configuration_is_valid>
            Test that plan_mode configuration has valid structure.
          <Function test_no_duplicate_domains>
            Test that there are no duplicate domains in allowed_domains.
        <Class TestSettingsLocalExclusion>
          Validate that settings.local.json is properly excluded from git.
          <Function test_gitignore_excludes_settings_local>
            Test that .gitignore excludes settings.local.json.
      <Module test_codex_config_validation.py>
        Validation tests for Codex recommendations configuration.

        Validates that pyproject.toml has the correct settings based on OpenAI Codex
        findings from 2025-11-14:
        - pytest-benchmark warning suppression
        - 80% coverage threshold enforcement
        <Class TestCodexConfigurationRecommendations>
          Validate Codex-recommended configuration changes.
          <Function test_pytest_benchmark_disabled_by_default>
            Verify that pytest-benchmark is disabled by default to suppress warnings.

            **Codex Finding:**
            - pytest-benchmark emits warning when xdist is active
            - Warning: "Benchmarks are automatically disabled because xdist plugin is active"
            - Source: pytest_benchmark/logger.py:39

            **Fix:**
            - Add --benchmark-disable to pytest addopts
            - Allows opt-in benchmarking with --benchmark-enable
            - Eliminates warning noise in test output
          <Function test_coverage_threshold_enforced>
            Verify that coverage threshold is enforced to prevent regressions.

            **Codex Finding:**
            - Current coverage: 64% (baseline as of 2025-11-14)
            - Many infrastructure modules untested (kubernetes_sandbox: 10%, server_streamable: 20%)
            - Recommendation: Enforce threshold to prevent regressions, improve to 80%+

            **Current Status:**
            - Baseline: 64% (prevents coverage from dropping below current level)
            - Target: 80%+ (systematic improvement plan in docs-internal/COVERAGE_IMPROVEMENT_PLAN.md)
            - Threshold will be incrementally raised as modules are tested

            **Fix:**
            - fail_under = 64 (baseline) in tool.coverage.report
            - CI/CD will fail if coverage drops below baseline
            - Prevents coverage regressions while allowing incremental improvement
          <Function test_pytest_markers_include_codex_categories>
            Verify that pytest markers include categories used in Codex validation.

            Ensures regression, meta, unit markers are properly defined.
          <Function test_coverage_omits_test_directories>
            Verify that test directories are excluded from coverage.

            Ensures coverage measures production code, not test code.
          <Function test_coverage_parallel_enabled_for_xdist>
            Verify that parallel coverage is enabled for pytest-xdist.

            Required for accurate coverage collection with pytest-xdist.
          <Function test_benchmark_config_allows_marker_filtering>
            Verify that pytest-benchmark config allows marker-based filtering.

            Codex notes that benchmarks should use marker-based filtering
            rather than complete disablement.
      <Module test_codex_findings_validation.py>
        Meta-tests validating Codex findings are resolved and preventing regression.

        These tests ensure:
        1. Fixture scopes are correct (session-scoped for integration fixtures)
        2. Keycloak service is enabled and properly configured
        3. Tests skip gracefully when services unavailable
        4. No strict xfail markers on placeholder tests

        Reference: Codex Integration Failures Analysis (2025-11-13)
        ADR: docs-internal/ADR-0053-codex-findings-validation.md
        <Class TestFixtureScopeValidation>
          Validate that integration fixtures have correct scopes to prevent ScopeMismatch errors.
          <Function test_integration_test_env_fixture_is_session_scoped>
            Validate integration_test_env fixture is session-scoped.

            CODEX FINDING: ScopeMismatch propagation for OpenFGA/SCIM security tests
            came from test-runner image using function-scoped fixture. This test
            ensures the fixture remains session-scoped.

            References:
            - tests/conftest.py:970 - integration_test_env fixture definition
          <Function test_dependent_fixtures_are_session_scoped>
            Validate that fixtures dependent on integration_test_env are also session-scoped.

            CODEX FINDING: Dependent fixtures (postgres_connection_real, redis_client_real,
            openfga_client_real) must have matching scope to prevent ScopeMismatch.

            References:
            - tests/fixtures/database_fixtures.py - database fixtures location
          <Function test_openfga_scim_test_fixtures_compatible_with_session_scope>
            Validate OpenFGA/SCIM security tests use session-compatible fixtures.

            CODEX FINDING: ScopeMismatch errors in OpenFGA/SCIM security tests when
            using pytest-xdist parallel execution.

            References:
            - tests/security/test_scim_service_principal_openfga.py
          <Function test_pytest_asyncio_event_loop_scope_matches_session_fixtures>
            Validate pytest-asyncio default event loop scope is session-scoped.

            CODEX FINDING (2025-11-20): 34 integration tests fail with ScopeMismatch
            because session-scoped async fixtures (postgres_connection_real, redis_client_real,
            openfga_client_real) require session-scoped event loop.

            Problem:
            - pyproject.toml sets asyncio_default_fixture_loop_scope = "function"
            - Session-scoped async fixtures try to use function-scoped event loop
            - pytest-asyncio 1.2.0 raises ScopeMismatch error

            Solution: Set asyncio_default_fixture_loop_scope = "session"

            References:
            - pyproject.toml:469 - asyncio_default_fixture_loop_scope configuration
            - tests/conftest.py:1404 - postgres_connection_real (session-scoped)
            - tests/conftest.py:1453 - redis_client_real (session-scoped)
            - tests/conftest.py:1483 - openfga_client_real (session-scoped)
            - pytest-asyncio docs: https://pytest-asyncio.readthedocs.io/en/latest/concepts.html#event-loop-scope
        <Class TestKeycloakServiceConfiguration>
          Validate Keycloak service is enabled and properly configured in docker-compose.
          <Function test_keycloak_service_enabled_in_docker_compose>
            Validate Keycloak service is uncommented and enabled in docker-compose.test.yml.

            CODEX FINDING: TestStandardUserJourney::test_01_login fails because Keycloak
            service is commented out in docker/docker-compose.test.yml:74.

            Decision: Enable Keycloak for full E2E auth testing (trade-off: +60s startup).
          <Function test_keycloak_service_has_health_check>
            Validate Keycloak service has proper health check configuration.

            Health checks ensure tests only run when Keycloak is ready.
          <Function test_keycloak_environment_variables_configured>
            Validate Keycloak has required environment variables.

            Required vars:
            - Admin credentials: KEYCLOAK_ADMIN (legacy) OR KC_BOOTSTRAP_ADMIN_USERNAME (Keycloak 22+)
            - Database: KC_DB, KC_DB_URL
            - Health: KC_HEALTH_ENABLED
        <Class TestGracefulServiceSkipping>
          Validate tests skip gracefully when optional services are unavailable.
          <Function test_keycloak_tests_skip_when_service_unavailable>
            Validate E2E tests using Keycloak skip gracefully when service is down.

            CODEX FINDING: TestStandardUserJourney::test_01_login should skip when
            Keycloak is unreachable, not fail.

            References:
            - tests/e2e/test_full_user_journey.py::test_01_login
        <Class TestPlaceholderTestMarkers>
          Validate placeholder tests don't have strict xfail markers that cause XPASS errors.
          <Function test_service_principal_lifecycle_no_strict_xfail>
            Validate test_full_service_principal_lifecycle doesn't have strict xfail marker.

            CODEX FINDING: Test reports [XPASS(strict)] because it's a placeholder with
            strict xfail that "passes" (no assertions yet).

            Solution: Remove xfail or implement full test.

            References:
            - tests/integration/api/test_service_principals_endpoints.py::test_full_service_principal_lifecycle

            Note: File was moved from tests/api/ to tests/integration/api/ in project reorganization.
        <Class TestDockerImageContents>
          Validate Docker test image includes necessary directories.
          <Function test_dockerfile_copies_required_directories_for_integration_tests>
            Validate Dockerfile final-test stage copies src/, tests/, and pyproject.toml.
            Per ADR-0053, scripts/ and deployments/ must be EXCLUDED from Docker image.

            CODEX FINDING: ModuleNotFoundError for 'scripts' module when importing
            documentation validators. ADR-0053 resolution: scripts/ and deployments/
            are excluded from Docker image. Meta-tests requiring these directories
            run on the host, not in containers.

            This test validates ADR-0053 compliance.

            References:
            - docker/Dockerfile final-test stage (lines 262-265)
            - ADR-0053: Docker Image Contents Policy
          <Function test_meta_tests_run_on_host_not_docker>
            Validate meta-tests (requiring scripts/) are not marked as integration tests.

            Meta-tests should run on host with full repo context, not in Docker container.
        <Class TestCodexFindingsValidationSummary>
          Summary test confirming all Codex findings have validation coverage.
          <Function test_all_codex_findings_have_validation_tests>
            Confirm all Codex findings mentioned in the report have corresponding tests.

            Codex Findings:
            1. ✅ ScopeMismatch propagation - Validated by test_integration_test_env_fixture_is_session_scoped
            2. ✅ Keycloak service unreachable - Validated by test_keycloak_service_enabled_in_docker_compose
            3. ✅ XPASS(strict) placeholder - Validated by test_service_principal_lifecycle_no_strict_xfail
            4. ✅ ModuleNotFoundError scripts - Validated by test_dockerfile_copies_required_directories
            5. ✅ FileNotFoundError deployments - Validated by test_dockerfile_copies_required_directories
            6. ✅ Graceful skipping - Validated by test_keycloak_tests_skip_when_service_unavailable
      <Module test_codex_regression_prevention.py>
        Meta-tests for OpenAI Codex Finding Regression Prevention

        These tests use AST analysis to detect patterns that were identified
        as problematic in the OpenAI Codex audit. They ensure these issues
        cannot recur as the codebase evolves.

        Following TDD principles - written to prevent regression of fixes.
        <Class TestUnconditionalSkipDetection>
          Detect unconditional pytest.skip() calls that should be xfail
          <Function test_no_unconditional_skips_in_e2e_tests>
            CODEX FINDING: E2E tests had 35 unconditional pytest.skip() calls.

            Prevention: Detect any unconditional pytest.skip() at top of test functions.
            These should use @pytest.mark.xfail(strict=True) instead.
        <Class TestStateIsolationPatterns>
          Detect state mutations without proper isolation
          <Function test_no_manual_try_finally_for_settings_mutations>
            CODEX FINDING: 34 try/finally blocks for settings restoration.

            Prevention: Detect manual try/finally around settings mutations.
            Should use monkeypatch fixture instead.
        <Class TestCLIToolGuards>
          Detect subprocess calls to CLI tools without availability guards
          <Function test_all_cli_subprocess_calls_have_guards>
            CODEX FINDING: subprocess calls to kubectl, kustomize, terraform, helm
            without availability guards caused hard failures.

            Prevention: Detect unguarded subprocess.run() calls to these tools.
        <Class TestPrivateAPIUsage>
          Detect tests calling private methods (leading underscore)
          <Function test_llm_property_tests_use_public_apis>
            CODEX FINDING: Property tests called _format_messages(), _setup_environment().

            Prevention: Detect LLM property tests calling private methods.
            Tests should test public behavior, not implementation details.

            NOTE: test_cache_properties.py still has _get_ttl_from_key calls.
            This is documented as technical debt for future refactoring.
        <Class TestXFailStrictUsage>
          Verify xfail(strict=True) is used for unimplemented tests
          <Function test_e2e_placeholder_tests_use_xfail_strict>
            CODEX FINDING: E2E tests used pytest.skip() for placeholders.

            Prevention: Verify E2E placeholder tests use @pytest.mark.xfail(strict=True).
            This ensures CI fails when features are implemented, forcing test enablement.
        <Class TestMonkeypatchUsage>
          Verify monkeypatch is used for settings isolation
          <Function test_distributed_checkpointing_uses_monkeypatch>
            CODEX FINDING: test_distributed_checkpointing.py had 34 manual try/finally blocks.

            Prevention: Verify all tests use monkeypatch parameter for settings isolation.
        <Class TestRequiresToolDecorator>
          Verify @requires_tool decorator is used consistently
          <Function test_kustomize_tests_use_requires_tool_decorator>
            CODEX FINDING: Kustomize tests had manual shutil.which() checks.

            Prevention: Verify kustomize tests use @requires_tool decorator.
        <Class TestDeadCodeInFixtures>
          Detect dead code after return statements in test fixtures
          <Function test_no_dead_code_after_fixture_returns>
            CODEX FINDING: Dead test code in test_code_generator.py:33-64.

            Prevention: Detect code after return statements in pytest fixtures.
            This code never executes and represents lost test coverage.

            Pattern detected:
                @pytest.fixture
                def my_fixture():
                    return value

                    # Dead code - never executes!
                    assert something
        <Class TestCodexFindingCompliance>
          High-level validation that all Codex findings are addressed
          <Function test_codex_validation_commit_exists>
            Verify Codex findings validation commits exist in git history

            Note: Searches last 100 commits to account for recent work on other areas.
            Codex findings commits exist but may be older than 20 commits.
          <Function test_critical_findings_have_fixes>
            Verify critical findings (E2E skips, state mutations, CLI guards) are fixed
        <Class TestInfrastructurePortIsolation>
          Detect hard-coded infrastructure ports that break pytest-xdist worker isolation.

          CODEX FINDING (2025-11-13):
          Infra fixture still conflicts under xdist – tests/conftest.py:664 and tests/conftest.py:675
          hard-code health checks to http://localhost:9080/9082, while test_infrastructure_ports
          pretends to offset ports per worker. Because docker-compose.test.yml:25-90 exposes fixed
          host ports and Makefile:411 runs E2E with pytest -n auto, parallel workers race on the
          same Keycloak/OpenFGA endpoints.

          Prevention: Ensure all health check URLs use dynamic ports from test_infrastructure_ports fixture.
          <Function test_no_hard_coded_ports_in_conftest_health_checks>
            Verify conftest.py health checks use test_infrastructure_ports fixture, not hard-coded ports.

            RED phase (initial): Will fail because lines 667 and 675 have hard-coded ports
            GREEN phase (after fix): Will pass when using f-strings with test_infrastructure_ports
          <Function test_docker_compose_ports_are_documented_as_serial_only>
            Verify docker-compose.test.yml documents that it requires serial test execution.

            Until docker-compose templating is implemented, E2E tests must run with pytest -n0.
      <Module test_codex_validations.py>
        Meta-tests to validate OpenAI Codex findings remediation.

        These tests validate that identified issues have been properly fixed:
        1. E2E tests use real clients instead of mocks
        2. Documentation accurately reflects implementation
        3. No bare @pytest.mark.skip markers (should use xfail with strict=True)
        4. TODO tests have proper xfail markers

        These tests follow TDD: they FAIL initially (RED), pass after fixes (GREEN).
        <Class TestCodexFindingsRemediation>
          Meta-tests validating OpenAI Codex findings have been addressed.

          GIVEN: OpenAI Codex identified 4 valid issues in test suite
          WHEN: Fixes are implemented following TDD principles
          THEN: These meta-tests validate the fixes are complete
          <Function test_e2e_uses_real_clients_not_mocks>
            GIVEN: E2E test file (test_full_user_journey.py)
            WHEN: Checking imports and function calls
            THEN: Should import from real_clients, not helpers mocks
            AND: Should call real_keycloak_auth and real_mcp_client
            AND: Should NOT call mock_keycloak_auth or mock_mcp_client

            Codex Finding #3: E2E tests still use mock_keycloak_auth/mock_mcp_client
          <Function test_helpers_documentation_reflects_reality>
            GIVEN: E2E helpers file with documentation
            WHEN: Checking documentation claims vs implementation
            THEN: Documentation should NOT claim migration is complete
            OR: If claiming complete, real clients must be fully implemented

            Codex Finding #4: Documentation claims "Migrated" but code uses mocks
          <Function test_no_bare_skip_markers>
            GIVEN: Test files with @pytest.mark.skip markers
            WHEN: Checking for proper test markers
            THEN: Should use @pytest.mark.xfail(strict=True) instead of skip
            AND: Should include descriptive reason parameter

            Codex Finding #6: test_cost_tracker.py has @pytest.mark.skip
            without xfail(strict=True)

            Note: Some skip markers are acceptable (e.g., skip if dependency missing).
            This test focuses on skip markers that should be xfail for incomplete features.
          <Function test_todo_tests_have_xfail_markers>
            GIVEN: Test files with TODO placeholder tests
            WHEN: Checking test functions with only 'pass' and TODO comments
            THEN: Should have @pytest.mark.xfail(strict=True) decorator

            Codex Finding #9: test_gdpr.py:728-731 has TODO test without xfail marker
          <Function test_hypothesis_configuration_is_guarded>
            GIVEN: tests/conftest.py with Hypothesis configuration
            WHEN: Hypothesis is not available (import fails)
            THEN: Hypothesis profile configuration should be wrapped in availability check
            AND: Should not cause AttributeError when settings is None

            Codex Finding #3 (New): Hypothesis configuration runs unconditionally
            even when import fails, causing AttributeError on machines without Hypothesis.
        <Class TestCodexValidationMetaTest>
          Meta-test the meta-tests themselves.
          <Function test_codex_validation_file_exists>
            GIVEN: Meta-test file for Codex validations
            WHEN: Checking file existence
            THEN: This file should exist at tests/meta/test_codex_validations.py
          <Function test_meta_tests_are_well_documented>
            GIVEN: Meta-test file
            WHEN: Checking documentation
            THEN: Each test should have clear docstring with GIVEN/WHEN/THEN
      <Module test_compose_file_usage.py>
        Meta-tests for Docker Compose file usage enforcement.

        These tests validate that only the root docker-compose.test.yml is used
        across all contexts (scripts, Makefile, CI, conftest), ensuring local/CI parity.

        PURPOSE:
        --------
        Prevent the local integration tooling vs CI stack mismatch identified in
        OpenAI Codex findings. Ensure developers and CI use identical infrastructure.

        VALIDATION:
        -----------
        1. Only root docker-compose.test.yml exists and is referenced
        2. docker/docker-compose.test.yml is removed/deprecated
        3. All scripts, Makefile targets, and conftest use root compose file
        4. CI workflows use root compose file

        References:
        - OpenAI Codex Finding #1: Local integration tooling vs CI stack mismatch
        - docker-compose.test.yml (root): Canonical infrastructure definition
        <Class TestComposeFileConsolidation>
          Validate that compose file usage is consolidated on root file.

          These meta-tests ensure local/CI parity and prevent topology mismatches.
          <Function test_root_compose_file_exists>
            🟢 GREEN: Verify root docker-compose.test.yml exists.

            This is the canonical compose file for all testing contexts.
          <Function test_legacy_compose_file_removed>
            🟢 GREEN: Verify legacy docker/docker-compose.test.yml is removed.

            The legacy compose file created local/CI topology mismatches.
            It should be completely removed after consolidation.
          <Function test_test_integration_script_uses_root_compose>
            🟢 GREEN: Verify scripts/test-integration.sh uses root compose file.

            The integration test script must use the same compose file as CI.
          <Function test_makefile_uses_root_compose>
            🟢 GREEN: Verify Makefile test-integration targets use root compose file.

            All Makefile integration test targets must reference root compose file.
          <Function test_conftest_uses_root_compose>
            🟢 GREEN: Verify tests/conftest.py uses root compose file.

            The docker_compose_file fixture must return root compose file path.
          <Function test_ci_workflows_use_root_compose>
            🟢 GREEN: Verify CI workflows use root compose file.

            GitHub Actions workflows must use root docker-compose.test.yml.
          <Function test_no_test_runner_container_references>
            🟢 GREEN: Verify no references to legacy test-runner container pattern.

            The containerized test execution pattern (tests run inside Docker) should
            be completely removed in favor of host-based execution (CI parity).
        <Class TestComposeFileDocumentation>
          Validate compose file usage is properly documented.
          <Function test_testing_md_documents_compose_usage>
            🟢 GREEN: Verify TESTING.md documents compose file usage.

            Documentation should guide developers to use root compose file.
          <Function test_compose_file_has_documentation_comments>
            🟢 GREEN: Verify root compose file has documentation comments.

            The compose file should explain its purpose and usage.
        <Class TestComposeFileConsistency>
          Validate compose file configuration is consistent.
          <Function test_compose_port_mappings_documented>
            🟢 GREEN: Verify compose file port mappings follow documented pattern.

            Ports should use 9XXX range to avoid conflicts with development services.
          <Function test_compose_services_match_conftest_expectations>
            🟢 GREEN: Verify compose services match conftest.py expectations.

            Service names in compose file should match what conftest.py expects.
        <Class TestComposeFileEnforcement>
          Document the enforcement strategy for compose file consolidation.
          <Function test_enforcement_strategy_documentation>
            📚 Document the enforcement strategy for compose file usage.

            Multiple layers ensure developers use the correct compose file.
          <Function test_consolidation_benefits_documented>
            📚 Document the benefits of compose file consolidation.

            Explain why this consolidation improves the development experience.
      <Module test_conftest_fixtures_plugin_enhancements.py>
        Tests for conftest_fixtures_plugin.py enhancements.

        Tests the runtime validation of FastAPI dependency override patterns,
        specifically the bearer_scheme requirement when overriding get_current_user.

        This validates the additional improvements from OpenAI Codex findings:
        - Extend conftest_fixtures_plugin.py to validate dependency overrides
        - Fail fast if async deps are overridden with sync callables
        - Fail fast if bearer_scheme isn't overridden when get_current_user is

        References:
        - OpenAI Codex Findings: Additional Improvements section
        - tests/conftest_fixtures_plugin.py
        - PYTEST_XDIST_BEST_PRACTICES.md
        <Class TestConfTestFixturesPluginEnhancements>
          Tests for enhanced fixture validation.
          <Function test_plugin_exists_and_is_loaded>
            🟢 GREEN: Verify the fixture organization plugin is loaded.

            This test validates that conftest_fixtures_plugin.py is loaded
            and active during test execution.
          <Function test_plugin_validates_fixture_organization>
            🟢 GREEN: Test that plugin validates fixture organization.

            The plugin should detect duplicate autouse fixtures and
            enforce that module/session-scoped autouse fixtures are only
            in conftest.py.

            This is the EXISTING functionality that we're building on.
          <Function test_bearer_scheme_validation_documentation>
            📚 Document the bearer_scheme validation enhancement.

            This test documents what the enhancement should do:

            ENHANCEMENT: Runtime Validation of FastAPI Dependency Overrides
            ================================================================

            The conftest_fixtures_plugin.py should be extended to validate:

            1. **bearer_scheme Override Requirement:**
               When get_current_user is overridden, bearer_scheme must also
               be overridden to prevent singleton pollution.

               Example violation:
                   app.dependency_overrides[get_current_user] = mock_async_func
                   # MISSING: app.dependency_overrides[bearer_scheme] = lambda: None

            2. **Async Override for Async Dependencies:**
               Async dependencies must be overridden with async callables.

               Example violation:
                   app.dependency_overrides[get_current_user] = lambda: user
                   # Should be: async def mock(): return user

            3. **Dependency Override Cleanup:**
               Fixtures that set dependency_overrides must clear them in teardown.

               Example violation:
                   @pytest.fixture
                   def test_app():
                       app.dependency_overrides[...] = ...
                       return app
                       # MISSING: yield + app.dependency_overrides.clear()

            Implementation Strategy:
            ------------------------
            The plugin could scan test files at collection time and:
            - Look for app.dependency_overrides[get_current_user] assignments
            - Verify bearer_scheme is also overridden
            - Verify overrides use async def for async dependencies
            - Verify fixtures with overrides have cleanup

            However, this is COMPLEX because:
            - Requires AST analysis of fixture code
            - Requires knowing which dependencies are async
            - May have false positives

            ALTERNATIVE APPROACH (Recommended):
            ------------------------------------
            Instead of runtime validation, we:
            1. Document the pattern in PYTEST_XDIST_BEST_PRACTICES.md ✅ (already done)
            2. Create validation script (validate_test_fixtures.py) ✅ (already exists)
            3. Create regression tests demonstrating correct pattern ✅ (already done)
            4. Add pre-commit hook to catch violations ✅ (already exists)
            5. Provide helper functions in tests/utils/mock_factories.py ✅ (already exists)

            Given the complexity and that we already have:
            - Validation script (validate_test_fixtures.py)
            - Pre-commit hooks
            - Regression tests
            - Documentation

            The runtime plugin enhancement is NICE-TO-HAVE but not critical.
            We should mark this as DEFERRED and focus on the other improvements.

            DECISION: Mark as future enhancement, focus on:
            - Splitting xdist_group markers
            - Adding infra test markers
            - Creating auth override sanity tests
            - Updating documentation
        <Function test_plugin_enhancement_decision>
          📝 Document decision on plugin enhancement implementation.

          DECISION: Defer runtime bearer_scheme validation in plugin
          ===========================================================

          Rationale:
          ----------
          1. **Already have comprehensive coverage:**
             - validate_test_fixtures.py script validates FastAPI patterns
             - Pre-commit hooks enforce validation
             - Regression tests demonstrate correct patterns
             - PYTEST_XDIST_BEST_PRACTICES.md documents requirements

          2. **Runtime validation is complex:**
             - Requires AST analysis of fixture implementation
             - Requires knowledge of which dependencies are async
             - May have false positives/negatives
             - Adds complexity to plugin code

          3. **Better alternatives exist:**
             - Static analysis via pre-commit hooks (faster, clearer errors)
             - Validation scripts run before commit
             - Regression tests catch violations in CI
             - Helper functions in mock_factories.py make it easy to do it right

          4. **Current enforcement is sufficient:**
             - All existing tests now follow the pattern
             - New code caught by pre-commit hooks
             - Regression tests prevent regressions

          Next Steps:
          -----------
          Instead of plugin enhancement, we'll:
          1. ✅ Split xdist_group markers by backend
          2. ✅ Add infra test markers
          3. ✅ Create auth override sanity tests
          4. ✅ Update PYTEST_XDIST_BEST_PRACTICES.md
          5. ✅ Update MEMORY_SAFETY_GUIDELINES.md

          If runtime validation becomes needed in the future, we can revisit.
          For now, the static analysis approach is superior.
      <Module test_consolidated_workflow_validator.py>
        Test consolidated GitHub workflow validator.

        This test validates that the consolidated workflow validator (combining context
        validation and action version validation) works correctly and prevents common
        GitHub Actions workflow issues.

        Following TDD principles - this test defines expected behavior before implementation.
        <Class TestConsolidatedWorkflowValidator>
          Verify consolidated GitHub workflow validator works correctly.

          This test calls scripts/validators/validate_github_workflows_comprehensive.py
          instead of duplicating validation logic. The script is the source of truth.

          Architecture Pattern:
          - Script = Source of truth (scripts/validators/validate_github_workflows_comprehensive.py)
          - Hook = Trigger (validate-github-workflows in .pre-commit-config.yaml)
          - Meta-Test = Validator of validator (this test)
          <Function test_validator_script_exists>
            Test that the consolidated workflow validator script exists.

            Expected location: scripts/validators/validate_github_workflows_comprehensive.py
          <Function test_validator_script_is_executable>
            Test that the validator script can be executed.

            The script should:
            1. Be a valid Python file
            2. Have a main() function or __main__ block
            3. Return exit code 0 when validation passes
          <Function test_validator_passes_on_valid_workflows>
            Test that validator passes when all workflows are valid.

            This validates that:
            1. Context usage matches enabled triggers
            2. Action versions are valid published tags
            3. Permissions are correctly configured
            4. Workflow YAML syntax is valid
          <Function test_validator_detects_invalid_context_usage>
            Test that validator detects undefined context variable usage.

            Example invalid patterns:
            - Using github.event.pull_request.* when pull_request trigger not enabled
            - Using github.event.workflow_run.* when workflow_run trigger not enabled
          <Function test_validator_detects_invalid_action_versions>
            Test that validator detects invalid action version tags.

            Invalid patterns to detect:
            - astral-sh/setup-uv@v7.1.1 (should be v7.1.0 or v7)
            - actions/cache@v4.3.0 (should be v4.2.0 or v4)
            - Very high major versions (e.g., @v99)
          <Function test_validator_detects_missing_permissions>
            Test that validator detects workflows creating issues without permissions.

            Workflows using github.rest.issues.create must have 'issues: write' permission.
          <Function test_validator_provides_helpful_error_messages>
            Test that validator provides actionable error messages.

            Error messages should include:
            - File name where error occurred
            - Line number (if applicable)
            - What the error is
            - How to fix it
          <Function test_validator_has_cli_interface>
            Test that validator has a proper CLI interface.

            Should support:
            - --repo-root argument to specify repository root
            - --verbose for detailed output
            - Exit code 0 for pass, 1 for validation errors, 2 for script errors
          <Function test_validator_consolidates_context_and_version_checks>
            Test that validator performs BOTH context and version validation.

            This ensures consolidation is complete - the script should:
            1. Check context usage (from validate_github_workflows.py)
            2. Check action versions (from test_github_actions_validation.py)
            3. Check permissions (from test_github_actions_validation.py)
            4. Check YAML syntax (from test_github_actions_validation.py)
          <Function test_validator_performance_is_reasonable>
            Test that validator completes in reasonable time.

            Should complete validation of all workflows in < 10 seconds.
        <Class TestConsolidatedWorkflowValidatorRegression>
          Regression prevention tests for consolidated workflow validator.

          These tests ensure that the consolidated validator prevents the same issues
          that the original separate validators detected.
          <Function test_validator_prevents_invalid_uv_version>
            Regression: Prevent astral-sh/setup-uv@v7.1.1 (invalid version).

            This was a critical finding from OpenAI Codex Phase 5.
            The consolidated validator must detect this.
          <Function test_validator_prevents_undefined_context_usage>
            Regression: Prevent using github.event.workflow_run.* without workflow_run trigger.

            This was the original issue that validate_github_workflows.py fixed.
            The consolidated validator must detect this.
          <Function test_validator_prevents_missing_issues_permission>
            Regression: Prevent workflows creating issues without 'issues: write' permission.

            This was detected by test_github_actions_validation.py permissions tests.
            The consolidated validator must detect this.
      <Module test_context_manager_quality.py>
        Meta-tests for Context Manager Test Quality

        These tests validate that context manager tests properly assert __exit__ is called.

        Purpose: Prevent regression of Codex Finding #3 (Redis checkpointer tests lack __exit__ assertions)
        <Class TestContextManagerTestQuality>
          Meta-tests that validate context manager tests have proper cleanup assertions.

          RED: These tests will fail if context manager tests lack __exit__ assertions.
          GREEN: Context manager tests properly validate cleanup.
          <Function test_context_manager_tests_assert_exit>
            Validate tests of context managers assert __exit__ is called.

            This prevents regressions of Codex Finding #3:
            "The Redis checkpointer lifecycle tests promise to guard cleanup but never
            assert that the context manager is closed, so regressions will slip through silently."

            RED: Fails if context manager tests lack __exit__ assertions.
            GREEN: All context manager tests properly validate cleanup.
          <Function test_context_manager_cleanup_functions_exist>
            Validate that context manager tests actually call cleanup functions.

            This ensures tests don't just create context managers but also test their cleanup.

            RED: Fails if cleanup functions aren't called.
            GREEN: All tests properly invoke cleanup.
        <Class TestContextManagerNaming>
          Validate context manager test naming conventions
          <Function test_context_manager_tests_have_descriptive_names>
            Context manager tests should describe what cleanup behavior is being tested.

            Good: test_redis_checkpointer_context_manager_cleanup
            Bad: test_context_manager, test_cleanup
      <Module test_coverage_enforcement.py>
        Coverage Enforcement - Minimum Coverage Threshold

        Validates that test coverage meets minimum quality standards.

        This meta-test prevents coverage regression by failing if:
        - Overall coverage < 64% (CI threshold)
        - Coverage decreases from previous baseline

        **PERFORMANCE OPTIMIZATION** (OpenAI Codex Finding - 2025-11-16):
        This test now reads existing `.coverage` file if available instead of
        re-running the full test suite. This eliminates duplicate test execution
        in pre-push hooks, saving 3-4 minutes per push.

        Workflow:
        1. Makefile PHASE 3 runs tests with coverage (creates `.coverage` file)
        2. This test reads `.coverage` and validates threshold
        3. Falls back to running tests if `.coverage` doesn't exist (CI/dev compatibility)

        Coverage targets:
        - Current: 65.78% (after Phase 1 improvements)
        - Minimum: 64% (CI threshold) ⚠️ MUST NOT DROP BELOW
        - Target: 80% (Codex recommendation) 🎯
        - Excellent: 90%+ ⭐

        Why this matters:
        - Low coverage = untested code paths
        - Untested code = potential bugs in production
        - Coverage regression = technical debt accumulation

        Coverage by module (after Phase 1):
        ✅ prometheus_client.py: 44% → 87% (+43%)
        ✅ budget_monitor.py: 47% → 81% (+34%)
        ✅ cost_api.py: 55% → 91% (+36%)
        ✅ cost_tracker.py: 76% (good)
        ⚠️ Other modules: Various (see coverage report)

        How to improve coverage:
        1. Identify low-coverage modules: pytest --cov --cov-report=term-missing
        2. Write tests for uncovered lines (see MISSING column)
        3. Focus on business logic first (highest value)
        4. Use TDD for new code (write tests first!)

        Related:
        - pyproject.toml: Coverage configuration (fail_under = 64)
        - .github/workflows/ci.yaml: CI coverage enforcement
        - Makefile: PHASE 3 runs tests with coverage (creates `.coverage` file)
        <Function test_minimum_coverage_threshold>
          Test that overall test coverage meets minimum 64% threshold.

          **OPTIMIZED** (2025-11-17): Reads existing `.coverage` file if available.
          Only runs tests if `.coverage` is missing (dev/CI fallback).

          Coverage is measured across all source files in src/mcp_server_langgraph/
          excluding test files, examples, and scripts.

          Performance:
          - With existing .coverage: < 1 second (fast!)
          - Without .coverage: ~2-3 minutes (fallback, runs tests with coverage)
      <Module test_docker_environment.py>
        Test Docker environment configuration for integration tests.

        These tests validate that the Docker test environment is properly configured
        with all required directories and dependencies. This prevents test collection
        errors when running tests in Docker containers.

        Regression Prevention:
        - Ensures scripts/ directory is available in Docker (for test imports)
        - Ensures deployments/ directory is available (for infrastructure tests)
        - Validates Dockerfile.test has required COPY directives

        Related Issues:
        - FileNotFoundError: '/app/deployments/overlays' (2025-11-14)
        - ModuleNotFoundError: No module named 'scripts' (2025-11-14)
        - ModuleNotFoundError: No module named 'fix_mdx_syntax' (2025-11-14)
        <Class TestDockerEnvironmentSetup>
          Test that Docker environment has required directories.
          <Function test_scripts_directory_exists>
            Test that scripts/ directory exists and is accessible.

            Required for:
            - tests/test_mdx_validation.py (imports from scripts/)
            - tests/unit/documentation/* (imports from scripts/validators/)
          <Function test_scripts_is_importable>
            Test that scripts can be imported as a module.
          <Function test_scripts_validators_exists>
            Test that scripts/validators/ subdirectory exists.
          <Function test_deployments_directory_exists>
            Test that deployments/ directory exists and is accessible.

            Required for:
            - tests/regression/test_pod_deployment_regression.py
          <Function test_deployments_overlays_exists>
            Test that deployments/overlays/ subdirectory exists.
          <Function test_docker_working_directory_is_app>
            Test that Docker container uses /app as working directory.
        <Class TestDockerfileTestConfiguration>
          Test that Dockerfile.test is properly configured.
          <Function test_dockerfile_test_exists>
            Test that docker/Dockerfile.test exists.
          <Function test_dockerfile_test_copies_scripts>
            Test that Dockerfile.test includes COPY directive for scripts/.

            Regression test for: ModuleNotFoundError: No module named 'scripts'
          <Function test_dockerfile_test_copies_deployments>
            Test that Dockerfile.test includes COPY directive for deployments/.

            Regression test for: FileNotFoundError: '/app/deployments/overlays'
          <Function test_dockerfile_test_copies_in_correct_order>
            Test that COPY directives come after pyproject.toml but before pip install.

            Ensures dependencies are available before installing packages.
        <Class TestDockerImageAssets>
          Meta-tests validating Docker image contents.

          These tests run on the host machine to validate the structure and contents
          of Docker images. They are NOT integration tests that run inside Docker.
          <Function test_docker_test_image_has_scripts>
            Integration test: Verify built Docker test image contains scripts/ directory.

            This test requires Docker to be available and will build the test image
            to verify scripts/ directory is included.
          <Function test_docker_test_image_has_deployments>
            Integration test: Verify built Docker test image contains deployments/ directory.

            This test requires Docker to be available and will build the test image
            to verify deployments/ directory is included.
      <Module test_docker_image_contents.py>
        Meta-test: Validate Docker Image Contents

        This test ensures that critical files and directories are copied into the Docker
        image, preventing test failures when containerized tests can't access required files.

        Related: OpenAI Codex Finding 2025-11-15 - ADR documentation test failed because
        adr/ directory was not copied to the Docker test image.
        <Class TestDockerImageContents>
          Meta-test validating Docker image contains required files and directories.

          This test runs on the host machine to validate Docker image contents,
          NOT as an integration test inside Docker.
          <Function test_critical_files_documented>
            Document which files/directories MUST be in the Docker image.

            This serves as a checklist for future Dockerfile updates.
      <Module test_documentation_references.py>
        Test documentation accuracy and script reference validation.

        This module validates that documentation files reference current tooling and scripts,
        preventing stale references to obsolete or renamed tools.

        Related: OpenAI Codex Finding #4 - Stale xdist state-pollution documentation
        <Class TestDocumentationReferences>
          Validate documentation references are accurate and current.
          <Function test_no_references_to_obsolete_scripts>
            Verify documentation doesn't reference obsolete/renamed scripts.

            Obsolete scripts:
            - analyze_state_pollution.py → Replaced by check_test_memory_safety.py
            - state_pollution.py → Replaced by check_test_memory_safety.py

            These tools were part of the evolution from diagnostic (analyze) to
            enforcement (check) tooling for pytest-xdist memory safety.
          <Function test_current_scripts_exist>
            Verify all current scripts referenced in docs actually exist.
          <Function test_xdist_prevention_doc_references_correct_script>
            Verify PYTEST_XDIST_PREVENTION.md references current tooling.

            This specific document is referenced in multiple training materials
            and pre-commit hooks, so it's critical it stays accurate.
          <Function test_state_pollution_report_references_correct_script>
            Verify state pollution report references current tooling.
          <Function test_documentation_cross_references_are_valid>
            Verify documentation cross-references point to existing files.

            Checks markdown links like:
            - [text](path/to/file.md)
            - See: docs/guide.md
            - Reference: ../OTHER.md
      <Module test_documentation_validation.py>
        Documentation Validation Tests

        These tests ensure documentation quality and prevent common documentation errors:
        - MDX parsing errors (unescaped < characters)
        - Broken internal links
        - Invalid frontmatter
        - ADR numbering consistency
        - Template file exclusions

        Following TDD principles to prevent regression of documentation issues.
        <Class TestMDXParsing>
          Test MDX files for parsing errors
          <Function test_no_unescaped_less_than_digits>
            Prevent MDX parsing errors from unescaped < followed by digits.

            Common patterns like <100ms or <2min must be escaped with backticks.
            This prevents MDX from interpreting them as HTML tags.

            Regression test for: mintlify dev parsing errors found on 2025-11-06
          <Function test_no_unescaped_email_addresses>
            Prevent MDX parsing errors from email addresses with < >.

            Email addresses like <email@example.com> must be escaped.
        <Class TestMintlifyNavigation>
          Test Mintlify navigation configuration
          <Function test_docs_json_valid>
            Validate docs.json is valid JSON
          <Function test_all_navigation_files_exist>
            Verify all files referenced in docs.json navigation exist
          <Function test_no_orphaned_documentation_files>
            Warn about documentation files not in navigation (excluding templates)
        <Class TestADRConsistency>
          Test Architecture Decision Records consistency
          <Function test_adr_numbering_sequential>
            Verify ADR numbers are sequential and unique
          <Function test_adr_sync_between_directories>
            Verify ADRs in adr/ have corresponding .mdx files in docs/architecture/
        <Class TestFrontmatter>
          Test MDX frontmatter consistency
          <Function test_mdx_files_have_frontmatter>
            Verify all MDX files (except templates and operational docs) have valid frontmatter
          <Function test_frontmatter_required_fields>
            Verify frontmatter contains required fields
        <Class TestLinkIntegrity>
          Test internal link integrity
          <Function test_no_broken_internal_links_to_docs>
            Detect broken internal links within documentation.

            This is a basic check - full link validation should use mintlify broken-links
      <Module test_e2e_keycloak_setup.py>
        Test E2E Keycloak realm configuration and setup.

        This test validates that the E2E test infrastructure has proper Keycloak
        configuration including client setup and test user provisioning.

        Following TDD RED-GREEN-REFACTOR cycle to fix E2E test failures caused by:
        - Missing Keycloak client 'mcp-server'
        - Missing test user 'alice' with password 'alice123'
        - No automated realm import on Keycloak startup

        Reference: E2E Tests workflow failures with 401 authentication errors
        <Function test_keycloak_realm_import_file_exists>
          Verify that keycloak-test-realm.json exists in tests/e2e/.

          This file should contain the realm configuration with pre-configured
          client and test users for E2E testing.
        <Function test_realm_json_has_mcp_server_client>
          Verify that the realm configuration includes the 'mcp-server' client.

          The E2E tests expect a client named 'mcp-server' with:
          - enabled: true
          - publicClient: true
          - directAccessGrantsEnabled: true (for password grant flow)
        <Function test_realm_json_has_test_users>
          Verify that the realm configuration includes test user 'alice'.

          The E2E tests expect a user 'alice' with:
          - username: alice
          - enabled: true
          - credentials: password = alice123
        <Function test_docker_compose_imports_realm>
          Verify that docker-compose.test.yml is configured to import the realm.

          The keycloak-test service should:
          - Mount the realm JSON file to /opt/keycloak/data/import/realm.json
          - Use command: start-dev --import-realm
      <Module test_e2e_organization.py>
        Meta-tests validating E2E test organization and tooling correctness.

        These tests ensure:
        1. E2E journey tests are in the correct location (tests/e2e/)
        2. E2E completion tracking script references the correct path
        3. E2E test markers are properly configured
        4. E2E infrastructure helpers are properly organized

        Reference: Testing Infrastructure Validation (2025-11-21)
        Finding: Broken E2E tracking tooling - script pointed to wrong path
        <Class TestE2EOrganization>
          Validate that E2E tests are correctly organized and tooling works.
          <Function test_e2e_journey_file_exists_in_correct_location>
            Validate test_full_user_journey.py exists in tests/e2e/.

            FINDING: E2E journey tests were incorrectly located in tests/integration/e2e/
            causing the check_e2e_completion.py script to fail.

            References:
            - tests/e2e/test_full_user_journey.py - Main E2E journey test file
            - scripts/check_e2e_completion.py - E2E completion tracking script
          <Function test_check_e2e_completion_script_references_correct_path>
            Validate scripts/check_e2e_completion.py points to correct E2E test file.

            FINDING: Script was hardcoded to tests/e2e/test_full_user_journey.py but
            file was actually at tests/integration/e2e/test_full_user_journey.py.

            References:
            - scripts/validation/check_e2e_completion.py:17 - E2E_TEST_FILE path constant
          <Function test_e2e_infrastructure_helpers_exist>
            Validate E2E infrastructure helpers remain accessible.

            E2E infrastructure (helpers, real clients, keycloak config) should
            remain in tests/e2e/ as shared utilities.

            References:
            - tests/e2e/helpers.py - E2E test helper functions
            - tests/e2e/real_clients.py - Real client implementations
            - tests/e2e/keycloak-test-realm.json - Keycloak test configuration
          <Function test_no_e2e_journeys_in_integration_directory>
            Validate that E2E journey tests are NOT in tests/integration/e2e/.

            This directory structure was confusing - integration tests should
            not contain E2E subdirectories.

            PREVENTION: Ensure we don't regress back to the old structure.
          <Function test_e2e_completion_script_can_find_test_file>
            Integration test: Validate the check_e2e_completion.py script can actually
            find and read the E2E journey test file.

            This test imports the script and validates it doesn't raise FileNotFoundError.
      <Module test_environment_isolation_enforcement.py>
        Meta-tests enforcing environment isolation in pytest-xdist execution.

        PROBLEM:
        --------
        Tests mutating os.environ directly cause pollution in pytest-xdist workers.
        Environment variables leak between tests running in the same worker, causing
        intermittent failures and flaky behavior.

        SOLUTION:
        ---------
        1. Provide centralized fixtures for common environment mutations
        2. Enforce monkeypatch.setenv() usage (auto-cleanup)
        3. Detect violations via meta tests and pre-commit hooks

        This ensures:
        ✅ No env pollution between tests in same worker
        ✅ Clean state for each test (via monkeypatch auto-cleanup)
        ✅ Consistent behavior in serial and parallel test execution

        Related Issues:
        ---------------
        - OpenAI Codex Finding: 15+ test files mutate os.environ without cleanup
        - tests/api/test_service_principals_endpoints.py: Missing cleanup in fixtures
        - tests/PYTEST_XDIST_PREVENTION.md: Documents monkeypatch pattern

        References:
        -----------
        - OpenAI Codex Finding: test files mutate os.environ directly (RESOLVED via fixtures)
        - tests/conftest.py: Centralized environment isolation fixtures
        <Class TestEnvironmentIsolationEnforcement>
          Enforce environment isolation patterns across test suite.
          <Function test_centralized_fixtures_exist>
            ✅ Validate that centralized environment isolation fixtures exist.

            Instead of each test manually setting/unsetting env vars, we provide
            reusable fixtures for common patterns (especially MCP_SKIP_AUTH).
          <Function test_critical_violations_are_fixed>
            ✅ Validate that critical environment pollution violations are fixed.

            The most critical violations (missing cleanup in fixtures) must be
            fixed to prevent worker pollution.

            Critical files:
            - tests/api/test_service_principals_endpoints.py (missing fixture cleanup)
        <Class TestEnvironmentPollutionDetection>
          Detect environment pollution patterns in test files.
          <Function test_monkeypatch_is_preferred_pattern>
            📚 Document that monkeypatch.setenv() is the preferred pattern.

            This test serves as documentation for the correct approach.
        <Class TestFixtureBasedIsolation>
          Validate fixture-based isolation approach.
          <Function test_disable_auth_skip_fixture_behavior>
            ✅ Test expected behavior of disable_auth_skip fixture.

            This fixture should set MCP_SKIP_AUTH=false for tests requiring real auth.
            After implementing in Phase 3.2, this will validate actual fixture behavior.
          <Function test_isolated_environment_fixture_behavior>
            ✅ Test expected behavior of isolated_environment fixture.

            This fixture provides a clean environment for tests sensitive to pollution.
            Can be used with autouse for entire test classes.
        <Function test_environment_isolation_documentation>
          📚 Document environment isolation strategy.

          This test serves as living documentation for the approach.
      <Module test_fixture_organization.py>
        Test fixture organization and best practices.

        This test module validates that test fixtures follow best practices:
        - No duplicate autouse fixtures across different files
        - All session/module-scoped autouse fixtures should be in conftest.py
        - Fixture names are unique or intentionally scoped
        <Function test_no_duplicate_autouse_fixtures>
          Test that autouse fixtures are not duplicated across test files.

          TDD RED phase: This test should FAIL initially if duplicate fixtures exist.
          After consolidating fixtures into conftest.py, this test should PASS.

          Best practice: Module/session-scoped autouse fixtures should be defined
          in conftest.py to avoid initialization conflicts and improve clarity.
        <Function test_autouse_fixtures_documented>
          Test that all autouse fixtures have proper documentation.

          Autouse fixtures run automatically, so they should be well-documented
          to explain why they run and what they do.
      <Module test_fixture_validation.py>
        Meta-tests to validate pytest fixture definitions and prevent missing decorators

        TDD Regression Test: Validates that all fixture-like functions have proper @pytest.fixture decorators
        to prevent silent failures where fixtures don't actually execute.

        Tests cover:
        1. Generator-returning functions have @pytest.fixture decorator
        2. Functions used as test parameters have @pytest.fixture decorator

        These tests ensure fixtures are properly configured and will execute as expected.

        References:
        - OpenAI Codex finding: test_health_check.py:11 missing @pytest.fixture (now fixed)
        - pytest fixture documentation
        <Class TestFixtureDecorators>
          Meta-tests to validate pytest fixture decorators
          <Function test_no_placeholder_tests_with_only_pass>
            TDD REGRESSION TEST: Ensure test functions don't have only 'pass' statement

            GIVEN: All test files in the test suite
            WHEN: Scanning for test functions
            THEN: No test should have only 'pass' as its body (unless it's xfail/skip)

            This prevents incomplete tests from silently passing.
            Tests should either:
            1. Have actual test logic
            2. Use @pytest.mark.xfail(strict=True) if not implemented yet
            3. Use @pytest.mark.skip with a valid reason
          <Function test_generator_functions_have_fixture_decorator>
            TDD REGRESSION TEST: Ensure Generator-returning functions have @pytest.fixture decorator

            GIVEN: All test files and conftest.py in the test suite
            WHEN: Scanning for functions that return Generator types
            THEN: They should have @pytest.fixture decorator if they look like fixtures

            This prevents bugs where fixture-like functions don't execute because
            they're missing the decorator, causing tests to silently fail or use
            wrong fixtures.
          <Function test_fixture_parameters_have_valid_fixtures>
            TDD REGRESSION TEST: Ensure test functions only reference valid fixtures

            GIVEN: All test files
            WHEN: Scanning for test function parameters
            THEN: Each parameter should either:
                - Have a corresponding @pytest.fixture in scope
                - Be a built-in pytest fixture (e.g., request, monkeypatch)
                - Be from pytest-asyncio (e.g., event_loop)

            This catches typos in fixture names and ensures all fixtures are properly defined.
          <Function test_fixture_scope_dependencies_are_compatible>
            TDD REGRESSION TEST: Ensure fixture scopes are compatible with their dependencies

            GIVEN: All fixtures in conftest.py and test files
            WHEN: Analyzing fixture dependencies
            THEN: Fixtures with wider scopes should not depend on fixtures with narrower scopes

            Scope hierarchy (widest to narrowest):
            - session: Fixture runs once per test session
            - package: Fixture runs once per package
            - module: Fixture runs once per module
            - class: Fixture runs once per class
            - function: Fixture runs once per test function (default)

            RULE: A fixture can only depend on fixtures with equal or wider scope.
            Example violations:
            - session-scoped fixture depending on function-scoped fixture ❌
            - module-scoped fixture depending on function-scoped fixture ❌
            - function-scoped fixture depending on session-scoped fixture ✅ (OK)

            This prevents pytest ScopeMismatch errors and ensures fixtures work correctly.
      <Module test_github_actions_validation.py>
        Test suite for validating GitHub Actions workflow configurations.

        This module ensures that all GitHub Actions workflows use valid action versions
        and have appropriate permissions configured. These tests prevent CI/CD failures
        caused by referencing non-existent action tags or missing required permissions.

        Following TDD principles - these tests define the expected valid state before fixes.
        <Class TestGitHubActionsVersions>
          Validate that all GitHub Actions use published, valid version tags.
          <Function test_astral_sh_setup_uv_version_is_valid>
            Ensure astral-sh/setup-uv uses valid version tag.

            INVALID: v7.1.1 (does not exist)
            VALID: v7.1.0 or v7 (latest published)

            This test will FAIL until all occurrences of v7.1.1 are fixed.
          <Function test_actions_cache_version_is_valid>
            Ensure actions/cache uses valid version tag.

            POTENTIALLY INVALID: v4.3.0 (not confirmed to exist)
            VALID: v4.2.0 or v4 (confirmed published)

            This test will FAIL until all occurrences of v4.3.0 are fixed.
          <Function test_no_other_suspicious_action_versions>
            Validate that commonly used actions have reasonable version patterns.

            This test checks for:
            - Very high major versions (e.g., @v10+ for actions that don't have them)
            - Suspicious patch versions (e.g., @v4.99.99)
            - Missing @ symbol (bare action names)
        <Class TestGitHubActionsPermissions>
          Validate that workflows have appropriate permissions configured.
          <Function test_scheduled_workflows_creating_issues_have_issues_write_permission>
            Ensure scheduled workflows that create GitHub issues have issues: write permission.

            Workflows that use actions/github-script to call github.rest.issues.create
            must have 'issues: write' in their permissions block.

            This test will FAIL for workflows missing this permission.
          <Function test_all_workflows_creating_issues_have_permission>
            Ensure ALL workflows that create GitHub issues have issues: write permission.

            This test checks ALL workflows (not just scheduled), as any workflow
            creating issues needs the permission regardless of trigger type.

            This prevents permission errors at runtime when creating issues.
          <Function test_workflows_have_minimal_permissions>
            Ensure workflows don't use overly broad permissions.

            Workflows should specify exact permissions needed rather than using
            'write-all' or 'permissions: {}' (which grants all permissions).

            This is a warning test - it will warn but not fail.
        <Class TestGitHubActionsWorkflowDependencies>
          Validate that workflow jobs have correct dependencies installed.
          <Function test_validate_workflows_job_has_required_dependencies>
            Ensure validate-workflows job uses setup-python-deps composite action
            instead of manually installing only pytest and pyyaml.

            INVALID: pip install pytest pyyaml (missing langchain_core and other deps)
            VALID: Uses ./.github/actions/setup-python-deps with extras: 'dev'

            This test validates Finding #1 from Codex report is fixed.
        <Class TestGitHubActionsStructure>
          Validate overall workflow structure and best practices.
          <Function test_all_workflows_are_valid_yaml>
            Ensure all workflow files are valid YAML.
          <Function test_composite_actions_use_valid_versions>
            Ensure composite actions in .github/actions/ use valid action versions.

            Composite actions inherit issues from the same version problems,
            but are more critical because they're reused across multiple workflows.
      <Module test_hook_fixture_resilience.py>
        Tests for hook fixture resilience on fresh clones.

        PROBLEM:
        --------
        The pre_push_hook_path fixture in test_local_ci_parity.py fails tests
        on fresh clones because .git/hooks/pre-push doesn't exist yet.

        This causes confusing test failures for new contributors who haven't
        run 'make git-hooks' yet.

        SOLUTION:
        ---------
        The fixture should use pytest.skip() with helpful guidance when the
        hook is missing, instead of failing tests.

        This provides a better developer experience:
        - Fresh clone: Tests skip with clear installation instructions
        - After 'make git-hooks': Tests run and validate hook configuration
        - CI: Hooks are installed explicitly, tests always run

        Tests:
        ------
        - test_fresh_clone_skips_hook_tests: Validates graceful skip behavior
        - test_hook_fixture_provides_installation_guidance: Validates skip message
        - test_installed_hook_allows_tests_to_run: Validates normal operation

        References:
        -----------
        - OpenAI Codex Finding: test_local_ci_parity.py:24 assumes hook exists
        - CONTRIBUTING.md: Documents 'make git-hooks' requirement
        <Class TestHookFixtureResilience>
          Validate that hook fixtures handle missing hooks gracefully.
          <Function test_missing_hook_should_skip_not_fail>
            ✅ Test that missing pre-push hook causes pytest.skip(), not assertion failure.

            When .git/hooks/pre-push doesn't exist (fresh clone), the fixture should
            skip tests with a helpful message, not fail them.

            This is the RED test - it will PASS after we fix the fixture.
          <Function test_skip_message_includes_documentation_links>
            ✅ Test that skip message links to documentation.

            Developers should know WHERE to find more information about
            why hooks are important and how to install them.
          <Function test_skip_message_explains_why_hooks_matter>
            ✅ Test that skip message explains why hooks are important.

            Developers should understand this isn't optional - hooks enforce
            quality standards and prevent CI failures.
          <Function test_fixture_behavior_on_missing_hook>
            ✅ Test the actual fixture behavior when hook is missing.

            This test simulates a fresh clone scenario where .git/hooks/pre-push
            doesn't exist and validates that the fixture skips appropriately.
        <Class TestPrePushHookFixture>
          Validate the pre_push_hook_path fixture from test_local_ci_parity.py.
          <Function test_current_fixture_requires_update>
            🔴 RED: Demonstrate that current fixture needs updating.

            The current fixture (test_local_ci_parity.py:38-41) returns a Path
            without checking if it exists. Tests then fail with assertion errors
            instead of skipping gracefully.

            This test documents the CURRENT (broken) behavior and will PASS now.
            After fixing the fixture, this test serves as documentation.
        <Class TestFixtureBehaviorAfterFix>
          Validate expected fixture behavior after implementing the fix.

          These tests define what the FIXED fixture should do.
          <Function test_fixture_skips_when_hook_missing>
            🟢 GREEN: After fix, fixture should skip when hook is missing.

            This test will FAIL initially (fixture doesn't skip yet).
            After implementing the fix, it will PASS.
          <Function test_fixture_returns_path_when_hook_exists>
            🟢 GREEN: After fix, fixture should return path when hook exists.

            This test validates that the fixture doesn't break normal operation
            when the hook IS installed.
        <Function test_documentation_about_hook_installation>
          📚 Document why hooks are important and how to install them.

          This test serves as living documentation about the hook system.
      <Module test_hook_sync_validation.py>
        Meta-validation: Ensure Git hooks match CI/CD configuration exactly.

        This test suite ensures that:
        1. Pre-push hook uses `-n auto` for all pytest calls (matches CI)
        2. Pre-push hook has MyPy configured as blocking (matches CI)
        3. Post-commit hook uses project-managed Python runtime (uv run)
        4. Hook configurations stay in sync with CI/CD workflows

        TDD Principle: These tests prevent configuration drift between local hooks and CI.
        They MUST pass to ensure consistent validation behavior everywhere.

        Related:
        - tests/meta/test_local_ci_parity.py - Overall CI/local parity
        - tests/meta/test_pytest_xdist_enforcement.py - xdist enforcement
        - .git/hooks/pre-push - Pre-push hook implementation
        - .github/workflows/ci.yaml - CI workflow configuration
        <Class TestPrePushHookSync>
          Validate pre-push hook configuration matches CI expectations.
          <Function test_all_pytest_calls_use_n_auto>
            Test that pre-push test orchestrator configures pytest with -n auto.

            Rationale:
            - CI uses `-n auto` to adapt to available GitHub runner cores
            - Local pre-push should match for consistent behavior
            - Using fixed worker counts (e.g., -n 4) causes:
              * Different parallel execution patterns locally vs CI
              * Missed pytest-xdist isolation bugs
              * Inconsistent test timing and resource usage

            Expected: scripts/run_pre_push_tests.py must configure -n auto
          <Function test_mypy_is_blocking>
            Test that MyPy type checking is configured as blocking (critical).

            NOTE: MyPy is NOT in the pre-commit test orchestrator (scripts/run_pre_push_tests.py).
            It's only in Makefile validate-pre-push target, which provides comprehensive
            CI-equivalent validation beyond just running tests.

            Rationale:
            - CI blocks on MyPy errors (no continue-on-error flag)
            - Local validation must match to prevent CI surprises
            - Warning-only MyPy locally means:
              * Developers push code with type errors
              * CI fails unexpectedly
              * Requires force-push fixes
              * Wastes time and breaks flow

            Expected: Makefile validate-pre-push must have blocking MyPy
          <Function test_no_hardcoded_worker_counts>
            Test that pre-push hook does not use hardcoded worker counts like -n 4.

            This is a negative test to catch regressions where someone might
            add a new pytest call with a fixed worker count.
        <Class TestPostCommitHookSync>
          Validate post-commit hook uses project-managed Python runtime.
          <Function test_uses_uv_run_python>
            Test that post-commit hook uses 'uv run python' instead of bare 'python'.

            Rationale:
            - Project uses uv for dependency management
            - Bare 'python' uses global interpreter (may have wrong deps)
            - 'uv run python' ensures project-managed runtime
            - Consistency across all automation

            Expected: All python invocations use 'uv run python'
        <Class TestHookTemplateSync>
          Validate hook templates stay in sync or are properly removed.
          <Function test_template_matches_or_documented>
            Test that template hook is either in sync with actual hook or documented as obsolete.

            Rationale:
            - scripts/git-hooks/pre-push is a tracked template
            - If it differs from .git/hooks/pre-push, it's confusing
            - Either keep them in sync or remove the template
            - Add documentation explaining the state

            This test will pass if:
            1. Template doesn't exist (removed as obsolete), OR
            2. Template contains a deprecation notice, OR
            3. Template matches actual hook configuration
      <Module test_id_pollution_prevention.py>
        ID Pollution Prevention - Pre-commit Hook Validation

        This meta-test ensures the pre-commit hook prevents hardcoded user IDs in tests.

        CRITICAL: Test files must use worker-safe ID helpers (get_user_id, get_api_key_id, etc.)
        instead of hardcoded IDs like "user:alice" or "apikey_abc123".

        Why this matters:
        - Hardcoded IDs cause state pollution in parallel test execution (pytest-xdist)
        - State pollution leads to intermittent test failures (flaky tests)
        - Flaky tests waste CI/CD time and reduce confidence

        Prevention mechanism:
        - Pre-commit hook validates test files BEFORE commit
        - Hook detects hardcoded IDs and blocks commit with helpful error message
        - Developers must use worker-safe helpers (enforced by automation)

        Worker-safe helper usage examples:
        ```python
        from tests.conftest import get_user_id, get_api_key_id

        def test_something():
            user_id = get_user_id()  # ✅ Worker-safe
            apikey_id = get_api_key_id()  # ✅ Worker-safe

            # NOT: user_id = "user:alice"  # ❌ Hardcoded - will fail pre-commit
        ```

        Related:
        - tests/conftest.py: Worker-safe ID helper implementations
        - scripts/validate_test_ids.py: Validation script (being validated here)
        - .pre-commit-config.yaml: Hook configuration
        <Class TestIDPollutionPreventionHook>
          Test pre-commit hook prevents hardcoded IDs in test files.
          <Function test_validation_script_exists>
            Test that validation script exists.
          <Function test_validation_script_is_executable>
            Test that validation script has execute permissions.
          <Function test_validation_script_detects_hardcoded_user_ids>
            Test script detects hardcoded user IDs like user:alice.
          <Function test_validation_script_detects_hardcoded_apikey_ids>
            Test script detects hardcoded API key IDs.
          <Function test_validation_script_allows_worker_safe_helpers>
            Test script allows proper usage of worker-safe ID helpers.
          <Function test_validation_script_allows_legitimate_test_assertions>
            Test script allows legitimate uses like checking OpenFGA format.
          <Function test_validation_script_allows_unit_tests_with_inmemory>
            Test script allows unit tests with InMemory backends (no pollution risk).
          <Function test_validation_script_allows_mock_configurations>
            Test script allows Mock/AsyncMock configurations in unit tests.
          <Function test_validation_script_still_detects_integration_test_violations>
            Test script still detects hardcoded IDs in integration tests (real pollution risk).
      <Module test_infrastructure_singleton.py>
        Meta-tests validating single shared test infrastructure pattern.

        This test suite enforces the architectural decision to use a single shared
        test infrastructure instance across all pytest-xdist workers, with logical
        isolation via PostgreSQL schemas, Redis DB indices, and OpenFGA stores.

        Architecture Decision:
        - ONE docker-compose instance runs on fixed base ports (9432, 9379, etc.)
        - ALL xdist workers (gw0, gw1, gw2, ...) connect to the SAME ports
        - Isolation achieved via:
          - PostgreSQL: Separate schemas per worker (test_worker_gw0, test_worker_gw1)
          - Redis: Separate DB indices per worker (DB 1, 2, 3, ...)
          - OpenFGA: Separate stores per worker (test_store_gw0, test_store_gw1)
          - Qdrant: Separate collections per worker
          - Keycloak: Separate realms per worker

        This is faster and simpler than per-worker infrastructure with dynamic port allocation.

        Related:
        - tests/conftest.py:582-619 (test_infrastructure_ports fixture)
        - docker-compose.test.yml (fixed port mappings)
        - tests/regression/test_pytest_xdist_port_conflicts.py (validates port behavior)
        <Class TestInfrastructureSingleton>
          Validate single shared infrastructure architecture.
          <Function test_infrastructure_ports_are_fixed_regardless_of_worker>
            Test that test_infrastructure_ports returns FIXED base ports.

            All xdist workers should connect to the same infrastructure ports,
            regardless of worker ID. This enforces the single-instance architecture.
          <Function test_no_worker_offset_calculation>
            Test that there is NO port offset calculation based on worker ID.

            The old multi-instance approach calculated offsets (gw0=+0, gw1=+100, etc.).
            The new single-instance approach uses fixed ports for all workers.
          <Function test_docker_compose_ports_match_fixture_ports>
            Test that docker-compose.test.yml ports match test_infrastructure_ports.

            This ensures consistency between the infrastructure definition and
            the ports tests expect to connect to.
          <Function test_logical_isolation_mechanisms_documented>
            Test that the codebase documents how logical isolation works.

            Since all workers share the same infrastructure, we need clear
            documentation of how they achieve test isolation.
        <Class TestWorkerIsolationMechanisms>
          Validate that logical isolation mechanisms work correctly.
          <Function test_postgres_schema_isolation_exists>
            Test that PostgreSQL uses separate schemas per worker.

            This is HOW workers sharing the same PostgreSQL port (9432)
            achieve data isolation.
          <Function test_worker_utils_available>
            Test that worker utility functions are available.

            These utilities help tests achieve logical isolation by providing
            worker-specific identifiers for schemas, DB indices, store names, etc.
        <Class TestRegressionTestCorrectness>
          Validate that port conflict regression tests expect correct behavior.
          <Function test_regression_tests_expect_fixed_ports>
            Test that regression tests validate FIXED port behavior.

            After fixing the architecture, the regression tests should assert
            that ports are ALWAYS the base ports (9432, etc.), not worker-offset.

            This test validates that test_pytest_xdist_port_conflicts.py has
            been updated to expect the correct (fixed port) behavior.
      <Module test_integration_test_organization.py>
        Meta-tests for integration test organization and marker consistency.

        These tests validate that integration tests are properly organized and that
        marker-based selection matches directory-based selection for local/CI parity.

        PURPOSE:
        --------
        Prevent integration test fragmentation where tests marked with
        @pytest.mark.integration live outside tests/integration/, causing pre-commit
        hooks to miss them while CI catches them.

        VALIDATION:
        -----------
        1. All @pytest.mark.integration tests are in tests/integration/
        2. All files in tests/integration/ have integration marker
        3. Pre-commit and CI use same selection strategy
        4. No integration tests in unit/, patterns/, or root tests/

        References:
        - OpenAI Codex Finding #3: Integration test fragmentation
        - Pre-commit hook: run-integration-tests (directory-based)
        - CI workflow: integration-tests.yaml (marker-based)
        <Class TestIntegrationTestOrganization>
          Validate that integration tests are properly organized.

          Ensures all integration-marked tests are in tests/integration/
          <Function test_all_integration_markers_in_integration_directory>
            🟢 GREEN: Verify all @pytest.mark.integration tests are in tests/integration/.

            Integration tests should be consolidated in one directory for clarity
            and to ensure pre-commit hooks catch them all.

            EXCEPTION: tests/meta/ tests may use integration marker for testing
            integration test infrastructure itself.
          <Function test_all_integration_directory_files_have_integration_marker>
            🟢 GREEN: Verify all files in tests/integration/ have @pytest.mark.integration.

            Reverse check: ensure directory structure matches marker usage.
          <Function test_no_integration_tests_in_unit_directory>
            🟢 GREEN: Verify no @pytest.mark.integration tests in tests/unit/.

            Unit tests should not have integration marker.
          <Function test_no_integration_tests_in_root_tests_directory>
            🟢 GREEN: Verify no @pytest.mark.integration tests in root tests/ directory.

            Integration tests should be in tests/integration/, not scattered in root.
          <Function test_integration_directory_exists>
            🟢 GREEN: Verify tests/integration/ directory exists.

            The integration tests directory must exist.
        <Class TestIntegrationTestSelection>
          Validate that pre-commit and CI use consistent integration test selection.
          <Function test_precommit_uses_marker_based_selection>
            🟢 GREEN: Verify pre-commit hook uses marker-based selection.

            Pre-commit should use `pytest -m integration` to match CI, NOT
            directory-based selection which can miss tests.

            UPDATE (After Consolidation):
            ------------------------------
            After consolidating all integration tests into tests/integration/,
            directory-based selection is acceptable since both approaches yield
            the same results. However, marker-based is preferred for robustness.
          <Function test_ci_uses_marker_based_selection>
            🟢 GREEN: Verify CI workflow uses marker-based selection.

            CI should use `pytest -m integration` for maximum coverage.
        <Class TestIntegrationTestDocumentation>
          Validate integration test organization is documented.
          <Function test_integration_directory_has_readme>
            🟢 GREEN: Verify tests/integration/ has README explaining organization.

            Documentation helps developers understand where to put tests.
          <Function test_testing_md_documents_integration_test_organization>
            🟢 GREEN: Verify TESTING.md documents where integration tests live.

            Developers should know where to put integration tests.
        <Class TestIntegrationTestEnforcement>
          Document enforcement strategy for integration test organization.
          <Function test_enforcement_strategy_documentation>
            📚 Document the enforcement strategy for integration test organization.

            Multiple layers ensure integration tests are properly organized.
          <Function test_consolidation_benefits_documented>
            📚 Document the benefits of integration test consolidation.

            Explain why consolidating integration tests improves workflow.
      <Module test_kubectl_safety.py>
        Meta-tests for Kubectl Safety

        These tests validate that kubectl operations use dry-run mode to prevent
        accidental modifications to real clusters.

        Purpose: Prevent regression of Codex Finding #4 (DNS failover tests against real staging cluster)
        <Class TestKubectlSafetyEnforcement>
          Meta-tests that validate kubectl operations use --dry-run.

          RED: These tests will fail if kubectl apply/create/delete used without safety guards.
          GREEN: All kubectl operations are safe (dry-run or explicitly guarded).
          <Function test_kubectl_operations_use_dry_run_or_guards>
            Validate kubectl apply/create/delete operations use --dry-run or safety guards.

            This prevents regressions of Codex Finding #4:
            "Deployment DNS failover tests run real kubectl apply against staging-mcp-server-langgraph,
            risking live cluster drift from a local test run."

            RED: Fails if kubectl operations lack safety measures.
            GREEN: All kubectl operations are safe.
          <Function test_kubectl_tests_have_requires_kubectl_marker>
            Validate tests using kubectl have @pytest.mark.requires_kubectl marker.

            This ensures kubectl tests can be skipped when kubectl is not available.

            RED: Fails if kubectl tests lack the marker.
            GREEN: All kubectl tests properly marked.
          <Function test_kubectl_namespace_not_production>
            Validate kubectl operations don't target production namespaces.

            RED: Fails if production namespaces detected.
            GREEN: All operations target safe namespaces.
        <Class TestKubectlTestNaming>
          Validate kubectl test naming conventions
          <Function test_kubectl_tests_describe_operation>
            Kubectl tests should describe what they're validating.

            Good: test_deployment_manifest_valid_dry_run
            Bad: test_kubectl, test_apply
      <Module test_local_ci_parity.py>
        Meta-validation: Ensure local validation matches CI validation exactly.

        This test suite ensures that:
        1. Pre-push hooks exist and have correct permissions
        2. Pre-push hooks contain all required validation steps
        3. Makefile targets match pre-push hook validation
        4. Local validation steps match CI validation steps
        5. No validation gaps exist between local and CI environments

        TDD Principle: These tests MUST pass to ensure developers never experience CI surprises.
        <Class TestPrePushHookConfiguration>
          Validate pre-push hook is configured correctly.
          <Function test_pre_push_hook_exists>
            Test that pre-push hook file exists.
          <Function test_pre_push_hook_is_executable>
            Test that pre-push hook has execute permissions.
          <Function test_pre_push_hook_is_bash_script>
            Test that pre-push hook is a bash script.
          <Function test_pre_push_hook_validates_lockfile>
            Test that pre-push hook validates lockfile.
          <Function test_pre_push_hook_validates_workflows>
            Test that pre-push hook validates GitHub workflows.
          <Function test_pre_push_hook_runs_mypy>
            Test that pre-push hook runs MyPy type checking (OPTIONAL - project policy).
          <Function test_pre_push_hook_runs_precommit_all_files>
            Test that pre-push hook runs pre-commit on ALL files.
          <Function test_pre_push_hook_runs_property_tests_with_ci_profile>
            Test that pre-push hook runs property tests with CI profile.
          <Function test_pre_push_hook_has_clear_phases>
            Test that pre-push hook has clearly defined validation phases (legacy only).
          <Function test_pre_push_hook_provides_helpful_error_messages>
            Test that pre-push hook provides helpful troubleshooting info (legacy only).
        <Class TestMakefileValidationTarget>
          Validate Makefile validate-pre-push target.
          <Function test_validate_pre_push_target_exists>
            Test that validate-pre-push target exists in Makefile.
          <Function test_validate_pre_push_in_phony_targets>
            Test that validate-pre-push is declared as .PHONY.
          <Function test_validate_pre_push_runs_lockfile_check>
            Test that validate-pre-push target validates lockfile.
          <Function test_validate_pre_push_runs_workflow_tests>
            Test that validate-pre-push target runs workflow validation tests.
          <Function test_validate_pre_push_runs_mypy>
            Test that validate-pre-push target runs MyPy.
          <Function test_validate_pre_push_runs_precommit_all_files>
            Test that validate-pre-push runs pre-commit on all files.
          <Function test_validate_pre_push_runs_property_tests_with_ci_profile>
            Test that validate-pre-push runs property tests with CI profile.
          <Function test_validate_pre_push_in_help_output>
            Test that validate-pre-push is documented in help target.
        <Class TestLocalCIParity>
          Validate that local validation matches CI validation.
          <Function test_lockfile_validation_matches_ci>
            Test that lockfile validation command matches CI.
          <Function test_precommit_scope_matches_ci>
            Test that pre-commit scope matches CI (all files).
          <Function test_hypothesis_profile_matches_ci>
            Test that Hypothesis profile matches CI.
          <Function test_workflow_validation_matches_ci>
            Test that workflow validation tests match what CI runs.
        <Class TestCIGapPrevention>
          Tests to prevent CI validation gaps from being introduced.
          <Function test_ci_workflow_has_validation_job>
            Test that CI workflow has comprehensive validation.
          <Function test_contributing_docs_mention_validate_pre_push>
            Test that CONTRIBUTING.md documents validate-pre-push.
          <Function test_readme_or_quickstart_mentions_validation>
            Test that quick start documentation mentions validation.
        <Class TestPytestXdistParity>
          Validate that local tests use pytest-xdist (-n auto) like CI does.

          CRITICAL: These tests enforce Codex finding #7 - ensure local pre-push runs
          tests in parallel with -n auto to catch pytest-xdist isolation bugs before CI.
          <Function test_unit_tests_use_pytest_xdist_n_auto>
            Test that unit tests run with -n auto for parallel execution.

            CRITICAL: Without -n auto, pytest-xdist isolation bugs are only caught in CI.
            This causes "works locally, fails in CI" issues.

            Since migrating to pre-commit framework, test commands are in scripts/run_pre_push_tests.py
            instead of the bash hook file.
          <Function test_smoke_tests_use_pytest_xdist_n_auto>
            Test that smoke tests run with -n auto for parallel execution.

            Smoke tests are unit tests in tests/smoke/, covered by consolidated script's
            marker expression: (unit or api or property) and not llm
          <Function test_integration_tests_use_pytest_xdist_n_auto>
            Test that integration tests use -n auto when CI_PARITY=1.

            Integration tests are optional in pre-push (require Docker), but when enabled
            via CI_PARITY=1 they should use -n auto for parallel execution.
          <Function test_property_tests_use_pytest_xdist_n_auto>
            Test that property tests run with -n auto for parallel execution.

            Property tests are included in consolidated marker expression:
            (unit or api or property) and not llm
          <Function test_ci_uses_pytest_xdist_n_auto>
            Verify that CI uses -n auto (this is the baseline we're matching).
        <Class TestOtelSdkDisabledParity>
          Validate that local tests set OTEL_SDK_DISABLED=true like CI does.

          CRITICAL: These tests enforce Codex finding #2B - ensure local pre-push sets
          OTEL_SDK_DISABLED=true to match CI environment exactly.
          <Function test_unit_tests_set_otel_sdk_disabled>
            Test that unit tests set OTEL_SDK_DISABLED=true to match CI.

            Since migrating to pre-commit framework, the consolidated script sets
            OTEL_SDK_DISABLED in the environment for all tests.
          <Function test_smoke_tests_set_otel_sdk_disabled>
            Test that smoke tests set OTEL_SDK_DISABLED=true to match CI.

            Smoke tests are unit tests covered by the consolidated script, which sets
            OTEL_SDK_DISABLED for all tests in the environment.
          <Function test_integration_tests_set_otel_sdk_disabled>
            Test that integration tests set OTEL_SDK_DISABLED=true to match CI.

            Integration tests (when enabled via CI_PARITY=1) are covered by the same
            environment setup that sets OTEL_SDK_DISABLED for all tests.
          <Function test_property_tests_already_set_otel_sdk_disabled>
            Verify that property tests already set OTEL_SDK_DISABLED=true (should pass).

            Property tests are included in the consolidated marker expression and covered
            by the same environment setup.
          <Function test_ci_sets_otel_sdk_disabled>
            Verify that CI sets OTEL_SDK_DISABLED=true (this is the baseline).
        <Class TestApiMcpTestSuiteParity>
          Validate that local pre-push runs API/MCP test suites like CI does.

          CRITICAL: These tests enforce Codex finding #2D - ensure API and MCP tests
          run locally before push to prevent CI-only failures.
          <Function test_api_endpoint_tests_run_locally>
            Test that API endpoint tests run in pre-push hook like in CI.

            Since migrating to pre-commit framework, API tests are covered by the consolidated
            marker expression: (unit or api or property) and not llm
          <Function test_mcp_server_tests_run_locally>
            Test that MCP server tests run in pre-push hook like in CI.

            MCP server tests are unit tests in tests/unit/test_mcp_stdio_server.py, covered by
            the consolidated marker expression: (unit or api or property) and not llm
          <Function test_ci_runs_api_tests>
            Verify that CI runs API tests (this is the baseline).
          <Function test_ci_runs_mcp_tests>
            Verify that CI runs MCP tests (this is the baseline).
        <Class TestMakefilePrePushParity>
          Validate that Makefile validate-pre-push target matches pre-push hook exactly.

          CRITICAL: These tests enforce Codex finding #3 - ensure developers running
          'make validate-pre-push' get the same validation as the git pre-push hook.
          <Function test_makefile_includes_unit_tests>
            Test that Makefile validate-pre-push runs unit tests.
          <Function test_makefile_includes_smoke_tests>
            Test that Makefile validate-pre-push runs smoke tests (covered by unit).
          <Function test_makefile_includes_integration_tests>
            Test that Makefile validate-pre-push runs integration tests.
          <Function test_makefile_includes_api_mcp_tests>
            Test that Makefile validate-pre-push runs API/MCP tests.
          <Function test_makefile_uses_n_auto>
            Test that Makefile validate-pre-push uses -n auto like pre-push hook.
          <Function test_makefile_sets_otel_sdk_disabled>
            Test that Makefile validate-pre-push sets OTEL_SDK_DISABLED=true.
        <Class TestActionlintHookStrictness>
          Validate that actionlint hook fails on errors (no || true bypass).

          CRITICAL: Codex finding #1 - actionlint hook currently has || true which
          causes it to NEVER fail even when workflows are invalid. This creates a
          local/CI divergence where CI fails but local validation passes.
          <Function test_actionlint_hook_has_no_bypass>
            Test that actionlint hook does NOT have || true bypass.

            CRITICAL: Without this test, developers can push invalid workflows that
            only fail in CI, wasting time and breaking builds.
          <Function test_actionlint_hook_configured_for_pre_push>
            Test that actionlint hook runs during pre-push stage.
        <Class TestMyPyBlockingParity>
          Validate that MyPy blocking behavior matches between local and CI.

          CRITICAL: Codex finding #2 - MyPy is currently non-blocking in pre-push hook
          but blocking in CI. This creates local/CI divergence where type errors pass
          locally but fail in CI.
          <Function test_mypy_is_blocking_locally>
            Test that MyPy is configured in pre-commit framework.

            NOTE: Project policy is MyPy non-blocking (manual stage) in pre-push due to
            110+ pre-existing type errors. CI runs MyPy as blocking in separate job.

            This test validates that MyPy is configured, even if set to manual stage.
          <Function test_mypy_comment_reflects_blocking_behavior>
            Test that MyPy configuration includes appropriate documentation.

            Validates that MyPy hook has description explaining non-blocking policy.
          <Function test_ci_mypy_is_blocking>
            Test that CI MyPy step is blocking (no continue-on-error).
        <Class TestIsolationValidationStrictness>
          Validate that test isolation validation script promotes warnings to errors.

          CRITICAL: Codex finding from pytest-xdist section - validate_test_isolation.py
          currently returns 0 (success) when tests are missing xdist_group or gc.collect,
          allowing regressions to slip through.
          <Function test_missing_xdist_group_is_error_not_warning>
            Test that missing xdist_group marker is treated as ERROR not WARNING.

            CRITICAL: User chose "Promote warnings to errors (strict enforcement)".
            Missing xdist_group markers cause memory explosion in pytest-xdist.
          <Function test_missing_gc_collect_is_error_not_warning>
            Test that missing gc.collect() is treated as ERROR not WARNING.
        <Class TestMakefileDependencyExtras>
          Validate that Makefile install-dev includes all required dependency extras.

          CRITICAL: Codex finding #4 - install-dev runs 'uv sync' without --extra flags,
          while CI uses --extra dev --extra builder. This causes missing import errors
          when running pre-push validation locally.
          <Function test_install_dev_includes_dev_extra>
            Test that install-dev target includes --extra dev.

            CRITICAL: User chose "Add --extra dev --extra builder to install-dev".
            Without dev extra, pytest and testing tools are missing.
          <Function test_install_dev_includes_builder_extra>
            Test that install-dev target includes --extra builder.

            Required because unit tests import builder modules (per CI comments).
          <Function test_ci_uses_dev_and_builder_extras>
            Verify that CI uses both dev and builder extras (baseline check).
        <Class TestPrePushDependencyValidation>
          Validate that pre-push hook includes dependency validation (uv pip check).

          CRITICAL: User chose "Yes, add uv pip check to pre-push". CI runs this check
          (ci.yaml:220-235) but pre-push hook doesn't, allowing dependency conflicts
          to slip through to CI.
          <Function test_pre_push_includes_uv_pip_check>
            Test that pre-push hook includes 'uv pip check' (adapted for pre-commit framework).

            CRITICAL: User chose to add this check. Without it, dependency conflicts
            pass locally but fail in CI.

            Since migrating to pre-commit framework, dependency validation is handled by
            .pre-commit-config.yaml hooks rather than inline bash commands.
          <Function test_ci_includes_dependency_validation>
            Verify that CI includes dependency validation (baseline check).
        <Class TestPreCommitHookStageFlag>
          Validate that Makefile validate-pre-push uses --hook-stage push.

          CRITICAL: Codex finding #3 - Makefile validate-pre-push runs
          'pre-commit run --all-files' without --hook-stage push, so none of the
          push-only hooks execute when developers follow documented command.
          <Function test_validate_pre_push_uses_hook_stage_push>
            Test that validate-pre-push includes --hook-stage push/pre-push flag.

            CRITICAL: Without --hook-stage push (or pre-push), the 45 push-stage hooks
            configured in .pre-commit-config.yaml won't execute, creating false confidence.

            NOTE: Both 'push' and 'pre-push' are valid hook stage names (they're equivalent).
        <Class TestContractTestMarkerParity>
          Validate that contract test markers are consistent between local and CI.

          CRITICAL: Codex finding #1 - Contract tests have both @pytest.mark.unit and
          @pytest.mark.contract markers. Local pre-push uses '-m unit and not contract'
          which excludes them, but CI uses '-m unit and not llm' which includes them.
          This creates a CI surprise where tests pass locally but fail in CI.
          <Function test_pre_push_uses_same_marker_as_ci>
            Test that pre-push hook uses same pytest marker expression as CI.

            CRITICAL: Contract tests should run consistently in both local and CI.
            User chose: "Include contract tests in both local pre-push AND CI".
          <Function test_makefile_uses_same_marker_as_ci>
            Test that Makefile validate-pre-push uses same marker as CI.
          <Function test_all_three_sources_use_identical_marker>
            Test that pre-push hook, Makefile, and CI use IDENTICAL base markers.

            This is the ultimate parity test - all three must use "unit and not llm" as the base.
            Additional exclusions like "and not property" are acceptable.
        <Class TestCIPushStageValidatorsJob>
          Validate that CI has a dedicated job for push-stage validators.

          CRITICAL: Codex finding #4 - CI's "Pre-commit Hooks" job runs
          'pre-commit run --all-files' without --hook-stage push, so 50+ push-stage
          validators (actionlint, memory safety, test isolation, etc.) never run in CI.

          Local pre-push runs these with --hook-stage push, creating a validation gap.
          <Function test_ci_has_push_stage_validators_job>
            Test that CI workflow has a job that runs push-stage validators.

            User chose: "Add new dedicated 'Push-Stage Validators' job"
          <Function test_push_stage_job_runs_correct_command>
            Test that push-stage job runs the correct pre-commit command.
        <Class TestPostCommitHookTemplate>
          Validate that post-commit hook template uses 'uv run python'.

          CRITICAL: Codex finding #3 - The template in scripts/workflow/update-context-files.py
          generates hooks with bare 'python' command, but the project requires 'uv run python'
          to use the project-managed environment.
          <Function test_hook_template_uses_uv_run_python>
            Test that the hook template uses 'uv run python' not bare 'python'.

            CRITICAL: Project requires uv-managed Python environment.
          <Function test_hook_template_has_explanatory_comment>
            Test that hook template includes comment explaining uv run usage.
        <Class TestHypothesisProfileParity>
          Validate that pre-push hook sets HYPOTHESIS_PROFILE=ci for unit tests.

          User chose: "Yes - Add HYPOTHESIS_PROFILE=ci to pre-push and exclude property tests"

          This prevents property tests from running twice:
          1. Once in unit tests phase (with dev profile)
          2. Again in property tests phase (with ci profile)
          <Function test_unit_tests_set_hypothesis_profile_ci>
            Test that unit tests phase sets HYPOTHESIS_PROFILE=ci.

            This ensures Hypothesis uses CI settings (100 examples) for unit tests,
            matching CI behavior exactly.
          <Function test_unit_tests_exclude_property_marker>
            Test that unit tests phase excludes property marker.

            User chose: "exclude property tests from unit test phase"

            This prevents property tests from running twice:
            - Once in unit tests phase (this test ensures they're excluded)
            - Once in dedicated property tests phase (with proper CI profile)
        <Class TestPrePushEnvironmentSanityChecks>
          Validate that pre-push hook has environment sanity checks.

          Codex recommendation: "Add a quick sanity check at the top of the pre-push
          script to assert that uv and .venv exist, printing a friendly setup hint
          instead of failing deep in the workflow."
          <Function test_pre_push_checks_venv_exists>
            Test that pre-push hook checks for .venv existence.

            Should fail early with helpful message if .venv doesn't exist.
          <Function test_pre_push_checks_uv_exists>
            Test that pre-push hook checks for uv command existence.

            Should fail early if uv is not installed.
          <Function test_pre_push_sanity_checks_run_early>
            Test that sanity checks run before any heavy work.

            Checks should be at the top of the script, not buried deep.
        <Class TestRegressionPrevention>
          Tests to ensure validation doesn't regress over time.
          <Function test_this_test_file_runs_in_ci>
            Meta-test: Ensure this test file itself runs in CI.
          <Function test_pre_push_hook_is_version_controlled_or_documented>
            Test that pre-push hook setup is documented.
          <Function test_minimum_validation_steps_documented>
            Test that minimum required validation steps are documented.
      <Module test_makefile_parallelization.py>
        Meta-test to validate that Makefile test targets are properly parallelized.

        TDD Approach:
        1. Write this test FIRST to enforce parallelization (RED)
        2. Add `-n auto` to Makefile targets (GREEN)
        3. Verify this test passes (REFACTOR)

        This ensures we never accidentally remove parallelization from test targets.
        <Class TestMakefileParallelization>
          Validate that test targets in Makefile use pytest-xdist for parallelization.
          <Function test_test_ci_uses_parallel_execution>
            test-ci should use -n auto for parallel execution to match CI behavior.

            GIVEN: The test-ci target in Makefile
            WHEN: Reading the Makefile
            THEN: test-ci should include -n auto flag
          <Function test_test_mcp_server_uses_parallel_execution>
            test-mcp-server should use -n auto for parallel execution.

            GIVEN: The test-mcp-server target in Makefile
            WHEN: Reading the Makefile
            THEN: test-mcp-server should include -n auto flag
          <Function test_test_new_uses_parallel_execution>
            test-new should use -n auto for parallel execution.

            GIVEN: The test-new target in Makefile
            WHEN: Reading the Makefile
            THEN: test-new should include -n auto flag
          <Function test_test_integration_local_uses_parallel_execution>
            test-integration-local should use -n auto for parallel execution.

            GIVEN: The test-integration-local target in Makefile
            WHEN: Reading the Makefile
            THEN: test-integration-local should include -n auto flag
          <Function test_test_e2e_uses_parallel_execution>
            test-e2e should use -n auto for parallel execution.

            GIVEN: The test-e2e target in Makefile
            WHEN: Reading the Makefile
            THEN: test-e2e should include -n auto flag
          <Function test_parallelized_targets_mention_parallel_in_output>
            Parallelized targets should mention 'parallel' in their echo messages for clarity.

            GIVEN: Parallelized test targets (test-ci, test-mcp-server, test-new, etc.)
            WHEN: Reading their echo messages
            THEN: Should mention 'parallel execution' or similar for user awareness
        <Class TestMakefileParallelizationBestPractices>
          Validate best practices for parallelized test targets.
          <Function test_all_unit_test_targets_are_parallelized>
            All test targets that run unit tests should use -n auto.

            GIVEN: Test targets that run unit tests (marked with -m unit)
            WHEN: Reading the Makefile
            THEN: All such targets should use -n auto for optimal performance
      <Module test_makefile_prepush_parity.py>
        Meta-validation: Ensure Makefile validate-pre-push matches pre-push hook exactly.

        This test suite ensures that:
        1. Makefile validate-pre-push target includes all validation steps from pre-push hook
        2. MyPy blocking behavior matches between Makefile and pre-push hook
        3. All pytest commands use same flags (-n auto, OTEL_SDK_DISABLED=true)
        4. Warning/error handling behavior is consistent

        TDD Principle: These tests MUST pass to ensure `make validate-pre-push` provides
        same validation as actual git push, preventing "works locally, fails on push" scenarios.

        Issue: Makefile has drifted from pre-push hook, making it less strict and creating
        false confidence when developers run `make validate-pre-push` before pushing.
        <Class TestMakefilePrePushParity>
          Validate Makefile validate-pre-push target matches pre-push hook.
          <Function test_makefile_includes_uv_pip_check>
            Test that Makefile validate-pre-push includes uv pip check.

            Dependency validation prevents conflicting package versions from being pushed.
            This is a Makefile-only check (not in pre-commit test orchestrator since that
            focuses on tests, while Makefile provides comprehensive CI-equivalent validation).

            Makefile validate-pre-push Phase 1 should include:
            - uv lock --check (lockfile validation)
            - uv pip check (dependency tree validation)

            Without uv pip check, dependency conflicts could pass local validation and only
            be caught during CI deployment, wasting time.
          <Function test_makefile_mypy_is_blocking>
            Test that Makefile MyPy step blocks on errors (not warning-only).

            MyPy type checking should be CRITICAL in validate-pre-push, matching CI behavior.
            This ensures type errors are caught before push, not discovered in CI.

            Makefile validate-pre-push Phase 2 should use blocking pattern:
            - Blocking: && echo success || (echo error && exit 1)
            - Non-blocking (wrong): || echo warning

            Without blocking MyPy, type errors pass local validation and fail in CI.
          <Function test_makefile_xdist_enforcement_uses_n_auto>
            Test that Makefile xdist enforcement test uses -n auto flag.

            pytest-xdist enforcement tests should run in parallel (-n auto) to validate that
            tests work correctly in parallel execution mode. This matches how CI runs tests
            and how the pre-commit test orchestrator runs tests.

            Without -n auto, xdist-specific isolation bugs could pass local validation
            and only be caught in CI.
          <Function test_makefile_phase_2_title_matches_behavior>
            Test that Makefile Phase 2 title reflects actual behavior.

            DOCUMENTATION DRIFT: Makefile Phase 2 title says "(Warning Only)" but
            after fixing MyPy to be blocking, the title should reflect critical behavior.

            Current Makefile (line 535):
                PHASE 2: Type Checking (Warning Only)

            After MyPy fix (blocking behavior):
                PHASE 2: Type Checking (Critical - matches CI)

            Fix: Update Makefile line 535 title after making MyPy blocking.
          <Function test_makefile_uses_correct_hook_stage_name>
            Test that Makefile validate-pre-push uses correct --hook-stage value.

            CRITICAL BUG: Makefile line 698 uses '--hook-stage push' which is INVALID.
            The correct value is '--hook-stage pre-push'.

            Impact:
            - pre-commit silently skips ALL hooks configured for pre-push stage
            - validate-pre-push target provides NO actual validation
            - Complete CI/CD parity failure - local validation doesn't match CI
            - Developers get false confidence that code is ready to push

            Pre-commit valid stage names:
            - commit
            - merge-commit
            - push-commit
            - prepare-commit-msg
            - commit-msg
            - post-commit
            - manual
            - pre-push  ← CORRECT
            - post-checkout
            - post-merge
            - post-rewrite

            Current Makefile (line 698):
                @pre-commit run --all-files --hook-stage push --show-diff-on-failure
                                                          ^^^^ INVALID - should be 'pre-push'

            Fix: Change line 698 to use '--hook-stage pre-push'

            Reference: OpenAI Codex Finding 1a - validate-pre-push stage name
        <Class TestMakefileEfficiency>
          Test that Makefile targets are optimized for developer productivity.
          <Function test_test_targets_do_not_have_redundant_uv_sync>
            Test that test-* targets don't have redundant uv sync calls.

            PERFORMANCE BUG: Four test targets (test-unit, test-property, test-contract,
            test-regression) run 'uv sync' before pytest, adding 30-60s overhead PER target.

            Impact:
            - Developers running multiple test types waste 2-4 minutes per iteration
            - 'make test-property && make test-contract' wastes 60-120s on redundant syncs
            - UV_RUN (uv run) already auto-syncs on demand, making explicit syncs unnecessary
            - Slower test loops discourage TDD workflow

            Current Makefile:
            - test-unit (line 234): @uv sync --extra dev --extra code-execution --quiet
            - test-property (line 369): @uv sync --extra dev --extra code-execution --quiet
            - test-contract (line 375): @uv sync --extra dev --extra code-execution --quiet
            - test-regression (line 382): @uv sync --extra dev --extra code-execution --quiet

            Why redundant:
            - Makefile line 12 defines: UV_RUN := uv run
            - 'uv run' automatically syncs if needed (on-demand)
            - Explicit sync before each target means syncing 4x even if deps unchanged

            Fix:
            - Remove explicit 'uv sync' from test targets
            - Rely on UV_RUN's automatic sync (faster, only syncs when needed)
            - Add prerequisite check: fail fast if .venv missing with clear error message

            Reference: OpenAI Codex Finding 1b - Redundant uv sync
        <Class TestMakefileValidationConsistency>
          Test that Makefile validation handles failures consistently.
          <Function test_makefile_critical_checks_exit_on_failure>
            Test that critical checks in Makefile exit on failure.

            Validates that critical validation steps (lockfile, unit tests, etc.)
            use '&& echo success || (echo failure && exit 1)' pattern to stop
            execution on failure.

            Non-critical checks can use '|| echo warning' pattern.
      <Module test_marker_enforcement.py>
        Meta-test to enforce pytest marker presence on all test files.

        This test validates that all test files have appropriate pytest markers
        to ensure they run in CI and are properly categorized.

        RATIONALE (OpenAI Codex Finding #1):
        - CI runs: pytest -n auto -m "unit and not llm"
        - Tests without @pytest.mark.unit are INVISIBLE to CI
        - 176 test files (57%) were unmarked, including critical guard-rail tests
        - This meta-test prevents regression by enforcing marker requirements

        VALIDATION CRITERIA:
        - All test files must have at least one marker from: unit, integration, e2e
        - Critical guard-rail tests must have both unit AND meta markers
        - Files can opt-out via MARKER_EXEMPT_FILES list (with justification)

        References:
        - .github/workflows/ci.yaml:243 - CI test filter
        - CLAUDE.md - Test marker conventions
        - OpenAI Codex validation report (2025-11-15)
        <Class TestMarkerEnforcement>
          Enforce pytest marker presence on all test files.
          <Function test_all_test_files_have_markers>
            RED PHASE TEST: Validates all test files have appropriate markers.

            This test will FAIL initially, proving it works. After adding markers
            to all test files, it will pass (GREEN phase).

            EXPECTED MARKERS:
            - unit: Fast tests, no external dependencies
            - integration: Tests requiring infrastructure (Redis, Keycloak, etc.)
            - e2e: End-to-end tests with full stack
            - meta: Meta-tests validating test infrastructure
          <Function test_critical_guard_rail_tests_have_meta_marker>
            Validates critical guard-rail tests have both 'unit' and 'meta' markers.

            Critical guard-rail tests prevent bad code from being committed:
            - gitignore validation
            - documentation integrity
            - workflow security
            - MDX validation
            - Mintlify docs validation

            These MUST run in CI to provide protection.
          <Function test_meta_test_classes_have_xdist_group_markers>
            Validates meta test classes have xdist_group markers for isolation.

            Related: OpenAI Codex Finding #5

            Meta test classes that perform repository-wide operations (reading/writing
            configuration files, checking documentation, etc.) should have xdist_group
            markers to ensure they run in the same worker and don't cause state pollution.

            Even if they don't use AsyncMock, grouping is still required for:
            1. Coding standard compliance
            2. Predictable worker assignment
            3. Future-proofing if mocks are added later
          <Function test_marker_enforcement_statistics>
            Reports statistics on marker coverage (informational, always passes).

            Provides visibility into marker adoption across the test suite.
      <Module test_mcp_public_interface.py>
        Meta-test: Validate MCP Public Interface

        This test ensures that MCPAgentServer exposes expected public methods,
        preventing regressions where tests try to call non-existent methods.

        Related: OpenAI Codex Finding 2025-11-15 - Integration test failures due to
        missing call_tool_public() method.
        <Class TestMCPPublicInterface>
          Validate MCPAgentServer public interface
          <Function test_mcp_server_has_call_tool_public>
            Verify MCPAgentServer.call_tool_public() method exists and is callable.

            This prevents regression of the issue where integration tests tried to call
            call_tool() directly on the server instance, but it was only available as
            an inner function in the decorator.

            References:
            - src/mcp_server_langgraph/mcp/server_stdio.py:272-355
            - tests/integration/test_mcp_code_execution.py:91,112,150,163
          <Function test_mcp_server_has_list_tools_public>
            Verify MCPAgentServer.list_tools_public() method exists and is callable.

            This follows the same pattern as call_tool_public - a public method
            that the MCP protocol handler delegates to.

            References:
            - src/mcp_server_langgraph/mcp/server_stdio.py:164-270
          <Function test_mcp_server_has_auth_attribute>
            Verify MCPAgentServer.auth attribute exists for authorization.

            This prevents regression where tests try to patch authorization methods
            that don't exist.

            References:
            - src/mcp_server_langgraph/mcp/server_stdio.py:335 (self.auth.authorize)
            - tests/integration/test_mcp_code_execution.py:106 (patch target)
          <Function test_mcp_server_public_methods_are_documented>
            Verify public methods have docstrings explaining their purpose.

            Good documentation helps prevent confusion about which methods are
            for testing vs production use.
      <Module test_mcp_skip_auth_enforcement.py>
        Meta-validation: Enforce MCP_SKIP_AUTH configuration in API test fixtures.

        This test suite ensures that API test fixtures explicitly set MCP_SKIP_AUTH="false"
        to prevent conftest.py pollution from affecting authentication behavior.

        CODEX FINDING (2025-11-13):
        tests/api/test_service_principals_endpoints.py has comments at lines 155-158 and 211-214
        stating the intent to set MCP_SKIP_AUTH="false" before creating FastAPI app, but the
        actual assignment is missing. This creates risk of conftest.py pollution where
        MCP_SKIP_AUTH="true" (set for most tests) leaks into service principal tests that
        need authentication enabled.

        TDD Principle: This meta-test enforces the fix MUST exist and prevents regression.
        <Class TestMCPSkipAuthFixtureEnforcement>
          Validate that API test fixtures explicitly set MCP_SKIP_AUTH="false".
          <Function test_sp_test_client_sets_mcp_skip_auth_false>
            Test that sp_test_client fixture sets MCP_SKIP_AUTH="false" explicitly.

            CRITICAL: The fixture comment at line 155-158 says:
            "CRITICAL: Set MCP_SKIP_AUTH="false" BEFORE creating app"

            But the actual assignment is missing. This test enforces it exists.
          <Function test_admin_test_client_sets_mcp_skip_auth_false>
            Test that admin_test_client fixture sets MCP_SKIP_AUTH="false" explicitly.

            Same requirement as sp_test_client - must prevent conftest.py pollution.
          <Function test_both_fixtures_restore_mcp_skip_auth_in_cleanup>
            Test that both fixtures document cleanup behavior for MCP_SKIP_AUTH.

            This is informational - the fixtures already clean up via dependency_overrides.clear()
            but we want to ensure the pattern is documented.
      <Module test_migration_checklists.py>
        Test suite for migration checklist validation.

        Validates that migration checklists exist and are properly structured:
        - .github/checklists/ directory exists
        - Required checklists are present
        - Checklists have proper markdown structure
        - Items are actionable and testable
        - Progress tracking sections exist

        Following TDD: These tests are written FIRST, before checklists exist.
        They will FAIL initially (RED phase), then PASS after implementation (GREEN phase).

        Regression prevention for Anthropic Claude Code best practices (large task checklists).
        See: https://www.anthropic.com/engineering/claude-code-best-practices
        <Class TestMigrationChecklists>
          Validate migration checklists exist and have proper structure.
          <Function test_checklists_directory_exists>
            Test that .github/checklists/ directory exists.
          <Function test_required_checklists_exist>
            Test that all required checklists are present.
          <Function test_checklists_are_markdown>
            Test that all checklists are markdown files.
          <Function test_type_safety_checklist_structure>
            Test that TYPE_SAFETY_MIGRATION.md has proper structure.
          <Function test_checklists_have_task_items>
            Test that checklists contain task items (- [ ] format).
          <Function test_checklists_are_not_empty>
            Test that checklists have substantial content (> 200 chars).
        <Class TestTypeSafetyChecklist>
          Specific validation for TYPE_SAFETY_MIGRATION.md.
          <Function test_mentions_145_errors>
            Test that checklist mentions the 145+ MyPy errors baseline.
          <Function test_has_phase_structure>
            Test that checklist has multi-phase breakdown.
          <Function test_references_mypy_commands>
            Test that checklist includes MyPy commands for verification.
      <Module test_mypy_enforcement.py>
        Test mypy type checking enforcement in pre-commit hooks.

        This module validates that mypy is properly configured and enabled in the
        pre-commit configuration, ensuring type safety is enforced locally before push.

        Related: OpenAI Codex Finding #2 - Mypy enforcement contradiction
        <Class TestMypyEnforcement>
          Validate mypy type checking is properly configured and enabled.
          <Function test_mypy_hook_exists_in_config>
            Verify mypy hook is defined in pre-commit config.
          <Function test_mypy_hook_is_not_commented_out>
            Verify mypy hook is active (not commented out in YAML).
          <Function test_mypy_runs_on_pre_push_stage>
            Verify mypy is configured to run during pre-push stage or manual stage.

            Manual stage is acceptable when there are extensive pre-existing type errors
            that would block all development. This allows incremental type safety improvements
            without blocking productive work.
          <Function test_mypy_targets_correct_package>
            Verify mypy is configured to check the correct source package.
          <Function test_mypy_has_appropriate_configuration>
            Verify mypy has sensible configuration flags.
          <Function test_mypy_passes_on_current_codebase>
            Verify mypy type checking passes on current codebase.

            This is the critical test - if mypy is re-enabled in pre-commit,
            it must actually pass on the current codebase. Otherwise developers
            will be blocked from committing.

            NOTE: Currently marked as xfail due to 148 pre-existing type errors that
            require substantial application code refactoring. This should be unmarked
            once type errors are resolved.
          <Function test_mypy_comment_reflects_actual_state>
            Verify comment in pre-commit config reflects reality (enabled, not disabled).
      <Module test_otel_security_context.py>
        Test OpenTelemetry Collector security context configuration.

        This test validates the fix for Trivy security findings:
        - AVD-KSV-0014 (HIGH): Container should set readOnlyRootFilesystem
        - AVD-KSV-0118 (HIGH): Container using default security context
        - AVD-KSV-0118 (HIGH): Deployment using default security context (allows root)

        The OTel collector deployment must have:
        - Pod-level security context (runAsNonRoot, runAsUser, fsGroup, seccomp)
        - Container-level security context (readOnlyRootFilesystem, drop ALL capabilities)
        - tmpfs volumes for writable directories (/tmp, /home/otelcol)

        Reference: Deploy to GKE Staging workflow failures with 5 HIGH Trivy findings
        Verified UID: 10001 (from official otel/opentelemetry-collector-contrib:0.137.0 image)
        <Function test_otel_deployment_has_pod_security_context>
          Verify that OTel collector deployment has pod-level security context.

          Required for Trivy AVD-KSV-0118 compliance:
          - runAsNonRoot: true
          - runAsUser: 10001 (verified from official image)
          - fsGroup: 10001
          - seccompProfile.type: RuntimeDefault
        <Function test_otel_deployment_has_container_security_context>
          Verify that OTel collector container has security context.

          Required for Trivy AVD-KSV-0014 and AVD-KSV-0118 compliance:
          - allowPrivilegeEscalation: false
          - readOnlyRootFilesystem: true
          - runAsNonRoot: true
          - runAsUser: 10001
          - capabilities.drop: [ALL]
        <Function test_otel_has_readonly_root_filesystem>
          Verify that readOnlyRootFilesystem is set to true.

          This is the specific finding from Trivy AVD-KSV-0014:
          "Container 'otel-collector' should set 'securityContext.readOnlyRootFilesystem' to true"
        <Function test_otel_runs_as_nonroot>
          Verify that OTel collector runs as non-root user.

          Both pod and container levels must specify runAsNonRoot: true.
          This addresses Trivy AVD-KSV-0118 finding about default security context.
        <Function test_otel_has_tmpfs_volumes>
          Verify that OTel collector has tmpfs volumes for writable directories.

          When readOnlyRootFilesystem is true, OTel needs writable volumes for:
          - /tmp - Temporary files
          - /home/otelcol - OTel collector home directory

          These should be emptyDir volumes (tmpfs in memory).
        <Function test_otel_drops_all_capabilities>
          Verify that OTel collector drops all Linux capabilities.

          Security best practice: Drop all capabilities unless specifically needed.
          OTel collector doesn't need any special capabilities.
        <Function test_rendered_staging_manifest_has_otel_security_context>
          Verify that the rendered staging manifest includes OTel security contexts.

          This is an integration test that renders the complete Kustomize overlay
          and validates that the security contexts are present in the final output
          that will be deployed to GKE.
      <Module test_path_helpers.py>
        Meta-tests for path helper utilities.

        **Purpose:**
        Validates that test path helpers work correctly and prevent file reference
        regressions (e.g., broken paths after file migrations).

        **OpenAI Codex Finding (2025-11-16):**
        - Regression tests referenced `tests/api/test_*.py` but files moved to `tests/integration/api/`
        - Hard-coded path construction caused FileNotFoundError and test failures
        - Need centralized, validated helper for integration test file references

        **Solution:**
        - `get_integration_test_file()` helper with existence validation
        - Meta-test ensures helper works correctly
        - Prevents future file migration regressions
        <Function test_get_integration_test_file_returns_valid_path>
          get_integration_test_file() returns correct path to integration test files.

          **Test Coverage:**
          - Returns Path object
          - Path points to tests/integration/{relative_path}
          - Path is absolute (not relative)
          - Relative path is correctly appended

          **Regression Prevention:**
          - Validates helper works for known files (test_api_keys_endpoints.py)
          - Ensures path construction is correct
          - Guards against path manipulation bugs
        <Function test_get_integration_test_file_validates_existence>
          get_integration_test_file() raises FileNotFoundError for non-existent files.

          **Test Coverage:**
          - Raises FileNotFoundError (not generic Exception)
          - Error message includes the attempted path
          - Prevents silent failures from broken references

          **Security Note:**
          - Fail-fast on invalid paths prevents accidental file creation
          - Clear error messages aid debugging
        <Function test_get_integration_test_file_handles_nested_paths>
          get_integration_test_file() works with nested directory structures.

          **Test Coverage:**
          - Handles multi-level paths (e.g., "api/auth/test_bearer.py")
          - Path separator handling (forward slashes)
          - Relative path construction from tests/integration/ base

          **Use Cases:**
          - API tests: api/test_*.py
          - Auth tests: api/auth/test_*.py
          - Storage tests: storage/test_*.py
        <Function test_get_integration_test_file_works_with_actual_files>
          Integration test: Verify helper works with real integration test files.

          **Test Coverage:**
          - Tests against actual filesystem (not mocked)
          - Validates file.exists() returns True for real files
          - Ensures end-to-end functionality

          **Known Files (as of 2025-11-16):**
          - tests/integration/api/test_api_keys_endpoints.py
          - tests/integration/api/test_service_principals_endpoints.py
        <Function test_get_integration_test_file_prevents_directory_traversal>
          get_integration_test_file() doesn't allow directory traversal attacks.

          **Security Test:**
          - Rejects paths with ".." (parent directory)
          - Prevents escaping tests/integration/ sandbox
          - Raises ValueError for malicious paths

          **Attack Scenarios:**
          - "../../../etc/passwd"
          - "api/../../secret.py"
          - "./api/../../../etc/hosts"
        <Function test_regression_path_references_are_valid>
          Meta-regression test: Validate all path references in regression tests.

          **Purpose:**
          - Scans all regression test files for Path() construction patterns
          - Validates referenced files exist
          - Prevents broken file references from file migrations

          **Regression Prevention:**
          - Catches hard-coded paths to tests/api/ (old location)
          - Ensures all references use tests/integration/api/ (new location)
          - Alerts to file moves before regression tests break

          **Known Issues (2025-11-16):**
          - test_bearer_scheme_override_diagnostic.py:52, 325
          - test_uv_lockfile_sync.py:273, 294
      <Module test_performance_regression.py>
        Meta-tests to prevent performance regressions in the test suite.

        CODEX FINDING #2: Timeout tests were using real asyncio.sleep(5-10s) calls,
        burning ~15s per test run. This meta-test ensures timeout tests complete quickly.

        TDD Approach:
        1. Write this test FIRST (will fail with slow sleeps)
        2. Refactor timeout tests to use shorter sleeps
        3. Verify this test passes
        <Class TestTimeoutTestPerformance>
          Validate that timeout tests execute quickly (CODEX Finding #2)
          <Function test_timeout_tests_complete_within_2_seconds>
            CODEX FINDING #2: Timeout tests should not use real long sleeps.

            GIVEN: Timeout tests in test_parallel_executor_timeout.py
            WHEN: Running all timeout tests
            THEN: Should complete in < 2 seconds (not 15+ seconds)

            This test enforces that timeout tests use efficient mocking/short sleeps
            instead of burning time with real long sleeps.

            NOTE: This test is skipped when running under pytest-xdist (parallel mode)
            because the subprocess timing is affected by parallel execution overhead.
          <Function test_timeout_tests_use_short_sleep_values>
            CODEX FINDING #2: Ensure timeout tests don't use sleep values > 1 second.

            This is a static check that scans the test file for long sleep calls.
        <Class TestTimeBudgets>
          Enforce per-test time budgets to prevent performance regressions.
          <Function test_unit_tests_complete_within_budget>
            Unit tests should complete quickly (< 100ms per test on average).

            This budget excludes setup/teardown and focuses on test execution time.
          <Function test_integration_tests_have_reasonable_budgets>
            Integration tests should complete within reasonable time (< 5s per test).

            Tests exceeding this should be marked as @pytest.mark.slow and
            run separately in CI.
        <Class TestPropertyTestBudgets>
          Validate property test deadlines are reasonable.
          <Function test_property_tests_have_reasonable_deadlines>
            Property tests should have deadline < 3000ms per example.

            CODEX audit found property tests with long deadlines that burn time
            when examples fail or edge cases are explored.
        <Class TestBulkheadPerformance>
          Validate bulkhead tests don't burn unnecessary time.
          <Function test_bulkhead_tests_use_short_sleeps>
            Bulkhead tests should use sleep values < 0.5s.

            CODEX audit found bulkhead tests with 1s sleeps that could be reduced.
        <Class TestPollingOptimizations>
          Validate polling helpers are used instead of fixed sleeps.
          <Function test_kubernetes_sandbox_uses_polling>
            Kubernetes sandbox cleanup should use poll_until() instead of fixed sleep.

            CODEX audit found 5-second sleeps that could be replaced with polling.
          <Function test_docker_sandbox_uses_polling>
            Docker sandbox cleanup should use poll_until() instead of fixed sleep.

            CODEX audit found 2-second sleeps that could be replaced with polling.
        <Class TestVirtualClockAvailability>
          Validate VirtualClock is available for future optimizations.
          <Function test_virtual_clock_exists_and_works>
            VirtualClock should be available for instant time advancement in tests.
          <Function test_time_fixtures_available>
            Time fixtures should be available for test use.
      <Module test_performance_regression_suite.py>
        Performance Regression Test - Test Suite Duration

        Validates that the entire unit test suite completes within acceptable time limits.

        This meta-test prevents performance regression by failing if:
        - Total test suite duration > 120 seconds (2 minutes)

        Performance targets based on Codex findings:
        - Current: 220s (3m 40s) ❌ TOO SLOW
        - Target: < 120s (2 minutes) ✅ ACCEPTABLE
        - Ideal: < 60s (1 minute) ⭐ EXCELLENT

        Why this matters:
        - Fast tests = faster development iteration
        - Slow tests discourage running tests frequently
        - Performance regressions accumulate over time

        How to fix if this test fails:
        1. Check test durations: pytest --durations=20
        2. Look for tests > 5s (should be < 1s for unit tests)
        3. Common causes:
           - Unnecessary sleeps/waits
           - Real I/O instead of mocks
           - Circuit breaker/retry delays
           - Large dataset generation
        4. Optimization strategies:
           - Use fast_resilience_config fixture for CB tests
           - Use freezegun for time-based tests
           - Mock expensive operations
           - Reduce test data size

        Related:
        - tests/meta/test_slow_test_detection.py - Detects individual slow tests
        - tests/conftest.py:fast_resilience_config - Reduces CB timeouts
        <Function test_unit_test_suite_performance>
          Test that unit test suite completes within 120 seconds.

          This is a meta-test that validates test suite performance to prevent regression.

          IMPORTANT: This test runs the full unit test suite, so it's slow by design.
          It should be run:
          - In CI/CD to catch performance regressions
          - Before releases to ensure acceptable performance
          - NOT in regular development (too slow for TDD workflow)

          Skip in development with: pytest -m "not performance"
      <Module test_performance_workflow_pytest_flags.py>
        Test that performance regression workflow correctly handles pytest flags.

        This test validates the fix for the issue where performance benchmarks fail because:
        1. Workflow disables xdist with `-p no:xdist` to avoid pytest-benchmark conflicts
        2. But `--dist loadscope` from pyproject.toml addopts is still applied
        3. Without xdist plugin, `--dist` flag is unrecognized causing: "pytest: error: unrecognized arguments: --dist"

        The fix ensures addopts is overridden to exclude xdist-specific flags when running benchmarks.

        Reference: Performance Regression Detection workflow failures (runs 19250359776, 19250511465, etc.)
        <Function test_performance_workflow_overrides_addopts_to_exclude_xdist_flags>
          Verify that the performance regression workflow overrides addopts to exclude --dist flag.

          When using `-p no:xdist` to disable xdist (required for pytest-benchmark),
          the --dist flag from pyproject.toml addopts must be removed, otherwise pytest fails with:
          "pytest: error: unrecognized arguments: --dist"

          Expected solution: Use `-o addopts="..."` to override addopts without --dist flag
        <Function test_performance_workflow_disables_xdist_for_benchmark_compatibility>
          Verify that the performance workflow disables xdist to avoid pytest-benchmark conflicts.

          pytest-benchmark auto-disables when xdist is active, which conflicts with --benchmark-only,
          causing: "Can't have both --benchmark-only and --benchmark-disable options"

          The workflow must use `-p no:xdist` to prevent this.
        <Function test_performance_workflow_uses_benchmark_only_mode>
          Verify that the performance workflow runs only benchmark tests, skipping regular tests.

          This ensures performance tests are isolated and run with proper benchmark configuration.
        <Function test_pyproject_toml_addopts_includes_dist_flag>
          Verify that pyproject.toml addopts includes --dist flag (for normal test runs).

          This confirms that the --dist flag exists in the default config,
          which is why the performance workflow needs to override it.
      <Module test_plugin_guards.py>
        Meta-tests to validate pytest plugin behavior with various CLI modes

        TDD Regression Test: Ensures fixture organization plugin doesn't interfere
        with informational pytest commands like --version, --markers, --fixtures

        References:
        - OpenAI Codex finding: Plugin may exit during harmless --collect-only runs
        - pytest best practices: Plugins should not block informational commands
        <Class TestPluginCLIModeGuards>
          Validate that pytest plugins don't interfere with CLI informational modes
          <Function test_plugin_allows_help_command>
            TDD REGRESSION TEST: Ensure plugin doesn't block --help

            GIVEN: Pytest with fixture organization plugin
            WHEN: Running pytest --help
            THEN: Command succeeds without fixture validation errors
          <Function test_plugin_allows_version_command>
            TDD REGRESSION TEST: Ensure plugin doesn't block --version

            GIVEN: Pytest with fixture organization plugin
            WHEN: Running pytest --version
            THEN: Command succeeds without fixture validation errors
          <Function test_plugin_allows_markers_command>
            TDD REGRESSION TEST: Ensure plugin doesn't block --markers

            GIVEN: Pytest with fixture organization plugin
            WHEN: Running pytest --markers
            THEN: Command succeeds and shows markers without fixture validation
          <Function test_plugin_allows_fixtures_command>
            TDD REGRESSION TEST: Ensure plugin doesn't block --fixtures

            GIVEN: Pytest with fixture organization plugin
            WHEN: Running pytest --fixtures
            THEN: Command succeeds and shows fixtures without validation errors
          <Function test_plugin_allows_collect_only_command>
            TDD REGRESSION TEST: Ensure plugin doesn't block --collect-only

            GIVEN: Pytest with fixture organization plugin
            WHEN: Running pytest --collect-only
            THEN: Command succeeds and collects tests without running validation
      <Module test_pre_push_orchestrator.py>
        Tests for the pre-push test orchestrator script.

        This test suite validates that the orchestrator correctly combines multiple
        test marker expressions into a single pytest session to eliminate redundant
        test discovery overhead.

        TDD Principle: These tests ensure the orchestrator correctly implements the
        Codex Finding 2a fix - consolidating 5 separate pytest sessions into one.
        <Class TestPrePushOrchestrator>
          Test the pre-push test orchestrator script.
          <Function test_orchestrator_script_exists>
            Test that the orchestrator script exists and is executable.
          <Function test_orchestrator_combines_all_markers>
            Test that orchestrator combines all test markers from 5 hooks.

            The orchestrator MUST combine these marker expressions:
            1. unit and not llm (from run-unit-tests)
            2. tests/smoke/ (from run-smoke-tests)
            3. api and unit and not llm (from run-api-tests)
            4. tests/unit/test_mcp_stdio_server.py -m "not llm" (from run-mcp-server-tests)
            5. property (from run-property-tests)

            Strategy: Run single pytest session with combined marker logic
          <Function test_orchestrator_uses_parallel_execution>
            Test that orchestrator uses pytest-xdist for parallel execution.
          <Function test_orchestrator_fails_fast>
            Test that orchestrator stops on first failure (-x).
          <Function test_orchestrator_uses_short_traceback>
            Test that orchestrator uses short traceback format.
          <Function test_orchestrator_excludes_llm_tests>
            Test that orchestrator excludes LLM tests (require API keys).
          <Function test_orchestrator_has_proper_shebang>
            Test that orchestrator script has proper Python shebang.
      <Module test_precommit_docker_image_validation.py>
        Meta-tests for pre-commit hook: validate-docker-image-contents

        Tests ensure the Docker image validation hook works correctly
        and prevents regression of Codex findings related to Docker image contents.

        TDD Cycle: RED → GREEN → REFACTOR

        Reference: ADR-0053 Future Work - Pre-commit hook: validate-docker-image-contents
        <Class TestDockerImageValidationHook>
          Test suite for validate_docker_image_contents.py pre-commit hook
          <Function test_hook_script_exists>
            Validate that the validation script exists.

            GIVEN: The scripts directory
            WHEN: Checking for validate_docker_image_contents.py
            THEN: The script should exist and be executable
          <Function test_hook_validates_required_directories_copied>
            Validate hook passes when required directories are copied.

            GIVEN: A Dockerfile with required COPY commands for src/, tests/, pyproject.toml
            WHEN: Running the validation hook
            THEN: Hook should exit with code 0 (success)
          <Function test_hook_fails_when_src_directory_missing>
            Validate hook fails when src/ directory is not copied.

            GIVEN: A Dockerfile missing COPY src/ command
            WHEN: Running the validation hook
            THEN: Hook should exit with code 1 (failure)
          <Function test_hook_fails_when_tests_directory_missing>
            Validate hook fails when tests/ directory is not copied.

            GIVEN: A Dockerfile missing COPY tests/ command
            WHEN: Running the validation hook
            THEN: Hook should exit with code 1 (failure)
          <Function test_hook_fails_when_pyproject_toml_missing>
            Validate hook fails when pyproject.toml is not copied.

            GIVEN: A Dockerfile missing COPY pyproject.toml command
            WHEN: Running the validation hook
            THEN: Hook should exit with code 1 (failure)
          <Function test_hook_validates_scripts_not_copied>
            Validate hook ensures scripts/ is NOT copied (correct design).

            GIVEN: A Dockerfile with COPY scripts/ command
            WHEN: Running the validation hook
            THEN: Hook should warn or fail (scripts/ should only be on host)
          <Function test_hook_validates_deployments_not_copied>
            Validate hook ensures deployments/ is NOT copied (correct design).

            GIVEN: A Dockerfile with COPY deployments/ command
            WHEN: Running the validation hook
            THEN: Hook should warn or fail (deployments/ should only be on host)
          <Function test_hook_validates_correct_stage>
            Validate hook checks COPY commands in the correct Dockerfile stage.

            GIVEN: A Dockerfile with final-test stage
            WHEN: Running the validation hook
            THEN: Hook should validate COPY commands in final-test stage only
          <Function test_hook_with_production_dockerfile>
            Validate hook works with the actual Dockerfile.

            GIVEN: The actual docker/Dockerfile file
            WHEN: Running the validation hook
            THEN: Hook should pass (design is correct per ADR-0053)
      <Module test_precommit_keycloak_validation.py>
        Meta-tests for pre-commit hook: validate-keycloak-config

        Tests ensure the Keycloak configuration validation hook works correctly
        and prevents regression of Codex findings related to Keycloak service availability.

        TDD Cycle: RED → GREEN → REFACTOR

        Reference: ADR-0053 Future Work - Pre-commit hook: validate-keycloak-config
        <Class TestKeycloakConfigValidationHook>
          Test suite for validate_keycloak_config.py pre-commit hook
          <Function test_hook_script_exists>
            Validate that the validation script exists.

            GIVEN: The scripts directory
            WHEN: Checking for validate_keycloak_config.py
            THEN: The script should exist and be executable
          <Function test_hook_detects_keycloak_service_enabled>
            Validate hook passes when Keycloak service is enabled.

            GIVEN: A docker-compose.test.yml with Keycloak service enabled
            WHEN: Running the validation hook
            THEN: Hook should exit with code 0 (success)
          <Function test_hook_fails_when_keycloak_service_missing>
            Validate hook fails when Keycloak service is missing.

            GIVEN: A docker-compose.test.yml without Keycloak service
            WHEN: Running the validation hook
            THEN: Hook should exit with code 1 (failure)
          <Function test_hook_validates_health_check_exists>
            Validate hook checks for health check configuration.

            GIVEN: A docker-compose.test.yml with Keycloak but no health check
            WHEN: Running the validation hook
            THEN: Hook should exit with code 1 (failure)
          <Function test_hook_validates_environment_variables>
            Validate hook checks for required environment variables.

            GIVEN: A docker-compose.test.yml with Keycloak but missing env vars
            WHEN: Running the validation hook
            THEN: Hook should exit with code 1 (failure)
          <Function test_hook_validates_start_period>
            Validate hook checks for adequate start_period (60s for Keycloak).

            GIVEN: A docker-compose.test.yml with Keycloak but short start_period
            WHEN: Running the validation hook
            THEN: Hook should exit with code 1 (warning/failure)
          <Function test_hook_with_production_docker_compose_file>
            Validate hook works with the actual docker-compose.test.yml.

            GIVEN: The actual docker-compose.test.yml file
            WHEN: Running the validation hook
            THEN: Hook should pass (Keycloak is enabled per ADR-0053)
      <Module test_property_test_quality.py>
        Meta-tests for Property Test Quality

        These tests validate that property tests follow best practices and don't have
        common anti-patterns identified in Codex findings.

        Purpose: Prevent regression of Codex Finding #9 (property tests with missing assertions)
        <Class TestPropertyTestQuality>
          Meta-tests that validate property tests have proper assertions.

          RED: These tests will fail if property tests lack assertions.
          GREEN: Property tests properly assert results.
          <Function test_property_tests_have_assertions>
            Validate all Hypothesis property tests have at least one assertion.

            This prevents regressions of Codex Finding #9:
            "Property tests like the TTL check call cache.get(...) without asserting
            the result because the design conflates 'miss' vs 'cached None'."

            RED: Fails if any property test lacks assertions.
            GREEN: All property tests properly validate behavior.
          <Function test_property_tests_dont_ignore_return_values>
            Validate property tests don't call functions without using/asserting results.

            This catches patterns like:
                result = cache.get("key")  # No assertion!

            Ignores legitimate side-effect calls in loops for statistics generation.

            RED: Fails if property tests have unused function calls.
            GREEN: All function calls are properly validated.
        <Class TestPropertyTestNaming>
          Validate property test naming conventions
          <Function test_property_test_names_describe_property>
            Property test names should describe the property being tested.

            Good: test_cache_expires_after_ttl
            Bad: test_cache, test_case_1
      <Module test_pytest_config_consistency.py>
        Test that validates pytest configuration consistency across the codebase.

        OpenAI Codex Finding (2025-11-16):
        ===================================
        tests/deployment/pytest.ini creates duplicate pytest configuration that conflicts
        with root pyproject.toml settings, causing tests to behave differently when run
        from the deployment directory.

        Issues with duplicate config:
        - testpaths: pytest.ini uses "." (current dir), pyproject.toml uses ["tests"] (root)
        - pythonpath: pytest.ini MISSING, pyproject.toml has [".", "scripts"]
        - addopts: pytest.ini missing --dist loadscope --timeout=60 --benchmark-disable
        - markers: pytest.ini has 11 markers, pyproject.toml has 38 markers (superset)

        Solution:
        - Single source of truth: pyproject.toml [tool.pytest.ini_options]
        - Delete tests/deployment/pytest.ini
        - All tests inherit same configuration regardless of run location

        This test ensures no duplicate pytest configurations exist.
        <Class TestPytestConfigConsistency>
          Validate single source of truth for pytest configuration.
          <Function test_no_duplicate_pytest_ini_files>
            GIVEN: Project root and all subdirectories
            WHEN: Searching for pytest configuration files
            THEN: Only pyproject.toml should contain pytest configuration
                  No pytest.ini, .pytest.ini, tox.ini, or setup.cfg with [tool:pytest] sections

            RED PHASE: This test should FAIL initially (tests/deployment/pytest.ini exists)
            GREEN PHASE: Test passes after deleting duplicate config files
          <Function test_pyproject_toml_has_pytest_config>
            GIVEN: Project root pyproject.toml
            WHEN: Checking for pytest configuration
            THEN: Should have [tool.pytest.ini_options] section with required settings
          <Function test_pytest_config_includes_timeout_protection>
            GIVEN: Project pyproject.toml pytest configuration
            WHEN: Checking addopts setting
            THEN: Should include --timeout flag for test timeout protection

            This prevents tests from hanging indefinitely.
          <Function test_pytest_config_includes_xdist_settings>
            GIVEN: Project pyproject.toml pytest configuration
            WHEN: Checking addopts setting
            THEN: Should include pytest-xdist settings for parallel execution

            Validates --dist loadscope is configured for proper test distribution.
      <Module test_pytest_config_validation.py>
        Test that pytest configuration (addopts) is compatible with installed plugins.

        This test validates the fix for the Codex finding where pytest addopts can
        reference flags from plugins that are not installed, causing cryptic CI failures.

        The validator checks that:
        1. All flags in pytest.addopts have required plugin dependencies
        2. All required plugins are actually installed
        3. Changes to addopts or dependencies trigger validation

        Reference: Codex finding - pytest addopts compatibility validation
        <Function test_pytest_addopts_flags_have_required_plugins>
          Verify that all pytest flags in addopts have required plugin dependencies.

          This prevents CI failures when someone:
          - Adds a flag to addopts without installing the plugin
          - Removes a plugin without removing dependent flags from addopts
          - Refactors dependencies and accidentally removes required plugins

          Expected mappings:
          - --dist, -n → pytest-xdist
          - --timeout → pytest-timeout
          - --cov → pytest-cov
          - --benchmark → pytest-benchmark

          NOTE: This test calls the validation script (scripts/validate_pytest_config.py)
          instead of duplicating the validation logic. The script is the source of truth.
        <Function test_validation_script_exists_and_is_executable>
          Verify that the pytest config validation script exists and can be run.

          The validation script should be at: scripts/validate_pytest_config.py
          It should be executable and return exit code 0 when validation passes.
        <Function test_all_pytest_flags_are_supported_by_installed_plugins>
          Verify that pytest can recognize all flags in addopts.

          This is a smoke test that runs `pytest --help` and checks that
          all flags from addopts appear in the help output.

          This catches cases where:
          - A plugin is installed but the wrong flag is used
          - A flag is misspelled in addopts
          - A plugin is version-incompatible and doesn't provide expected flags
        <Function test_pre_commit_hook_validates_pytest_config>
          Verify that pre-commit hook is configured to validate pytest config.

          This ensures that changes to pyproject.toml trigger validation
          before they can be committed.
        <Function test_ci_workflow_validates_pytest_config>
          Verify that CI workflow includes pytest config validation.

          This ensures validation runs in CI even if pre-commit hooks are bypassed.
        <Function test_pytest_plugins_are_actually_importable>
          Verify that all required pytest plugins can be imported.

          This is a runtime check that the plugins are not only listed in
          dependencies but are actually installed and importable.
      <Module test_pytest_marker_registration.py>
        Meta-test: Pytest Marker Registration Validation

        Ensures all pytest markers used in test files are registered in pyproject.toml.

        This test prevents marker registration errors like:
            PytestUnknownMarkWarning: Unknown pytest.mark.foo
            'foo' not found in `markers` configuration option

        See: https://docs.pytest.org/en/stable/how-to/mark.html
        <Function test_all_used_markers_are_registered>
          CRITICAL: Verify all pytest markers used in tests are registered in pyproject.toml

          This test prevents CI failures due to unregistered markers.

          Failure means:
          - A test file uses @pytest.mark.foo but foo is not in pyproject.toml
          - Solution: Add the marker to [tool.pytest.ini_options].markers in pyproject.toml
        <Function test_no_unused_marker_registrations>
          Optional: Warn about registered markers that are never used

          This helps keep pyproject.toml clean and identifies outdated markers.

          Note: This is a warning, not a failure. Unused markers may be intended
          for future use or external tooling.
        <Function test_marker_naming_conventions>
          Validate marker naming follows conventions

          Conventions:
          - Lowercase with underscores (snake_case)
          - No hyphens or spaces
          - Descriptive names
      <Module test_pytest_xdist_enforcement.py>
        Meta-tests for pytest-xdist isolation enforcement.

        These tests validate that our enforcement mechanisms (pre-commit hooks,
        validation scripts, regression tests) are working correctly and will
        catch violations.

        PURPOSE:
        --------
        Ensure that the patterns we've established to prevent pytest-xdist issues
        are actually enforced and violations will be caught before reaching production.

        This is a "test of the tests" - meta-validation that our guardrails work.

        References:
        - ADR-0052: Pytest-xdist Isolation Strategy
        - OpenAI Codex Findings: All resolved issues
        <Class TestEnforcementMechanisms>
          Validate that enforcement mechanisms catch pytest-xdist violations.

          These meta-tests ensure our guardrails work correctly.
          <Function test_validation_scripts_exist>
            🟢 GREEN: Verify all validation scripts exist.

            These scripts are run by pre-commit hooks and should exist.
          <Function test_pre_commit_hooks_configured>
            🟢 GREEN: Verify pre-commit hooks are configured for pytest-xdist.

            These hooks should catch violations before commit.
          <Function test_regression_tests_exist_for_all_codex_findings>
            🟢 GREEN: Verify regression tests exist for all Codex findings.

            Each Codex finding should have regression tests documenting the issue.
          <Function test_worker_utils_library_exists>
            🟢 GREEN: Verify worker utilities library exists.

            This library centralizes worker-aware logic.
          <Function test_conftest_uses_worker_aware_patterns>
            🟢 GREEN: Verify fixtures use worker-aware patterns.

            Critical fixtures should use PYTEST_XDIST_WORKER environment variable.

            Current Architecture (Single Shared Infrastructure):
            - FIXED ports (no per-worker offsets) in conftest.py
            - Logical isolation via PostgreSQL schemas, Redis DB indices in database_fixtures.py
            - Session-scoped infrastructure shared across all workers
          <Function test_documentation_exists>
            🟢 GREEN: Verify all documentation exists.

            Documentation should exist for pytest-xdist isolation patterns.
          <Function test_adr_references_regression_tests>
            🟢 GREEN: Verify ADR-0052 references the regression tests.

            The ADR should link to the regression tests as evidence.
          <Function test_best_practices_doc_has_worker_isolation_section>
            🟢 GREEN: Verify PYTEST_XDIST_BEST_PRACTICES.md has worker isolation section.

            The guide should document worker-scoped resource patterns.
        <Class TestEnforcementGaps>
          Identify gaps in current enforcement and suggest improvements.

          These tests document what is NOT currently enforced automatically.
          <Function test_gap_no_check_for_hardcoded_ports>
            🔴 GAP: No automated check for hardcoded ports in test_infrastructure_ports.

            PROBLEM:
            --------
            If someone modifies test_infrastructure_ports to use hardcoded ports
            instead of worker-aware ports, there's no pre-commit hook to catch it.

            CURRENT ENFORCEMENT:
            --------------------
            - Regression tests (test_pytest_xdist_port_conflicts.py)
            - Documentation (PYTEST_XDIST_BEST_PRACTICES.md)
            - Code review

            MISSING:
            --------
            - Pre-commit hook that parses test_infrastructure_ports
            - Validates it uses PYTEST_XDIST_WORKER
            - Validates it calculates offsets

            RECOMMENDATION:
            ---------------
            Create scripts/validate_worker_aware_fixtures.py to check:
            1. test_infrastructure_ports uses PYTEST_XDIST_WORKER
            2. postgres_connection_clean creates worker schema
            3. redis_client_clean selects worker DB
          <Function test_gap_no_check_for_os_environ_mutations>
            🔴 GAP: No AST-based check for os.environ mutations without monkeypatch.

            PROBLEM:
            --------
            If someone writes: os.environ["KEY"] = "value" in a test without using
            monkeypatch, it will cause environment pollution in pytest-xdist.

            CURRENT ENFORCEMENT:
            --------------------
            - Regression tests
            - Documentation
            - Code review
            - validate_test_isolation.py (partial - only checks some patterns)

            MISSING:
            --------
            - AST-based pre-commit hook that finds:
              - os.environ[...] = ... (direct assignment)
              - os.environ.update(...)
              - os.putenv(...)
            - And validates monkeypatch is used instead

            RECOMMENDATION:
            ---------------
            Create scripts/check_environ_mutations.py with AST parsing:
            - Find all os.environ mutations
            - Verify they're in fixtures with monkeypatch parameter
            - Or verify they restore original value in finally block
          <Function test_gap_no_runtime_validation_of_bearer_scheme_override>
            🔴 GAP: No runtime validation for bearer_scheme override requirement.

            PROBLEM:
            --------
            When get_current_user is overridden, bearer_scheme must also be
            overridden. Currently this is only documented and tested via
            regression tests.

            CURRENT ENFORCEMENT:
            --------------------
            - Regression tests (test_bearer_scheme_override_is_required)
            - Documentation (PYTEST_XDIST_BEST_PRACTICES.md)
            - validate_test_fixtures.py (partial check)

            MISSING:
            --------
            - Runtime validation in conftest_fixtures_plugin.py
            - AST-based pre-commit hook
            - Could scan for app.dependency_overrides[get_current_user]
            - And verify bearer_scheme is also overridden

            RECOMMENDATION:
            ---------------
            This was intentionally deferred (per test_conftest_fixtures_plugin_enhancements.py)
            because static analysis is complex and we have good coverage.

            DECISION: Acceptable gap - mitigated by:
            - Regression tests
            - Validation scripts
            - Code review
            - TDD backstop (test_fastapi_auth_override_sanity.py)
          <Function test_gap_no_enforcement_of_worker_utils_usage>
            🔴 GAP: No check that fixtures use worker_utils instead of manual calculations.

            PROBLEM:
            --------
            New code could manually calculate worker offsets instead of using
            the centralized worker_utils library, leading to inconsistencies.

            CURRENT ENFORCEMENT:
            --------------------
            - Documentation recommends worker_utils
            - Code review

            MISSING:
            --------
            - Check that fixtures use worker_utils functions
            - Warn if manual calculations detected (e.g., worker_num * 100)

            RECOMMENDATION:
            ---------------
            Create a linting rule or validation script that:
            - Detects worker_num * 100 patterns
            - Suggests using get_worker_port_offset() instead
            - Detects f"test_worker_{worker_id}"
            - Suggests using get_worker_postgres_schema() instead
        <Class TestEnforcementStrategy>
          Document the complete enforcement strategy.

          This test serves as living documentation of how we prevent
          pytest-xdist isolation issues.
          <Function test_enforcement_strategy_documentation>
            📚 Document the multi-layered enforcement strategy.

            Our enforcement uses defense-in-depth with multiple layers.
        <Class TestEnforcementRecommendations>
          Recommend additional enforcement mechanisms if needed.
          <Function test_recommendation_create_enforcement_meta_test>
            ✅ IMPLEMENTED: Create meta-test validating enforcement.

            This file (test_pytest_xdist_enforcement.py) IS the meta-test!

            It validates:
            - Validation scripts exist
            - Pre-commit hooks are configured
            - Regression tests exist
            - Documentation exists
            - Patterns are implemented
          <Function test_recommendation_optional_ast_based_hooks>
            📋 OPTIONAL: Create AST-based pre-commit hooks for critical patterns.

            These are OPTIONAL enhancements that could be added in the future:

            1. scripts/check_os_environ_mutations.py
               - Parse test files with AST
               - Find os.environ assignments
               - Verify monkeypatch is used
               - Warn on violations

            2. scripts/check_hardcoded_test_ports.py
               - Parse conftest.py:test_infrastructure_ports
               - Verify it uses PYTEST_XDIST_WORKER
               - Verify it calculates offsets
               - Fail if hardcoded

            3. scripts/check_bearer_scheme_overrides.py
               - Find app.dependency_overrides[get_current_user]
               - Verify bearer_scheme is also overridden
               - Fail on missing override

            DECISION:
            ---------
            These are NOT implemented because:
            - Current enforcement is sufficient
            - AST parsing is complex and error-prone
            - Regression tests provide good coverage
            - Patterns are already fixed in code
            - Code review catches new violations

            If violations start occurring despite current enforcement,
            we can add these hooks. For now, YAGNI applies.
          <Function test_enforcement_is_sufficient>
            ✅ CONCLUSION: Current enforcement is sufficient.

            Multi-layered defense-in-depth provides adequate protection:

            1. Documentation (guides developers to correct patterns)
            2. Worker utilities (makes it easy to do it right)
            3. Regression tests (catch violations in test runs)
            4. Pre-commit hooks (catch many violations before commit)
            5. Validation scripts (comprehensive pattern checking)
            6. Runtime validation (conftest plugin)
            7. CI/CD (validates changes before merge)
            8. Code review (human validation)

            With 8 layers of defense, the probability of a violation
            reaching production is extremely low.

            Estimated violation prevention rate: > 99%

            Conclusion: No additional enforcement needed at this time.
        <Class TestXdistGroupCoverage>
          Enforce 100% xdist_group coverage on integration tests.

          These tests validate that ALL integration tests have xdist_group markers
          to prevent worker isolation issues and memory leaks.

          References:
          - OpenAI Codex Finding #4: Pytest-xdist isolation inconsistencies
          - ADR-0052: Pytest-xdist Isolation Strategy
          - MEMORY_SAFETY_GUIDELINES.md
          <Function test_all_integration_tests_have_xdist_group_marker>
            🟢 GREEN: Verify ALL integration tests have xdist_group markers.

            Enforces 100% coverage to prevent worker isolation issues.

            REQUIREMENT:
            ------------
            Every test file with @pytest.mark.integration MUST have at least one
            @pytest.mark.xdist_group(name="...") marker to ensure proper worker
            isolation and prevent:
            - Port conflicts between workers
            - Database isolation failures
            - Environment variable pollution
            - Memory leaks from AsyncMock/MagicMock accumulation

            TARGET: 100% coverage (not 80%)
          <Function test_xdist_group_markers_have_teardown_method>
            🟢 GREEN: Verify tests with xdist_group also have teardown_method with gc.collect().

            The 3-part memory safety pattern requires:
            1. @pytest.mark.xdist_group(name="...")
            2. teardown_method() with gc.collect()
            3. Performance tests skip parallel mode

            This test validates the second requirement.
          <Function test_integration_tests_xdist_group_coverage_percentage>
            🟢 GREEN: Verify integration test xdist_group coverage meets target.

            This test tracks progress toward 100% coverage and fails if coverage
            drops below the current baseline.

            CURRENT BASELINE: 80% (from investigation)
            TARGET: 100%
      <Module test_pytestmark_placement.py>
        Meta-test: Validate pytestmark appears after all imports.

        Prevents SyntaxErrors from pytestmark inside import blocks.

        This test was created in response to OpenAI Codex findings that identified
        16 test files with pytestmark declarations placed incorrectly inside import
        blocks, causing Python SyntaxErrors during test collection.

        Security Impact: MEDIUM
        - Test collection failures prevent security tests from running
        - Broken tests create false confidence in test suite
        - Syntax errors bypass pre-commit hooks if not detected

        Test Coverage: 100% of test files
        Compliance: TDD Meta-Test, Pre-Commit Enforcement
        <Class TestPytestmarkPlacement>
          Validate that pytestmark declarations appear AFTER all module-level imports.

          Background:
          -----------
          Python requires all import statements to appear at the top of a module
          (after docstrings and module-level comments). Placing any non-import
          statement (like pytestmark assignment) INSIDE or BETWEEN import statements
          causes a SyntaxError.

          Incorrect Placement (SyntaxError):
          ----------------------------------
          ```python
          import pytest

          from some_module import (
          pytestmark = pytest.mark.unit  # ❌ SyntaxError!

              SomeClass,
              some_function,
          )
          ```

          Correct Placement:
          ------------------
          ```python
          import pytest
          from some_module import (
              SomeClass,
              some_function,
          )

          pytestmark = pytest.mark.unit  # ✅ After all imports
          ```

          Detection Method:
          -----------------
          Uses AST (Abstract Syntax Tree) parsing to:
          1. Find pytestmark assignment line number
          2. Find last module-level import line number
          3. Assert pytestmark appears AFTER last import

          This approach is more reliable than regex because it understands
          Python's syntax structure and correctly identifies module-level
          vs nested imports.

          References:
          -----------
          - OpenAI Codex Regression Report (2025-11-20): 16 files with this issue
          - scripts/fix_missing_pytestmarks.py:108-129 - Correct placement logic
          - ADR-0XXX: Pytestmark Placement Validation
          <Function test_pytestmark_appears_after_all_imports>
            🔴 RED PHASE: Test that pytestmark appears AFTER all imports.

            This test will FAIL initially (RED) because 16 files have
            pytestmark inside import blocks. After fixing those files,
            the test will PASS (GREEN).

            Validation Logic:
            -----------------
            For each test file:
            1. Parse file with AST to get syntax tree
            2. Find pytestmark assignment (if present)
            3. Find last module-level import
            4. Assert pytestmark_line > last_import_line

            Scope:
            ------
            - Scans: tests/**/*.py (all Python files)
            - Excludes: __init__.py, conftest.py (no pytestmark required)
            - Reports: All violations with file:line format

            Expected Initial State: ❌ FAIL (16 violations)
            Expected After Fix: ✅ PASS (0 violations)
        <Class TestPytestmarkPlacementEdgeCases>
          Test edge cases for pytestmark placement validation.
          <Function test_files_without_imports_are_valid>
            Files without any imports can have pytestmark anywhere.

            This is an edge case - if a test file has no imports (unusual
            but technically valid), pytestmark can appear anywhere since
            there are no import constraints.
          <Function test_files_without_pytestmark_are_valid>
            Files without pytestmark don't trigger validation.

            This test ensures that test files without module-level pytestmark
            (but with class/function-level markers) don't cause false positives.
          <Function test_correct_placement_after_imports_is_valid>
            Correctly placed pytestmark (after imports) passes validation.

            This is the GOLDEN PATH - pytestmark appears after all imports.
      <Module test_pytestmark_syntax_regression.py>
        Regression test for pytestmark placement bug (commit a57fcc95).

        Validates that pytestmark declarations are never placed inside import blocks,
        which causes SyntaxError and prevents test collection.

        This test prevents recurrence of the bug where an automation script incorrectly
        inserted `pytestmark = pytest.mark.<marker>` inside multi-line import statements,
        affecting 16 test files and blocking integration test execution.

        Test ID: TEST-META-PYTESTMARK-001
        <Class TestPytestmarkPlacementRegression>
          Prevent pytestmark from being placed inside import blocks.

          Regression prevention for commit a57fcc95 (2025-11-20).
          <Function test_pytestmark_not_inside_imports>
            Test that pytestmark is never placed inside import parentheses.

            Scans all test files to ensure pytestmark declarations appear AFTER
            import statements, not inside multi-line import blocks.

            This test would have caught the bug in commit a57fcc95 where
            pytestmark was incorrectly inserted inside import parentheses.

            Test ID: TEST-META-PYTESTMARK-001-01
          <Function test_all_test_files_have_valid_syntax>
            Test that all test files can be parsed without SyntaxError.

            This is a broader check that would catch the pytestmark-inside-import
            bug as well as other syntax issues.

            Test ID: TEST-META-PYTESTMARK-001-02
          <Function test_pytestmark_appears_after_last_import>
            Test that pytestmark appears after the last import statement.

            Validates the correct pattern where pytestmark is placed at module level,
            after all imports, before any code.

            Test ID: TEST-META-PYTESTMARK-001-03
        <Class TestPytestmarkDocumentation>
          Validate pytestmark placement guidelines are documented.

          Test ID: TEST-META-PYTESTMARK-002
          <Function test_pytestmark_guidelines_exist>
            Test that PYTESTMARK_GUIDELINES.md exists with placement rules.

            Test ID: TEST-META-PYTESTMARK-002-01
        <Class TestAutomationScriptFix>
          Validate the automation script bug is fixed.

          Test ID: TEST-META-PYTESTMARK-003
          <Function test_fix_missing_pytestmarks_uses_end_lineno>
            Test that fix_missing_pytestmarks.py uses end_lineno correctly.

            Validates that the script has been fixed to use node.end_lineno instead
            of node.lineno when determining where to insert pytestmark.

            Test ID: TEST-META-PYTESTMARK-003-01
          <Function test_automation_script_has_unit_tests>
            Test that the automation script has comprehensive unit tests.

            Test ID: TEST-META-PYTESTMARK-003-02
          <Function test_pre_commit_hooks_validate_syntax>
            Test that pre-commit hooks validate Python syntax.

            Test ID: TEST-META-PYTESTMARK-003-03
      <Module test_regression_prevention.py>
        Regression Tests for CI/CD and Testing Infrastructure

        This test suite prevents regression of critical issues that caused workflow failures:
        1. Missing pytest fixture decorators
        2. Invalid class-scoped fixtures
        3. Settings singleton not being reloaded after monkeypatch
        4. Use of archived/unmaintained tools in workflows

        Following TDD principles, these tests will FAIL if regressions are introduced.
        <Class TestPytestFixtureValidation>
          Validate that all pytest fixtures are properly decorated
          <Function test_all_yield_functions_have_fixture_decorator>
            REGRESSION TEST: Ensure all functions with yield have @pytest.fixture decorator.

            Prevents regression of Issue #6 (Quality Test Fixture Issues):
            - tests/test_health_check.py:11 was missing @pytest.fixture
          <Function test_no_class_scoped_fixtures>
            REGRESSION TEST: Ensure no @pytest.fixture(scope="class") on class definitions.

            Prevents regression of Issue #6 (Quality Test Fixture Issues):
            - tests/test_rate_limiter.py:20 had invalid class-scoped fixture

            Pytest does not support fixtures on classes, only on functions.
        <Class TestMonkeypatchReloadPattern>
          Validate that tests using monkeypatch properly reload Settings
          <Function test_monkeypatch_tests_reload_config_module>
            REGRESSION TEST: Ensure tests using monkeypatch.setenv() reload config modules.

            Prevents regression of Issue #7 (Smoke Test Environment Configuration):
            - Tests were setting env vars but Settings singleton wasn't reloaded
            - Fixed by adding importlib.reload(config_module) pattern
        <Class TestWorkflowToolMaintenance>
          Validate that GitHub Actions workflows use maintained tools
          <Function test_no_archived_tools_in_workflows>
            REGRESSION TEST: Ensure workflows don't use archived/unmaintained tools.

            Prevents regression of Issue #8 (Deployment Workflow kubeval Limitations):
            - kubeval is archived and doesn't support K8s 1.22+
            - Replaced with kubeconform (actively maintained)
          <Function test_workflows_use_kubeconform_with_ignore_missing_schemas>
            REGRESSION TEST: Ensure kubeconform uses -ignore-missing-schemas flag.

            Prevents regression: kubeconform needs -ignore-missing-schemas for CRDs
            to avoid false positive validation failures.
        <Class TestWorkflowActionVersions>
          Validate that GitHub Actions use pinned, non-deprecated versions
          <Function test_astral_sh_setup_uv_uses_v7_or_later>
            REGRESSION TEST: Ensure astral-sh/setup-uv uses v7.1.1 or later.

            Prevents regression: Earlier versions had bugs and deprecated parameters.
            Validated in commit ae71fb3 (Comprehensive GitHub Actions validation and fixes).
        <Class TestWorkflowSyntaxValidation>
          Validate GitHub Actions workflow syntax
          <Function test_all_workflows_are_valid_yaml>
            REGRESSION TEST: Ensure all workflow files are valid YAML.

            Prevents syntax errors that break CI/CD pipelines.
          <Function test_all_workflows_have_required_structure>
            REGRESSION TEST: Ensure all workflows have required fields.

            Required fields:
            - name: Workflow name
            - on: Trigger events
            - jobs: At least one job
      <Module test_report_freshness.py>
        Test that test infrastructure reports are fresh (< 7 days old).

        This meta-test ensures that automated reports are regularly regenerated
        and not stale.

        Reports are generated by:
        - make generate-reports (manual)
        - .github/workflows/weekly-reports.yaml (automated, every Sunday)
        <Class TestReportFreshness>
          Validate that test infrastructure reports are fresh.
          <Function test_async_mock_scan_report_is_fresh>
            GIVEN: AsyncMock scan report exists
            WHEN: Checking file modification time
            THEN: Should be < 7 days old
          <Function test_memory_safety_scan_report_is_fresh>
            GIVEN: Memory safety scan report exists
            WHEN: Checking file modification time
            THEN: Should be < 7 days old
          <Function test_test_suite_stats_report_is_fresh>
            GIVEN: Test suite statistics report exists
            WHEN: Checking file modification time
            THEN: Should be < 7 days old
          <Function test_all_reports_exist>
            GIVEN: Reports automation is configured
            WHEN: Checking for expected reports
            THEN: All reports should exist
          <Function test_reports_directory_exists>
            GIVEN: Reports automation is configured
            WHEN: Checking for reports directory
            THEN: Directory should exist
          <Function test_weekly_reports_workflow_exists>
            GIVEN: Automated report generation is configured
            WHEN: Checking for GitHub Actions workflow
            THEN: weekly-reports.yaml should exist
          <Function test_generate_reports_makefile_target_exists>
            GIVEN: Reports automation is configured
            WHEN: Checking Makefile for generate-reports target
            THEN: Target should exist
      <Module test_scripts_governance.py>
        Meta-tests for scripts governance and validation.

        These tests validate that scripts in scripts/ directory are properly governed,
        documented, and validated to prevent ungoverned automation sprawl.

        PURPOSE:
        --------
        Ensure all scripts (especially validators) are documented, tested, and governed
        to prevent the situation where validator scripts themselves lack validation.

        VALIDATION:
        -----------
        1. All scripts in scripts/ are documented
        2. Validator scripts have meta-tests
        3. scripts/README.md or REGISTRY.md catalogs all scripts
        4. Shell scripts follow best practices (shebang, set -euo pipefail)

        References:
        - OpenAI Codex Finding #5: Scripts ungoverned and undocumented
        - scripts/: 114 automation scripts
        - scripts/README.md: Currently only documents 3 deployment scripts
        <Class TestScriptsGovernance>
          Validate that scripts are properly governed and documented.

          These meta-tests ensure scripts follow best practices.
          <Function test_scripts_directory_exists>
            🟢 GREEN: Verify scripts/ directory exists.

            The scripts directory contains automation and validation scripts.
          <Function test_scripts_readme_exists>
            🟢 GREEN: Verify scripts/README.md exists.

            README should document script organization and usage.
          <Function test_validator_scripts_exist>
            🟢 GREEN: Verify validator scripts exist.

            These scripts validate code quality and should be governed.
          <Function test_shell_scripts_have_shebang>
            🟢 GREEN: Verify shell scripts have proper shebang.

            All .sh scripts should start with #!/usr/bin/env bash or #!/bin/bash
          <Function test_python_validator_scripts_are_executable>
            🟢 GREEN: Verify Python validator scripts are executable.

            Validator scripts should be chmod +x for easy invocation.
          <Function test_python_validator_scripts_have_docstrings>
            🟢 GREEN: Verify Python validator scripts have module docstrings.

            Scripts should document their purpose.
        <Class TestScriptsRegistry>
          Validate that scripts registry exists and is maintained.
          <Function test_scripts_registry_exists>
            🟢 GREEN: Verify scripts/REGISTRY.md exists.

            REGISTRY.md should catalog all scripts by category.

            This test will FAIL initially (part of TDD approach) and pass
            after we create the registry in Phase 6.
          <Function test_registry_documents_validators>
            🟢 GREEN: Verify REGISTRY.md documents validator scripts.

            Registry should have a 'Validators' section.
          <Function test_registry_documents_deployment_scripts>
            🟢 GREEN: Verify REGISTRY.md documents deployment scripts.

            Registry should have a 'Deployment' section.
        <Class TestScriptsEnforcement>
          Document enforcement strategy for scripts governance.
          <Function test_enforcement_strategy_documentation>
            📚 Document the enforcement strategy for scripts governance.

            Multiple layers ensure scripts are governed and validated.
      <Module test_slash_commands.py>
        Test suite for slash command validation.

        Validates that slash commands in .claude/commands/ are properly structured:
        - Commands have descriptions
        - Commands use valid markdown syntax
        - No duplicate command names
        - Commands reference existing make targets
        - New recommended commands exist

        Following TDD: These tests are written FIRST, before new commands exist.
        They will FAIL initially (RED phase), then PASS after implementation (GREEN phase).

        Regression prevention for Anthropic Claude Code slash commands best practices.
        See: https://www.anthropic.com/engineering/claude-code-best-practices
        <Class TestSlashCommands>
          Validate slash commands are properly structured.
          <Function test_all_commands_have_heading>
            Test that all slash commands start with a markdown heading.
          <Function test_all_commands_have_substantial_content>
            Test that all commands have substantial content (> 100 chars).
          <Function test_no_duplicate_command_names>
            Test that there are no duplicate command names.
          <Function test_make_targets_referenced_exist>
            Test that make targets referenced in commands actually exist.
          <Function test_commands_use_valid_bash_syntax>
            Test that bash code blocks in commands have valid syntax.
          <Function test_new_recommended_commands_exist>
            Test that new recommended commands exist.
          <Function test_commands_have_consistent_format>
            Test that commands follow a consistent format.
          <Function test_tdd_command_references_red_green_refactor>
            Test that tdd.md command explains RED-GREEN-REFACTOR cycle.
          <Function test_verify_tests_command_runs_all_test_types>
            Test that verify-tests.md runs all test types.
          <Function test_commands_avoid_dangerous_operations>
            Test that commands don't include dangerous operations.
        <Class TestCommandDocumentation>
          Validate command documentation quality.
          <Function test_claude_md_documents_slash_commands>
            Test that CLAUDE.md documents the slash command system.
      <Module test_sleep_duration_linter.py>
        Tests for sleep duration linter (prevents test performance regressions).

        TDD: These tests are written FIRST before implementing the linter.
        <Class TestSleepDurationLinter>
          Test sleep duration linter functionality.
          <Function test_linter_detects_long_sleep_in_unit_test>
            Linter should detect time.sleep() > 0.5s in unit tests.
          <Function test_linter_allows_short_sleep_in_unit_test>
            Linter should allow time.sleep() <= 0.5s in unit tests.
          <Function test_linter_detects_long_sleep_in_integration_test>
            Linter should detect time.sleep() > 2.0s in integration tests.
          <Function test_linter_allows_long_sleep_in_integration_test>
            Linter should allow time.sleep() <= 2.0s in integration tests.
          <Function test_linter_detects_asyncio_sleep>
            Linter should detect asyncio.sleep() violations.
          <Function test_linter_detects_multiple_violations>
            Linter should detect multiple violations in one file.
          <Function test_linter_ignores_comments>
            Linter should ignore sleep calls in comments.
          <Function test_linter_reports_line_numbers>
            Linter should report accurate line numbers.
          <Function test_linter_checks_actual_test_files>
            Linter should successfully check real test files.
      <Module test_slow_test_detection.py>
        Slow Test Detection - Identify Individual Slow Tests

        Validates that individual unit tests complete quickly (< 10 seconds).

        This meta-test prevents performance regression by detecting:
        - Individual unit tests > 10 seconds
        - Common slow test patterns

        Performance guidelines:
        - Unit tests: < 1s ✅ IDEAL
        - Integration tests: < 5s ✅ ACCEPTABLE
        - E2E tests: < 30s ✅ ACCEPTABLE
        - Unit tests > 10s: ❌ NEEDS OPTIMIZATION

        Why this matters:
        - Unit tests should be fast for TDD workflow
        - Slow tests indicate integration/E2E behavior in unit tests
        - Each slow test compounds total suite time

        Common causes of slow tests:
        1. **Circuit breaker timeouts** (30-60s) → Use fast_resilience_config
        2. **Retry delays** (3 attempts × retries) → Mock retry logic
        3. **Real LangGraph execution** (14-29s) → Mock graph components
        4. **Sleep/time.sleep()** → Use freezegun
        5. **Real I/O** (network, disk, DB) → Use mocks
        6. **Large datasets** → Use smaller test data

        How to fix:
        1. Identify: pytest --durations=20
        2. Analyze: Look at test implementation
        3. Optimize: Apply appropriate technique above

        Related:
        - tests/meta/test_performance_regression_suite.py - Suite-level performance
        - tests/conftest.py:fast_resilience_config - Fast CB configuration
        <Function test_no_slow_unit_tests>
          Test that no individual unit test takes > 10 seconds.

          This catches performance regressions at the individual test level.

          Known slow tests (to be optimized in future):
          - OpenFGA circuit breaker tests: 45s (retry logic optimization needed)
          - Agent tests: 14-29s (LangGraph mocking needed)
          - Retry timing test: 14s (freezegun needed)

          These are documented and tracked. New slow tests should not be added.
      <Module test_subprocess_safety.py>
        Test subprocess safety enforcement in test files.

        This module validates that all subprocess.run() calls in test files include
        timeout parameters to prevent test hangs on CI runners.

        Related: OpenAI Codex Finding #6 - Subprocess test safeguards
        <Class TestSubprocessSafety>
          Validate subprocess calls in tests have appropriate safeguards.
          <Function test_subprocess_calls_have_timeout>
            Verify all subprocess.run() calls include timeout parameter.

            Long-running subprocess calls without timeout can hang test suites,
            especially on CI runners. All subprocess calls should have explicit
            timeout parameter.
          <Function test_subprocess_timeout_values_are_reasonable>
            Verify subprocess timeout values are >= 30 seconds.

            Timeouts that are too short can cause flaky tests on slow CI runners.
            Minimum recommended timeout is 30 seconds for CLI tools like kubectl,
            helm, kustomize.
          <Function test_subprocess_helper_function_exists>
            Verify subprocess helper function exists for consistent usage.
          <Function test_subprocess_helper_has_run_cli_tool_function>
            Verify subprocess helper exports run_cli_tool() function.
          <Function test_subprocess_helper_run_cli_tool_has_timeout_default>
            Verify run_cli_tool() has reasonable timeout default.
      <Module test_suite_validation.py>
        Meta-tests to validate test suite structure and prevent regressions

        TDD Regression Tests: These meta-tests validate the test suite itself
        to prevent recurrence of structural issues identified by OpenAI Codex.

        Tests cover:
        1. No conflicting pytest markers (unit + integration on same class)
        2. Optional imports use proper guards (pytest.importorskip or try/except)
        3. Infrastructure fixtures use pytest.skip, not pytest.fail

        These tests ensure the test suite remains maintainable and follows best practices.

        References:
        - OpenAI Codex findings: Conflicting markers, unguarded imports, pytest.fail usage
        - pytest best practices
        <Class TestMarkerConsistency>
          Meta-tests to validate pytest marker consistency across test suite
          <Function test_no_conflicting_unit_and_integration_markers>
            TDD REGRESSION TEST: Ensure no test class has both unit and integration markers

            GIVEN: All test files in the test suite
            WHEN: Scanning for pytest markers on test classes
            THEN: No test class should have both 'unit' and 'integration' markers
          <Function test_unimplemented_features_use_xfail_strict>
            TDD REGRESSION TEST: Ensure unimplemented features use xfail(strict=True) not skip

            GIVEN: All test files in the test suite
            WHEN: Scanning for tests with "not implemented" in skip/xfail reasons
            THEN: They should use @pytest.mark.xfail(strict=True), not @pytest.mark.skip

            Rationale:
            - skip: Test is silently skipped, no notification when implementation is ready
            - xfail(strict=True): Test FAILS CI when it starts passing, alerting team to remove marker
          <Function test_integration_tests_properly_marked>
            TDD REGRESSION TEST: Ensure integration tests are consistently marked

            GIVEN: All test files
            WHEN: Scanning for integration test patterns (infrastructure usage)
            THEN: Tests with real infrastructure should have @pytest.mark.integration

            Infrastructure patterns include:
            - test_infrastructure fixture usage
            - Real database/Redis/OpenFGA client fixtures
            - Docker/Keycloak/MCP references in test names or function parameters
          <Function test_integration_tests_use_conditional_skips_not_hard_skips>
            TDD REGRESSION TEST: Integration tests should use conditional skips, not hard skips

            GIVEN: All test files marked with @pytest.mark.integration
            WHEN: Scanning for @pytest.mark.skip decorators
            THEN: Integration tests should use skipif with availability checks
            OR: Use fixtures that auto-skip when infrastructure unavailable

            Rationale:
            - Hard skip: Test never runs, even when infrastructure is available
            - Conditional skip: Test runs when infrastructure is available (CI, local dev)
            - This enables tests to run in environments where infrastructure exists

            Codex Finding: "Several integration tests are permanently skipped"
        <Class TestImportGuards>
          Meta-tests to validate import guards for optional dependencies
          <Function test_optional_imports_use_guards>
            TDD REGRESSION TEST: Ensure optional imports use pytest.importorskip or try/except

            GIVEN: All test files
            WHEN: Scanning for imports of optional packages
            THEN: They should use pytest.importorskip or try/except ImportError
        <Class TestInfrastructureFixtures>
          Meta-tests to validate infrastructure fixture behavior
          <Function test_infrastructure_fixtures_use_skip_not_fail>
            TDD REGRESSION TEST: Ensure infrastructure fixtures use pytest.skip, not pytest.fail

            GIVEN: conftest.py with infrastructure fixtures
            WHEN: Health checks fail
            THEN: Should use pytest.skip(), not pytest.fail()
        <Class TestCLIToolGuards>
          Meta-tests to validate CLI tool availability guards (OpenAI Codex Finding #1)
          <Function test_cli_tool_tests_have_skipif_guards>
            TDD REGRESSION TEST: Ensure tests using CLI tools have proper skipif guards.

            CODEX FINDING #1: tests/test_ci_cd_validation.py:61 invokes kustomize
            unconditionally, causing hard failures when CLI is absent.

            GIVEN: All test files that invoke external CLI tools
            WHEN: Scanning for subprocess.run() calls to CLI tools
            THEN: Tests should use session-scoped availability fixtures and skipif guards

            CLI tools to guard:
            - kustomize
            - kubectl
            - helm
            - docker
      <Module test_test_utilities.py>
        Test suite for shared test utilities.

        This module validates the behavior of shared test utilities used across the test suite,
        including CLI tool availability decorators and settings isolation fixtures.

        Following TDD principles - these tests are written FIRST, before implementation.
        <Class TestRequiresToolDecorator>
          Test the @requires_tool decorator for CLI tool availability checking.
          <Function test_requires_tool_skips_when_tool_missing>
            Verify decorator skips test when CLI tool is not available.
          <Function test_requires_tool_passes_when_tool_available>
            Verify decorator allows test to run when CLI tool is available.
          <Function test_requires_tool_custom_message>
            Verify decorator uses custom skip message if provided.
        <Class TestSettingsIsolationFixture>
          Test the settings_isolation fixture for state mutation isolation.
          <Function test_settings_isolation_restores_original_state>
            Verify fixture restores settings to original state after test.
          <Function test_settings_isolation_handles_multiple_attributes>
            Verify fixture can isolate multiple settings attributes.
        <Class TestCLIAvailabilityFixtures>
          Test CLI tool availability fixtures.
          <Function test_kustomize_available_returns_false_when_missing>
            Verify kustomize_available fixture returns False when kustomize not installed.
          <Function test_kubectl_available_returns_value>
            Verify kubectl_available fixture returns a boolean value.
          <Function test_terraform_available_returns_value>
            Verify terraform_available fixture returns a boolean value.
          <Function test_helm_available_returns_value>
            Verify helm_available fixture returns a boolean value.
          <Function test_docker_compose_available_checks_functionality>
            Verify docker_compose_available fixture checks docker compose functionality.
          <Function test_cli_fixtures_are_booleans>
            Verify all CLI fixtures return boolean values.
        <Class TestFixtureScoping>
          Test that fixtures use appropriate scopes for performance.
          <Function test_cli_fixtures_are_session_scoped>
            Verify CLI availability fixtures are session-scoped for performance.
        <Class TestUtilityDocumentation>
          Test that utilities are properly documented.
          <Function test_requires_tool_has_docstring>
            Verify @requires_tool decorator has comprehensive docstring.
          <Function test_settings_isolation_has_docstring>
            Verify settings_isolation fixture has comprehensive docstring.
          <Function test_cli_fixtures_have_docstrings>
            Verify all CLI availability fixtures have docstrings.
      <Module test_trivy_scans_rendered_manifests.py>
        Test that Trivy security scanning uses rendered manifests instead of raw patches.

        This test validates the fix for the Codex finding where Trivy was scanning
        raw Kustomize overlay patches directly, producing false positives because
        security contexts from base manifests were not visible.

        The correct approach is to:
        1. Render the complete manifest using `kubectl kustomize`
        2. Scan the rendered output with Trivy
        3. Ensure security contexts are properly evaluated in context

        Reference: Codex finding - Deploy to GKE Staging (run 19250053057) failures
        <Function test_deploy_staging_gke_workflow_renders_manifests_before_trivy_scan>
          Verify that the deploy-staging-gke workflow renders Kustomize manifests
          before running Trivy security scans.

          This prevents false positives from scanning incomplete patch files.

          Expected workflow structure:
          1. Step: Render Kustomize manifests
             - Run: kubectl kustomize deployments/overlays/staging-gke > /tmp/staging-manifests.yaml
          2. Step: Security scan rendered manifests
             - scan-ref: /tmp/staging-manifests.yaml (NOT the overlay directory)
        <Function test_trivy_scan_allows_documented_suppressions>
          Verify that Trivy scanning policy allows documented suppressions for false positives.

          Per user requirement: "Suppress with .trivyignore (Recommended)"

          Policy:
          - Environment-specific .trivyignore files ARE allowed (staging-gke, production-gke, etc.)
          - Global .trivyignore files are NOT allowed (too broad, could hide real issues)
          - All suppressions must be documented (validated by test_trivy_suppressions.py)
        <Function test_rendered_manifests_include_security_contexts>
          Validate that rendered Kustomize manifests contain proper security contexts.

          This is a smoke test to ensure that when we render the staging overlay,
          the security contexts from both base and patches are properly merged.

          Expected security contexts (from qdrant-patch.yaml):
          - readOnlyRootFilesystem: true
          - runAsNonRoot: true
          - capabilities drop: [ALL]
        <Function test_workflow_step_order_is_correct>
          Verify that the Kustomize render step comes BEFORE the Trivy scan step.

          This ensures the manifest is rendered before it's scanned.
      <Module test_trivy_suppressions.py>
        Test Trivy security scan suppression configuration.

        This test validates the configuration for suppressing false positive Trivy findings:
        - AVD-KSV-0109 (HIGH): ConfigMap stores secrets in keys "dynamic_context_max_tokens", "model_max_tokens"

        These are NOT secrets - they are numeric configuration values for LLM token limits.
        Trivy's heuristic incorrectly flags any key containing "token" as sensitive data.

        The suppression should be documented and scoped to avoid hiding real security issues.

        Reference: Deploy to GKE Staging workflow Trivy false positives
        <Function test_trivyignore_exists_for_staging>
          Verify that .trivyignore file exists in the staging-gke overlay directory.

          This file should contain documented suppressions for false positive findings.
          Per user requirement: Suppress with .trivyignore (recommended approach).
        <Function test_trivyignore_documents_configmap_false_positive>
          Verify that .trivyignore contains proper documentation for ConfigMap suppression.

          The suppression for AVD-KSV-0109 must be clearly documented to explain:
          1. Why the finding is a false positive
          2. What the flagged values actually represent
          3. Why it's safe to suppress

          This prevents confusion and ensures future maintainers understand the decision.
        <Function test_trivyignore_workflow_integration>
          Verify that the deploy-staging-gke workflow is configured to use .trivyignore.

          The Trivy action should reference the trivyignores file so suppressions are applied.
      <Module test_validation_consolidation.py>
        Meta-tests for validation workflow consolidation.

        These tests validate that pre-commit framework is the canonical source for
        validation, eliminating duplication between Makefile targets, pre-commit hooks,
        and CI workflows.

        PURPOSE:
        --------
        Prevent pre-push validation duplication where developers run the same checks
        multiple times through different entry points (Makefile, pre-commit, CI).

        VALIDATION:
        -----------
        1. Pre-commit config is canonical source for validation hooks
        2. Makefile targets are minimal (quick checks only)
        3. CI workflows match pre-commit configuration
        4. No duplication of validation logic

        References:
        - OpenAI Codex Finding #2: Pre-push validation duplication
        - .pre-commit-config.yaml: Canonical validation configuration
        - Makefile: Manual validation targets (Tier 2 & 3)
        <Class TestValidationConsolidation>
          Validate that validation workflows are consolidated and not duplicated.

          These meta-tests ensure pre-commit framework is the single source of truth.
          <Function test_precommit_config_exists>
            🟢 GREEN: Verify .pre-commit-config.yaml exists.

            This is the canonical source for validation hooks.
          <Function test_precommit_has_required_test_hooks>
            🟢 GREEN: Verify pre-commit has required test hooks.

            Pre-commit uses consolidated `run-pre-push-tests` hook which runs:
            - Unit tests
            - Smoke tests
            - Integration tests (when CI_PARITY=1)
            - API tests
            - Property tests
            - MCP server tests

            This is a consolidated approach instead of separate hooks per test type.
          <Function test_precommit_hooks_run_on_push_stage>
            🟢 GREEN: Verify test hooks run on pre-push stage.

            Comprehensive tests should run before push, not on commit.
          <Function test_makefile_validate_pre_push_exists_and_documented>
            🟢 GREEN: Verify Makefile validate-pre-push target exists and is documented.

            The validate-pre-push target provides manual CI-equivalent validation.
            It complements pre-commit hooks by allowing developers to manually run
            the same comprehensive validation that CI runs.

            NOTE: This is NOT deprecated - it serves as manual validation option
            alongside automatic pre-commit hooks.
          <Function test_validate_push_is_tier_2_quick_checks>
            🟢 GREEN: Verify validate-push is Tier 2 (quick checks only).

            validate-push should be lightweight for quick manual validation.
          <Function test_validate_full_is_tier_3_comprehensive>
            🟢 GREEN: Verify validate-full is Tier 3 (comprehensive checks).

            validate-full should run everything for manual comprehensive validation.
          <Function test_ci_workflows_use_precommit_run>
            🟢 GREEN: Verify CI workflows use pre-commit run for validation.

            CI should use 'pre-commit run' to match local pre-commit behavior.
        <Class TestValidationDocumentation>
          Validate that validation workflows are properly documented.
          <Function test_testing_md_documents_validation_tiers>
            🟢 GREEN: Verify TESTING.md documents validation tiers.

            Documentation should explain when to use each validation approach.
          <Function test_makefile_has_validation_tier_comments>
            🟢 GREEN: Verify Makefile documents validation tiers.

            Makefile should have clear comments explaining each tier.
        <Class TestValidationEnforcement>
          Document enforcement strategy for validation consolidation.
          <Function test_enforcement_strategy_documentation>
            📚 Document the enforcement strategy for validation consolidation.

            Multiple layers ensure validation is efficient and not duplicated.
      <Module test_validator_consistency.py>
        Validator Consistency Integration Tests.

        This meta-test ensures that validation scripts produce identical results to the
        shared validation library. This guarantees consistency between:
        - Pre-commit hook script execution
        - Direct library function calls
        - Meta-test invocations

        CRITICAL: These tests catch divergence between script wrappers and library implementation.

        Following TDD: These tests validate the architecture we built in 3 phases.
        <Class TestValidatorConsistency>
          Ensure scripts and library produce identical results.
          <Function test_memory_safety_script_matches_library_on_violation>
            Test that memory safety script and library produce identical results for violations.
          <Function test_memory_safety_script_matches_library_on_valid>
            Test that memory safety script and library agree on valid files.
          <Function test_test_ids_script_matches_library_on_violation>
            Test that test IDs script and library produce identical results for violations.
          <Function test_test_ids_script_matches_library_on_valid>
            Test that test IDs script and library agree on valid files.
          <Function test_async_mock_config_script_matches_library_on_violation>
            Test that AsyncMock config script and library produce identical results for violations.
          <Function test_async_mock_config_script_matches_library_on_valid>
            Test that AsyncMock config script and library agree on valid files.
          <Function test_async_mock_usage_script_matches_library_on_violation>
            Test that AsyncMock usage script and library produce identical results for violations.
          <Function test_async_mock_usage_script_matches_library_on_valid>
            Test that AsyncMock usage script and library agree on valid files.
        <Class TestValidationLibraryVersion>
          Test that validation library has proper versioning.
          <Function test_validation_lib_has_version>
            Test that validation_lib package has __version__.
          <Function test_validation_lib_version_is_semantic>
            Test that validation_lib version follows semantic versioning.
        <Class TestValidationLibraryExports>
          Test that validation library properly exports modules.
          <Function test_can_import_memory_safety>
            Test that memory_safety can be imported from validation_lib.
          <Function test_can_import_test_ids>
            Test that test_ids can be imported from validation_lib.
          <Function test_can_import_async_mocks>
            Test that async_mocks can be imported from validation_lib.
      <Module test_xfail_strict_enforcement.py>
        Meta-test to enforce strict xfail usage in test suite.

        This test validates that all @pytest.mark.xfail decorators use strict=True
        to prevent regressions where expected failures silently start passing.

        RATIONALE (OpenAI Codex Finding #3):
        - Non-strict xfail tests (strict=False or omitted) fail silently when fixed
        - This can hide improvements or mask test logic errors
        - strict=True ensures we're notified when expected failures start passing
        - 3 tests initially violated this rule (finding #3)

        VALIDATION CRITERIA:
        - All @pytest.mark.xfail decorators must include strict=True
        - Files can opt-out via XFAIL_STRICT_EXEMPT_FILES (with justification)
        - Provides clear error messages with file paths and line numbers

        References:
        - pytest docs: https://docs.pytest.org/en/stable/how-to/skipping.html#strict-parameter
        - OpenAI Codex validation report (2025-11-15)
        - tests/test_feature_flags.py:197 (fixed example)
        - tests/integration/test_gdpr_endpoints.py:360 (fixed example)
        - tests/integration/execution/test_docker_sandbox.py:611 (fixed example)
        <Class TestXfailStrictEnforcement>
          Enforce strict=True on all xfail markers.
          <Function test_all_xfail_markers_use_strict_true>
            RED PHASE TEST: Validates all xfail markers use strict=True.

            This test will FAIL initially if any non-strict xfails exist,
            proving it works. After fixing all violations, it will pass.

            WHY strict=True MATTERS:
            - Without strict=True, xfail tests that start passing are silently ignored
            - This can hide bug fixes or improvements
            - strict=True makes pytest fail when an expected failure passes
            - Forces developers to remove xfail when test is fixed

            EXPECTED BEHAVIOR:
            - @pytest.mark.xfail(strict=True, reason="...")  ✅ PASS
            - @pytest.mark.xfail(reason="...")               ❌ FAIL (missing strict)
            - @pytest.mark.xfail()                           ❌ FAIL (missing strict)
          <Function test_xfail_strict_enforcement_statistics>
            Reports statistics on xfail usage (informational, always passes).

            Provides visibility into:
            - Total xfail markers in codebase
            - How many use strict=True
            - Coverage percentage
      <Package validation>
        <Module test_ci_cd_validation.py>
          CI/CD Validation Tests

          Tests to prevent common CI/CD workflow failures and configuration issues.

          Created in response to comprehensive CI/CD audit (2025-11-07) that identified:
          - Kustomize ConfigMap collisions causing 100% staging deployment failures
          - E2E test health check timeouts causing 100% E2E test failures
          - Composite action error handling causing 70% quality test failures

          These tests ensure such issues cannot recur.
          <Class TestKustomizeConfigurations>
            Test Kustomize configurations for common issues

            CODEX FINDING #1: These tests require kustomize CLI tool.
            Tests will skip gracefully if kustomize is not installed.
            <Function test_kustomize_overlay_builds_successfully>
              Test that all Kustomize overlays build successfully.

              FINDING #1: Kustomize ConfigMap collision in staging-gke overlay
              Root Cause: configMapGenerator with behavior:create conflicted with base ConfigMap
              Fix: Changed to strategic merge patch approach
              Prevention: This test validates all overlays build successfully
            <Function test_no_configmap_collisions>
              Test that overlay ConfigMaps don't collide with base ConfigMaps.

              FINDING #1 (detailed): ConfigMap collision detection
              Pattern: Overlays using configMapGenerator with behavior:create when
                       base already has a ConfigMap with the same name
              Solution: Use behavior:replace, behavior:merge, or strategic merge patch
              Prevention: This test detects the specific pattern that caused the failure
            <Function test_overlay_patches_target_existing_resources>
              Test that overlay patches target resources that exist in base or overlay.

              FINDING: Potential issue - patches targeting non-existent resources
              Prevention: Validate patch targets exist before deployment
          <Class TestGitHubActionsWorkflows>
            Test GitHub Actions workflows for best practices and error handling
            <Function test_composite_actions_have_error_handling>
              Test that composite actions have proper error handling with set -e.

              FINDING #3: Composite action error handling
              Root Cause: Missing set -euo pipefail in shell scripts
              Fix: Added set -euo pipefail to all shell steps
              Prevention: This test validates all composite actions use set -e
            <Function test_e2e_workflow_has_adequate_timeout>
              Test that E2E workflow has adequate health check timeout.

              FINDING #2: E2E test health check timeouts
              Root Cause: 100 iterations * 3s = 5 minutes insufficient for slow GitHub runners
              Fix: Increased to 150 iterations * 3s = 7.5 minutes
              Prevention: This test validates timeout is adequate (>= 7 minutes)
            <Function test_ci_workflow_validates_lockfile>
              Test that CI workflow validates UV lockfile is up-to-date.

              PREVENTION: Lockfile drift detection
              Pattern: Ensure dependencies match lockfile for reproducible builds
              Solution: Add 'uv lock --check' validation step in CI
            <Function test_workflow_bash_steps_have_error_handling>
              Test that workflow bash steps with multiple commands have error handling.

              FINDING #3 (extended): Workflow error handling
              Prevention: Ensure critical workflow steps fail fast on errors
          <Class TestDockerComposeTestInfra>
            Test Docker Compose test infrastructure configuration
            <Function test_all_services_have_health_checks>
              Test that all services in docker-compose.test.yml have health checks.

              FINDING #2 (related): E2E test health checks
              Prevention: Ensure all test infrastructure services define health checks

              Note: Migration/initialization services (with restart:no or one-time commands)
              are excluded as they don't need health checks.
            <Function test_keycloak_has_adequate_start_period>
              Test that Keycloak service has adequate start_period in health check.

              FINDING #2 (specific): Keycloak initialization delays
              Pattern: Keycloak requires 45s start period + retries for full initialization
              Prevention: Validate Keycloak health check configuration
          <Function test_ci_cd_validation_suite_info>
            Document the purpose and scope of this test suite.

            This is an informational test that always passes but provides context.
        <Module test_deployment_manifests.py>
          Test suite for validating Kubernetes deployment manifests.

          This test suite validates deployment configurations to prevent common
          misconfigurations identified by OpenAI Codex analysis.

          Test Coverage:
          - RBAC role configurations (resourceNames with list/watch)
          - NetworkPolicy egress rules for required services
          - Kustomize overlay buildability
          - Helm chart validation and rendering
          - Health probe path consistency
          - Istio integration conditional checks
          <Class TestRBACConfiguration>
            Test RBAC role configurations for invalid patterns.
            <Function test_rbac_roles_no_list_with_resource_names>
              Test that RBAC roles don't combine list/watch verbs with resourceNames.

              Kubernetes RBAC doesn't support scoping list/watch operations by
              resourceNames. This test ensures we don't have this invalid combination.

              Critical Issue #1: deployments/base/serviceaccount-roles.yaml:12/49/85/121/157
          <Class TestNetworkPolicy>
            Test NetworkPolicy configurations for completeness.
            <Function test_network_policy_has_required_egress_rules>
              Test that NetworkPolicy includes egress rules for all required services.

              Critical Issue #2: deployments/base/networkpolicy.yaml:42
              Missing egress rules for Keycloak, Redis, PostgreSQL, Qdrant.
          <Class TestKustomizeOverlays>
            Test that Kustomize overlays can build successfully.

            CODEX FINDING #1: These tests require kustomize CLI tool.
            Tests will skip gracefully if kustomize is not installed.
            <Function test_kustomize_overlay_builds[dev]>
              Test that Kustomize overlays build without errors.

              Critical Issue #4: deployments/overlays/dev/kustomization.yaml:28
              Patches reference resources not included in base.
            <Function test_kustomize_overlay_builds[staging]>
              Test that Kustomize overlays build without errors.

              Critical Issue #4: deployments/overlays/dev/kustomization.yaml:28
              Patches reference resources not included in base.
            <Function test_kustomize_overlay_builds[production]>
              Test that Kustomize overlays build without errors.

              Critical Issue #4: deployments/overlays/dev/kustomization.yaml:28
              Patches reference resources not included in base.
            <Function test_cloud_provider_overlay_builds[aws]>
              Test that cloud provider overlays build successfully.

              Critical Issue #3: deployments/kubernetes/overlays/aws/kustomization.yaml:8
              Invalid base path (../../base should be ../../../base).
            <Function test_cloud_provider_overlay_builds[gcp]>
              Test that cloud provider overlays build successfully.

              Critical Issue #3: deployments/kubernetes/overlays/aws/kustomization.yaml:8
              Invalid base path (../../base should be ../../../base).
            <Function test_cloud_provider_overlay_builds[azure]>
              Test that cloud provider overlays build successfully.

              Critical Issue #3: deployments/kubernetes/overlays/aws/kustomization.yaml:8
              Invalid base path (../../base should be ../../../base).
          <Class TestBaseKustomization>
            Test base kustomization includes all required resources.
            <Function test_base_kustomization_includes_all_manifests>
              Test that base/kustomization.yaml includes all required manifest files.

              Critical Issue #1 (from Codex): Missing key manifests like ingress-http.yaml,
              otel-collector-deployment.yaml, limitrange.yaml, postgres-networkpolicy.yaml, etc.
          <Class TestHelmChart>
            Test Helm chart configurations.

            CODEX FINDING #1: These tests require helm CLI tool.
            Tests will skip gracefully if helm is not installed.
            <Function test_helm_chart_lints_successfully>
              Test that Helm chart passes helm lint validation.

              FIXED (2025-11-16): Helm dependency build resolved CODEX FINDING #7.
              Prometheus rules file parsing error was fixed by running helm dependency update/build.

              Note: Kustomize deployments (primary method) are unaffected.
            <Function test_helm_chart_renders_successfully>
              Test that Helm chart renders without errors.

              FIXED (2025-11-16): Helm dependency build resolved CODEX FINDING #7.
              Chart now renders successfully after dependencies were built.
              Ran 'helm dependency update' to resolve prometheus rules parsing errors.

              Depends on successful lint. See test_helm_chart_lints_successfully.
            <Function test_helm_networkpolicy_template_exists_when_enabled>
              Test that NetworkPolicy template exists when feature is enabled in values.

              High Priority Issue #7: networkPolicy.enabled: true but no template exists.
            <Function test_helm_health_probe_paths_match_app>
              Test that Helm chart health probe paths match the canonical deployment.

              Medium Priority Issue #8: Helm uses /health but base uses /health/live.

              Note: Helm chart uses .Values.healthChecks structure, not .Values.livenessProbe.
            <Function test_helm_istio_resources_have_conditional_checks>
              Test that Istio resources have conditional capability checks.

              High Priority Issue #9: Istio enabled by default but no conditional checks.
              Charts fail on clusters without Istio CRDs.
          <Class TestCloudRunManifest>
            Test Cloud Run service manifest configurations.
            <Function test_cloudrun_no_bash_variable_substitutions>
              Test that Cloud Run manifest doesn't contain unresolved bash variables.

              High Priority Issue #5: ${DOMAIN}, ${PROJECT_ID} won't expand in YAML.
          <Class TestArgoCDApplication>
            Test ArgoCD application configurations.
            <Function test_argocd_helm_values_use_valid_schema>
              Test that ArgoCD application uses valid Helm chart value paths.

              High Priority Issue #6: Using externalHost instead of external.host.
          <Class TestHealthProbeConsistency>
            Test health probe path consistency across deployments.
            <Function test_optimized_deployment_health_probes>
              Test that optimized deployment uses canonical health probe paths.

              Medium Priority Issue #11: optimized/deployment.yaml uses /health instead of /health/live.
          <Class TestDocumentation>
            Test documentation accuracy.
            <Function test_readme_uses_modern_kustomize_syntax>
              Test that README examples use modern Kustomize syntax.

              Medium Priority Issue #10: README uses deprecated 'bases:' field.
        <Module test_docker_compose_validation.py>
          Test Docker Compose configuration validation.

          This test suite validates Docker Compose files to prevent common issues:
          - YAML syntax errors
          - Health checks using commands not available in container images
          - Invalid service configurations

          Following TDD principles:
          - RED: These tests will fail on current docker-compose.test.yml (wget not in Qdrant image)
          - GREEN: Tests will pass after fixing health checks
          - REFACTOR: Prevents future regressions

          Related Codex Finding: docker-compose.test.yml:214-233 uses wget for Qdrant health check
          <Class TestDockerComposeYAMLSyntax>
            Test Docker Compose files have valid YAML syntax.
            <Function test_compose_file_is_valid_yaml[NOTSET]>
              Test that Docker Compose file is valid YAML.
            <Function test_compose_file_has_services[NOTSET]>
              Test that Docker Compose file has a services section.
          <Class TestDockerComposeHealthChecks>
            Test Docker Compose health check configurations.
            <Function test_health_checks_avoid_problematic_commands[NOTSET]>
              Test that health checks don't use commands that may not be available.

              This is a WARNING test - it flags potential issues but allows exceptions
              if the command is verified to exist in the image.
            <Function test_image_specific_health_check_requirements[NOTSET]>
              Test that specific images use their recommended health check methods.

              This is a HARD REQUIREMENT test for known image configurations.
              RED phase: Will fail on current docker-compose.test.yml
            <Function test_health_checks_have_valid_structure[NOTSET]>
              Test that health checks have valid structure.
          <Class TestDockerComposeQdrantSpecific>
            Specific tests for Qdrant service configuration.
            <Function test_qdrant_uses_valid_health_check[NOTSET]>
              Test that Qdrant services use valid health check methods.

              Background (based on official Qdrant documentation):
              - Qdrant v1.15+ removed wget/curl for security (GitHub issue #3491)
              - grpc_health_probe is NOT included in Qdrant images (never was)
              - Official recommended endpoints: /healthz, /livez, /readyz (bypass auth)
              - HTTP endpoint :6333 and gRPC endpoint :6334 both available

              Valid health check methods:
              1. TCP check: /dev/tcp/localhost/6333 (works with minimal image, no HTTP client needed)
              2. HTTP check: curl/wget to /healthz, /livez, /readyz, or / (requires external HTTP client)
              3. Kubernetes httpGet probe (for K8s manifests, not tested here)

              Invalid methods:
              - wget/curl inside container (not in v1.15.1+ image)
              - grpc_health_probe (not in image, requires k8s 1.24+)

              References:
              - Qdrant Monitoring Guide: https://qdrant.tech/documentation/guides/monitoring/
              - GitHub #3491: curl not added to image (security)
              - GitHub #4250: Healthcheck command discussion
          <Function test_all_compose_files_found>
            Sanity test: Ensure we're finding all compose files.
        <Module test_documentation_integrity.py>
          Test documentation integrity to prevent broken docs, missing files, and sync issues.

          This test suite validates:
          1. ADR synchronization between adr/ and docs/architecture/
          2. docs.json validity and referenced files exist
          3. No HTML comments in MDX files (use JSX comments instead)
          4. Mermaid diagrams are properly formatted
          5. All navigation links point to existing files
          <Class TestADRSynchronization>
            Verify ADRs are synced between source (adr/) and Mintlify docs (docs/architecture/).

            NOTE: This test calls the validation script (scripts/validators/adr_sync_validator.py)
            instead of duplicating the validation logic. The script is the source of truth.

            Architecture Pattern:
            - Script = Source of truth (scripts/validators/adr_sync_validator.py)
            - Hook = Trigger (validate-adr-sync in .pre-commit-config.yaml)
            - Meta-Test = Validator of validator (this test)
            <Function test_adr_synchronization>
              Test that ADRs are synchronized between adr/ and docs/architecture/.

              This test validates:
              1. All ADRs in adr/ have corresponding .mdx files in docs/architecture/
              2. No orphaned .mdx files exist in docs/architecture/
              3. ADR count matches between source and docs
              4. No uppercase filename violations (should be adr-*, not ADR-*)

              The validation is performed by calling scripts/validators/adr_sync_validator.py,
              which is the authoritative implementation (single source of truth).
          <Class TestDocsJsonIntegrity>
            Verify docs.json is valid and all referenced files exist.
            <Function test_docs_json_is_valid_json>
              Test that docs.json is valid JSON.
            <Function test_all_navigation_files_exist>
              Test that all files referenced in docs.json navigation exist.
          <Class TestMDXSyntax>
            Verify MDX files use correct syntax and don't have common errors.
            <Function test_no_html_comments_in_mdx_files>
              Test that MDX files don't use HTML comments (use JSX comments instead).
            <Function test_jsx_comments_are_properly_closed>
              Test that JSX comments in MDX files are properly opened and closed.
            <Function test_no_unescaped_comparison_operators>
              Test that comparison operators < and > are properly escaped in MDX.
          <Class TestArchitectureOverview>
            Verify architecture overview is up to date with actual ADR count.
            <Function test_architecture_overview_adr_count_is_current>
              Test that architecture/overview.mdx has the correct ADR count.
          <Class TestMermaidDiagrams>
            Verify Mermaid diagrams are properly formatted.
            <Function test_all_mermaid_diagrams_have_closing_markers>
              Test that all Mermaid diagram code blocks are properly closed.
          <Class TestDocumentationCompleteness>
            Verify documentation is complete and up to date.
            <Function test_no_suspiciously_small_documentation_files>
              Test that there are no suspiciously small (<15 lines) MDX files.
            <Function test_monitoring_subdirectories_have_readmes>
              Test that monitoring subdirectories have README.md files.
            <Function test_monitoring_readmes_are_comprehensive>
              Test that monitoring READMEs are comprehensive (>50 lines).
        <Module test_gitignore_validation.py>
          Test suite for validating .gitignore patterns and preventing local config commits.

          This test module ensures that personal/local configuration files are never
          accidentally committed to version control, following security and collaboration
          best practices.
          <Class TestGitignoreValidation>
            Validate .gitignore patterns and tracked files.
            <Function test_no_local_config_files_tracked>
              CRITICAL: Verify no .local. files are tracked in git.

              Files matching *.local.* pattern should never be committed as they
              contain personal preferences and machine-specific configurations.
            <Function test_no_claude_settings_local_tracked>
              CRITICAL: Verify .claude/settings.local.json is not tracked.

              According to Claude Code documentation, settings.local.json should
              never be committed to version control.
            <Function test_gitignore_has_local_patterns>
              Verify .gitignore contains patterns to exclude .local. files.
            <Function test_no_env_local_files_tracked>
              Verify no .env.local or .env.*.local files are tracked.

              These contain environment-specific secrets and should never be committed.
            <Function test_no_personal_ide_configs_tracked>
              Verify personal IDE configuration files are not tracked.

              Files like .vscode/settings.json should be local to each developer.
            <Function test_gitignore_exists_and_valid>
              Verify .gitignore file exists and is valid.
            <Function test_no_backup_files_tracked>
              Verify no backup files (*.bak, *.backup) are tracked.
          <Class TestGitignoreComprehensiveness>
            Test that .gitignore covers all necessary patterns.
            <Function test_has_python_patterns>
              Verify .gitignore has Python-specific patterns.
            <Function test_has_secret_patterns>
              Verify .gitignore excludes secret files.
        <Module test_gitleaks_config.py>
          Tests for gitleaks configuration to prevent false positives.

          Following TDD principles:
          1. RED: These tests verify gitleaks config excludes documentation examples
          2. GREEN: Create .gitleaks.toml with proper allowlists
          3. REFACTOR: Update workflows to use the config
          <Class TestGitleaksConfig>
            Test gitleaks configuration and allowlist.
            <Function test_gitleaks_config_exists>
              Test that .gitleaks.toml configuration file exists.

              This test will initially FAIL because the config doesn't exist yet.
              After fix, gitleaks should use this config to avoid false positives.
            <Function test_gitleaks_config_valid_toml>
              Test that .gitleaks.toml is valid TOML syntax.
            <Function test_gitleaks_ignores_documentation_examples>
              Test that gitleaks doesn't flag documentation examples as secrets.

              The .openai/codex-instructions.md file contains example code showing
              what NOT to do (hardcoded secrets). These should be allowlisted.
            <Function test_gitleaks_ignores_venv_directories>
              Test that gitleaks config excludes .venv and similar directories.
            <Function test_gitleaks_ignores_generated_clients>
              Test that gitleaks config excludes generated client code.
            <Function test_gitleaks_detects_real_secrets>
              Test that gitleaks still detects real secrets (not a false negative factory).

              This ensures our allowlist doesn't go too far and miss real issues.
          <Class TestGitleaksWorkflowIntegration>
            Test that GitHub workflows use gitleaks config correctly.
            <Function test_validate_kubernetes_workflow_uses_gitleaks_config>
              Test that validate-kubernetes.yaml workflow uses .gitleaks.toml.
            <Function test_gcp_compliance_workflow_uses_gitleaks_config>
              Test that gcp-compliance-scan.yaml workflow uses .gitleaks.toml.
        <Module test_kubernetes_security.py>
          Tests for Kubernetes security context configuration.

          Following TDD principles:
          1. RED: Verify all pods have readOnlyRootFilesystem security context
          2. GREEN: Add security contexts to deployment patches
          3. REFACTOR: Extract common patterns to Helm values
          <Class TestSecurityContexts>
            Test Kubernetes pod security contexts.
            <Function test_all_containers_have_readonly_filesystem>
              Test that all container security contexts have readOnlyRootFilesystem: true.

              This test will initially FAIL because security contexts are missing.
              After fix, all containers should have this critical security setting.

              **EXCEPTION: Keycloak**
              Keycloak requires readOnlyRootFilesystem: false due to Quarkus JIT compilation.
              This is documented in deployments/overlays/staging-gke/.trivyignore (AVD-KSV-0014).

              Security mitigations for Keycloak:
              - emptyDir volumes (ephemeral, isolated per pod)
              - runAsNonRoot: true, runAsUser: 1000
              - allowPrivilegeEscalation: false
              - capabilities.drop: ALL

              See: https://github.com/keycloak/keycloak/issues/10150 (upstream tracking)
            <Function test_writable_volumes_mounted_as_emptydir>
              Test that writable directories use emptyDir volumes.

              Containers with readOnlyRootFilesystem need emptyDir volumes for:
              - /tmp (temporary files)
              - /var/tmp (temp files)
              - Application-specific writable paths
            <Function test_init_containers_have_security_context>
              Test that init containers also have proper security contexts.
          <Class TestImagePullPolicy>
            Test imagePullPolicy configuration for security compliance.
            <Function test_staging_overlay_has_imagepullpolicy_always>
              Test that staging overlay sets imagePullPolicy: Always for all containers.

              TDD RED phase: This test will initially FAIL because staging patches don't override imagePullPolicy.

              Rationale:
              - imagePullPolicy: IfNotPresent (base default) can use stale cached images in staging
              - imagePullPolicy: Always ensures latest security patches and prevents supply chain attacks
              - kube-score compliance: Production and staging MUST use Always for image pull policy

              This prevents scenarios where:
              1. A node caches a vulnerable image
              2. Updated image is pushed with same tag (e.g., v2.8.0 rebuilt with security fix)
              3. Pod uses cached vulnerable version instead of pulling updated image
          <Class TestRedisExternalNameService>
            Test Redis ExternalName service configuration.
            <Function test_redis_service_configuration>
              Test Redis service configuration for AVD-KSV-0108 compliance.

              Per user preference: Investigate before deciding on approach.
              This test documents the current state and expected fixes.
          <Class TestKubernetesValidation>
            Test Kubernetes manifest validation with security tools.
            <Function test_kubeconform_validates_manifests>
              Test that manifests pass kubeconform validation.
            <Function test_kustomize_builds_successfully>
              Test that Kustomize overlays build without errors.
        <Module test_link_checker.py>
          Test suite for documentation link checker.

          Following TDD principles - tests define expected behavior.
          <Class TestInternalLinkParsing>
            Test parsing of internal links from MDX files.
            <Function test_finds_relative_links>
              Test that relative links are correctly extracted.
            <Function test_ignores_external_links>
              Test that external links are ignored.
            <Function test_finds_anchor_links>
              Test that anchor links are correctly handled.
            <Function test_handles_mdx_link_components>
              Test parsing of MDX Link components.
          <Class TestLinkResolution>
            Test resolution of internal links to actual files.
            <Function test_resolves_relative_link>
              Test that relative links resolve correctly.
            <Function test_detects_broken_link>
              Test that broken links are detected.
            <Function test_resolves_absolute_link>
              Test that absolute links (from docs root) resolve.
          <Class TestLinkValidation>
            Test complete link validation workflow.
            <Function test_validates_all_links_in_file>
              Test that all links in a file are validated.
          <Class TestRealWorldExamples>
            Test cases based on actual documentation structure.
            <Function test_adr_cross_references>
              Test that ADR cross-references are valid.
            <Function test_navigation_links_exist>
              Test that docs.json references existing files.
        <Module test_mdx_validation.py>
          Test suite for MDX validation scripts.

          This test suite follows TDD principles to ensure MDX syntax errors
          are caught and fixed correctly, preventing regressions.
          <Class TestCodeBlockClosingFixes>
            Test fixes for malformed code block closings.
            <Function test_fixes_duplicate_lang_after_closing>
              Test Pattern 1: ``` followed by ```bash on next line.
            <Function test_fixes_lang_before_code_group_closing>
              Test Pattern 2: ```bash before </CodeGroup>.
            <Function test_fixes_lang_before_mdx_tags>
              Test Pattern 3: ```bash before <Note> or other MDX tags.
            <Function test_multiple_patterns_in_one_file>
              Test fixing multiple patterns in a single file.
            <Function test_preserves_valid_code_blocks>
              Test that valid code blocks are not modified.
            <Function test_handles_empty_content>
              Test handling of empty content.
            <Function test_handles_no_code_blocks>
              Test handling of content with no code blocks.
            <Function test_all_supported_languages>
              Test that all supported languages are handled.
          <Class TestRealWorldExamples>
            Test cases based on actual errors found in the documentation.
            <Function test_api_keys_pattern>
              Test the pattern found in api-keys.mdx.
            <Function test_authentication_pattern>
              Test the pattern found in authentication.mdx.
            <Function test_response_field_pattern>
              Test pattern with ResponseField tags.
            <Function test_nested_mdx_components>
              Test code blocks inside nested MDX components.
          <Class TestEdgeCases>
            Test edge cases and potential issues.
            <Function test_code_block_with_title>
              Test that labeled code blocks are preserved.
            <Function test_inline_code_not_affected>
              Test that inline code is not affected.
            <Function test_multiline_code_block_content>
              Test code blocks with multiple lines are preserved.
          <Class TestFileOperations>
            Test file reading and writing operations.
            <Function test_can_read_and_write_file>
              Test basic file I/O.
            <Function test_dry_run_does_not_modify_file>
              Test that dry run doesn't modify files.
        <Module test_path_validation.py>
          Meta-tests for validating path calculations in test files.

          These tests ensure that all test files correctly calculate repository root paths,
          preventing FileNotFoundError and incorrect path assumptions.

          Context:
          - Tests often need to calculate repo_root to access deployments/, config files, etc.
          - Incorrect .parents[N] calculations lead to paths pointing to tests/ instead of repo root
          - Using marker files (.git, pyproject.toml) is safer than hardcoded parent counts

          Test Coverage:
          1. Keycloak deployment tests calculate correct repo_root
          2. LangGraph deployment tests calculate correct repo_root
          3. All deployment paths point to existing directories
          4. No hardcoded parent counts without marker file validation
          <Class TestRepoRootCalculations>
            Validate repo_root calculations in test files
            <Function test_keycloak_deployment_test_repo_root_is_correct>
              GIVEN tests/integration/deployment/test_keycloak_readonly_filesystem.py
              WHEN repo_root fixture calculates path
              THEN it should point to actual repository root (not tests/)
              AND deployments/base directory should exist at that location
            <Function test_langgraph_deployment_test_repo_root_is_correct>
              GIVEN tests/integration/deployment/test_langgraph_platform.py
              WHEN path to deployments/ is calculated
              THEN it should use marker file approach (not hardcoded .parents[N])
              AND it should point to actual repository deployments/ directory

              NOTE: Directory consolidation completed - test_langgraph_platform.py
              moved from deployments/ (plural) to deployment/ (singular) and
              updated to use get_repo_root() function with marker file validation.
            <Function test_all_deployment_test_paths_point_to_existing_directories>
              GIVEN all test files in tests/integration/deployment*/
              WHEN they calculate paths to deployments/
              THEN those paths should exist
            <Function test_no_hardcoded_parents_without_marker_validation>
              GIVEN any test file that uses .parents[N] to find repo root
              WHEN validating path calculation pattern
              THEN it should include marker file validation (.git, pyproject.toml)
              OR use the centralized project_root fixture from conftest.py

              This test serves as documentation of best practices, not strict enforcement.
          <Class TestPathCalculationPatterns>
            Validate path calculation patterns and best practices
            <Function test_conftest_provides_project_root_fixture>
              GIVEN tests/conftest.py
              WHEN checking for centralized path fixtures
              THEN it should provide a project_root fixture that tests can use
            <Function test_repo_root_points_to_directory_with_pyproject_toml>
              GIVEN calculated repo_root
              WHEN validating it's the correct location
              THEN pyproject.toml should exist at that path
            <Function test_repo_root_points_to_directory_with_git_folder>
              GIVEN calculated repo_root
              WHEN validating it's the correct location
              THEN .git directory or file (for worktrees) should exist
        <Module test_validate_mintlify_docs.py>
          Tests for scripts/validate_mintlify_docs.py

          Following TDD principles:
          1. RED: These tests verify the regex pattern compiles correctly
          2. GREEN: Fix the regex in validate_mintlify_docs.py:168
          3. REFACTOR: Add documentation and prevent regressions
          <Class TestMermaidArrowRegex>
            Test suite for Mermaid arrow syntax validation regex.

            The regex on line 168 should:
            1. Compile without errors (no invalid character ranges)
            2. Match invalid arrow syntax (single dash not followed by > or -)
            3. NOT match valid patterns like:
               - Number ranges: "70-80%", "1-5"
               - Valid arrows: "A-->B", "A--B"
               - HTML entities or other valid contexts
            <Function test_arrow_regex_compiles_without_error>
              Test that the arrow syntax regex compiles without re.error.

              This test will FAIL initially because [^->-\d] contains invalid range >-\d.
              After fix, pattern should be [^->\d] (no range, just character exclusion).
            <Function test_arrow_regex_matches_invalid_single_dash>
              Test regex correctly identifies invalid single-dash arrow syntax.
            <Function test_arrow_regex_ignores_valid_patterns>
              Test regex doesn't match valid patterns (number ranges, valid arrows).
            <Function test_mermaid_check_uses_correct_regex>
              Integration test: check_mermaid_diagrams uses compilable regex.
          <Class TestFrontmatterParsing>
            Test frontmatter parsing functionality.
            <Function test_parse_valid_frontmatter>
              Test parsing valid YAML frontmatter.
            <Function test_parse_missing_frontmatter>
              Test handling of content without frontmatter.
            <Function test_check_frontmatter_missing_required_fields>
              Test detection of missing required frontmatter fields.
          <Class TestFilenamConvention>
            Test filename convention validation.
            <Function test_valid_kebab_case_filenames>
              Test that valid kebab-case filenames pass validation.
            <Function test_invalid_filenames>
              Test that invalid filenames are detected.
          <Class TestInternalLinks>
            Test internal link validation.
            <Function test_skip_external_links>
              Test that external HTTP(S) links are not validated.
            <Function test_skip_source_file_references>
              Test that intentional source file references are not validated as docs links.
          <Class TestValidationReport>
            Test ValidationReport functionality.
            <Function test_add_and_count_issues>
              Test adding issues and counting by severity.
            <Function test_print_summary_returns_error_count>
              Test that print_summary returns the correct error count.
        <Module test_yaml_syntax.py>
          Tests for YAML syntax validation across the project.

          Following TDD principles:
          1. RED: These tests verify all YAML files parse correctly
          2. GREEN: Fix YAML syntax errors
          3. REFACTOR: Add pre-commit hooks to prevent regressions
          <Class TestPreCommitConfigSyntax>
            Test pre-commit configuration files have valid YAML syntax.
            <Function test_pre_commit_requirements_check_valid_yaml>
              Test that .pre-commit-config-requirements-check.yaml is valid YAML.

              This test will initially FAIL because the multiline bash script on line 20
              uses single quotes in a plain scalar, which causes parsing issues.

              After fix, should use block scalar syntax (|) for multiline content.
            <Function test_all_workflow_yaml_files_valid>
              Test that all GitHub workflow YAML files are valid.
            <Function test_block_scalars_used_for_multiline_scripts>
              Test that multiline scripts in pre-commit config are properly parsed.

              After using block scalar syntax (|), multiline bash scripts should parse
              correctly as strings with embedded newlines.
          <Class TestKubernetesYAMLSyntax>
            Test Kubernetes manifest YAML files have valid syntax.
            <Function test_helm_templates_directory_exists>
              Verify Helm templates directory exists.
            <Function test_kustomize_overlays_valid_yaml>
              Test that Kustomize overlay files are valid YAML.
          <Class TestYAMLFileEncoding>
            Test YAML files use correct encoding and no BOM.
            <Function test_yaml_files_utf8_no_bom>
              Test that YAML files are UTF-8 without BOM.
          <Class TestYAMLIndentation>
            Test YAML files follow consistent indentation.
            <Function test_yaml_uses_2_space_indentation>
              Test that YAML files use 2-space indentation (not tabs).
    <Dir middleware>
      <Module test_rate_limiter.py>
        Comprehensive tests for rate limiting middleware.

        Tests:
        - Tiered rate limits (anonymous, free, standard, premium, enterprise)
        - User tier extraction from JWT
        - Rate limit key generation (user ID > IP > global)
        - Redis-backed distributed rate limiting
        - Custom rate limit exception handler
        - Endpoint-specific rate limit decorators
        - Fail-open behavior when Redis is unavailable
        <Class TestRateLimitConstants>
          Test rate limit configuration constants
          <Function test_rate_limits_defined>
            Test all tier rate limits are defined
          <Function test_rate_limits_progressive>
            Test rate limits increase with tier
          <Function test_endpoint_rate_limits_defined>
            Test endpoint-specific rate limits
          <Function test_auth_endpoints_have_strict_limits>
            Test authentication endpoints have strict limits
        <Class TestUserIDExtraction>
          Test user ID extraction from JWT

          NOTE: get_user_id_from_jwt() relies on request.state.user being set by
          AuthMiddleware to avoid event loop issues in slowapi's synchronous context.
          <Function test_get_user_id_from_request_state>
            Test extracting user ID from request.state.user (preferred method)
          <Function test_get_user_id_from_valid_jwt>
            Test extracting user ID from request.state.user
          <Function test_get_user_id_fallback_to_user_id_claim>
            Test extracting user_id claim from request.state.user
          <Function test_get_user_id_no_auth_header>
            Test user ID extraction with no auth header
          <Function test_get_user_id_invalid_token_format>
            Test user ID extraction with invalid token format (no user in state)
          <Function test_get_user_id_invalid_jwt>
            Test user ID extraction with invalid JWT (no user in state)
        <Class TestUserTierExtraction>
          Test user tier extraction from JWT

          NOTE: get_user_tier() relies on request.state.user being set by
          AuthMiddleware to avoid event loop issues in slowapi's synchronous context.
          <Function test_get_tier_from_request_state>
            Test extracting tier from request.state.user (preferred method)
          <Function test_get_tier_from_valid_jwt>
            Test extracting tier from request.state.user
          <Function test_get_tier_fallback_to_plan_claim>
            Test fallback to 'plan' field in request.state.user
          <Function test_get_tier_defaults_to_free>
            Test tier defaults to 'free' when user has no tier/roles
          <Function test_get_tier_anonymous_no_auth>
            Test tier is 'anonymous' with no authentication
          <Function test_get_tier_invalid_tier_defaults_to_free>
            Test that invalid tier names default to 'free'
        <Class TestRateLimitKeyGeneration>
          Test rate limit key generation
          <Function test_key_prioritizes_user_id>
            Test rate limit key prioritizes user ID from request.state.user
          <Function test_key_falls_back_to_ip>
            Test rate limit key falls back to IP address
          <Function test_key_global_anonymous_fallback>
            Test rate limit key falls back to global anonymous
        <Class TestRateLimitForTier>
          Test tier-based rate limit determination
          <Function test_get_rate_limit_for_each_tier>
            Test getting rate limit for each tier
          <Function test_get_rate_limit_defaults_to_free>
            Test unknown tier defaults to free tier
        <Class TestDynamicLimitDetermination>
          Test dynamic limit determination from request
          <Function test_dynamic_limit_for_premium_user>
            Test dynamic limit for premium user
          <Function test_dynamic_limit_for_anonymous>
            Test dynamic limit for anonymous user
        <Class TestRedisStorageURI>
          Test Redis storage URI generation
          <Function test_get_redis_storage_uri_default>
            Test Redis storage URI with default settings
          <Function test_get_redis_storage_uri_format>
            Test Redis URI format
        <Class TestCustomRateLimitHandler>
          Test custom rate limit exceeded handler
          <Coroutine test_rate_limit_handler_response_structure>
            Test rate limit handler returns proper structure
          <Coroutine test_rate_limit_handler_includes_tier_info>
            Test rate limit handler includes tier information
        <Class TestSetupRateLimiting>
          Test rate limiting setup function
          <Function test_setup_adds_limiter_to_app_state>
            Test setup_rate_limiting adds limiter to app state
          <Function test_setup_registers_exception_handler>
            Test setup_rate_limiting registers exception handler
        <Class TestEndpointSpecificDecorators>
          Test endpoint-specific rate limit decorators
          <Function test_rate_limit_for_auth_decorator>
            Test auth endpoint rate limiter
          <Function test_rate_limit_for_llm_decorator>
            Test LLM endpoint rate limiter
          <Function test_rate_limit_for_search_decorator>
            Test search endpoint rate limiter
          <Function test_exempt_from_rate_limit_decorator>
            Test rate limit exemption decorator
        <Class TestLimiterConfiguration>
          Test limiter instance configuration
          <Function test_limiter_has_correct_key_func>
            Test limiter uses correct key function
          <Function test_limiter_uses_fixed_window_strategy>
            Test limiter uses fixed-window strategy
          <Function test_limiter_headers_enabled>
            Test limiter has headers enabled
          <Function test_limiter_swallows_errors>
            Test limiter is configured to fail-open
        <Class TestRateLimitingIntegration>
          Integration tests for rate limiting
          <Function test_rate_limit_key_hierarchy>
            Test that rate limit key follows hierarchy (user > IP > global)
          <Function test_full_tier_based_limiting_flow>
            Test complete tier-based rate limiting flow
        <Class TestRateLimitErrorHandling>
          Test error handling and resilience
          <Function test_jwt_decode_error_handled_gracefully>
            Test that invalid tokens are handled gracefully (no user in state)
          <Function test_missing_jwt_secret_handled>
            Test that missing JWT secret is handled (AuthMiddleware would fail)
        <Class TestKeycloakIntegration>
          Integration tests for Keycloak RS256 token support
          <Function test_get_user_id_from_request_state>
            Test user ID extraction from request.state.user (auth middleware)
          <Function test_get_tier_from_request_state_roles>
            Test tier extraction from request.state.user roles
          <Function test_get_tier_from_request_state_enterprise_role>
            Test tier extraction for enterprise role
          <Function test_get_tier_from_request_state_standard_role>
            Test tier extraction for standard role
          <Function test_get_tier_from_request_state_free_role>
            Test tier extraction for free role
          <Function test_get_tier_fallback_to_tier_field>
            Test tier extraction falls back to tier field if no matching role
          <Function test_no_request_state_user_falls_back_to_anonymous>
            Test that missing request.state.user falls back to anonymous tier
          <Function test_rate_limit_key_uses_request_state_user_id>
            Test rate limit key generation uses request.state.user
    <Package monitoring>
      <Module test_budget_monitor.py>
        Tests for Budget Monitor

        Comprehensive test suite for budget monitoring and alerting following TDD principles.

        Tests cover:
        - Budget creation and retrieval
        - Budget period calculations (daily, weekly, monthly, quarterly, yearly)
        - Alert threshold detection (75%, 90%, custom)
        - Email alert delivery via SMTP
        - Webhook alert delivery via HTTP POST
        - Budget status and utilization tracking
        - Budget reset functionality
        - Alert history and filtering
        - Edge cases and error handling
        <Class TestBudgetCreation>
          Test suite for budget creation and retrieval.
          <Coroutine test_create_budget_stores_budget>
            Test create_budget() stores budget in memory.
          <Coroutine test_get_budget_retrieves_created_budget>
            Test get_budget() retrieves previously created budget.
          <Coroutine test_get_budget_returns_none_for_nonexistent_budget>
            Test get_budget() returns None for non-existent budget.
          <Coroutine test_create_budget_uses_default_start_date_if_not_provided>
            Test create_budget() uses current time as default start_date.
          <Coroutine test_create_budget_uses_default_alert_thresholds>
            Test create_budget() uses [0.75, 0.90] as default alert thresholds.
          <Coroutine test_get_all_budgets_returns_all_created_budgets>
            Test get_all_budgets() returns list of all budgets.
        <Class TestBudgetPeriodCalculations>
          Test suite for budget period boundary calculations.
          <Coroutine test_calculate_period_boundaries_for_daily_budget>
            Test _calculate_period_boundaries() for DAILY period.
          <Coroutine test_calculate_period_boundaries_for_weekly_budget>
            Test _calculate_period_boundaries() for WEEKLY period (Monday-Sunday).
          <Coroutine test_calculate_period_boundaries_for_monthly_budget>
            Test _calculate_period_boundaries() for MONTHLY period.
          <Coroutine test_calculate_period_boundaries_for_monthly_budget_in_december>
            Test _calculate_period_boundaries() handles December → January transition.
          <Coroutine test_calculate_period_boundaries_for_quarterly_budget>
            Test _calculate_period_boundaries() for QUARTERLY period.
          <Coroutine test_calculate_period_boundaries_for_yearly_budget>
            Test _calculate_period_boundaries() for YEARLY period.
        <Class TestBudgetAlerts>
          Test suite for budget alert triggering.
          <Coroutine test_check_budget_triggers_alert_at_75_percent>
            Test check_budget() triggers WARNING alert at 75% utilization.
          <Coroutine test_check_budget_triggers_alert_at_90_percent>
            Test check_budget() triggers CRITICAL alert at 90% utilization.
          <Coroutine test_check_budget_no_alert_below_threshold>
            Test check_budget() does not trigger alert below 75%.
          <Coroutine test_check_budget_only_alerts_once_per_threshold>
            Test check_budget() only alerts once per threshold (no duplicate alerts).
          <Coroutine test_check_budget_returns_none_when_budget_disabled>
            Test check_budget() returns None when budget is disabled.
        <Class TestEmailAlerts>
          Test suite for email alert delivery.
          <Coroutine test_send_email_alert_sends_smtp_message>
            Test _send_email_alert() sends email via SMTP.
          <Coroutine test_send_email_alert_skips_when_smtp_not_configured>
            Test _send_email_alert() skips sending when SMTP not configured.
          <Coroutine test_send_email_alert_includes_utilization_percentage>
            Test _send_email_alert() includes utilization percentage in message.
        <Class TestWebhookAlerts>
          Test suite for webhook alert delivery.
          <Coroutine test_send_webhook_alert_posts_to_webhook_url>
            Test _send_webhook_alert() sends HTTP POST to webhook URL.
          <Coroutine test_send_webhook_alert_skips_when_webhook_not_configured>
            Test _send_webhook_alert() skips sending when webhook URL not configured.
          <Coroutine test_send_webhook_alert_includes_timestamp>
            Test _send_webhook_alert() includes ISO 8601 timestamp in payload.
        <Class TestBudgetStatus>
          Test suite for budget status tracking.
          <Coroutine test_get_budget_status_calculates_utilization>
            Test get_budget_status() calculates budget utilization percentage.
          <Coroutine test_get_budget_status_detects_exceeded_budget>
            Test get_budget_status() detects when budget is exceeded.
          <Coroutine test_get_budget_status_returns_none_for_nonexistent_budget>
            Test get_budget_status() returns None for non-existent budget.
        <Coroutine test_reset_budget_clears_alerted_thresholds>
          Test reset_budget() clears alerted thresholds so alerts can trigger again.
        <Coroutine test_get_alerts_returns_all_alerts>
          Test get_alerts() returns all triggered alerts.
        <Coroutine test_get_alerts_filters_by_budget_id>
          Test get_alerts() filters by budget_id.
      <Module test_cost_api.py>
        Tests for Cost API Endpoints

        Comprehensive test suite for FastAPI cost monitoring endpoints following TDD principles.

        Tests cover:
        - GET /api/cost/summary - Aggregated cost summary
        - GET /api/cost/usage - Detailed usage records
        - GET /api/cost/budget/{id} - Budget status
        - POST /api/cost/budget - Create budget
        - GET /api/cost/trends - Time-series trends
        - GET /api/cost/export - CSV/JSON export
        - Edge cases and error handling
        <Function test_root_returns_api_information>
          Test GET / returns API information.
        <Function test_health_check_returns_healthy>
          Test GET /health returns healthy status.
        <Class TestCostSummaryEndpoint>
          Test suite for GET /api/cost/summary endpoint.
          <Function test_get_cost_summary_returns_aggregated_data>
            Test GET /api/cost/summary returns aggregated cost data.
          <Function test_get_cost_summary_aggregates_by_model>
            Test GET /api/cost/summary aggregates costs by model.
          <Function test_get_cost_summary_supports_different_periods>
            Test GET /api/cost/summary supports day, week, month periods.
        <Class TestUsageRecordsEndpoint>
          Test suite for GET /api/cost/usage endpoint.
          <Function test_get_usage_records_returns_detailed_records>
            Test GET /api/cost/usage returns detailed usage records.
          <Function test_get_usage_records_filters_by_user_id>
            Test GET /api/cost/usage filters by user_id.
          <Function test_get_usage_records_filters_by_model>
            Test GET /api/cost/usage filters by model.
          <Function test_get_usage_records_respects_limit_parameter>
            Test GET /api/cost/usage respects limit parameter.
        <Class TestBudgetEndpoints>
          Test suite for budget-related endpoints.
          <Function test_create_budget_creates_new_budget>
            Test POST /api/cost/budget creates new budget.
          <Function test_get_budget_status_returns_budget_details>
            Test GET /api/cost/budget/{id} returns budget status.
          <Function test_get_budget_status_returns_404_for_nonexistent_budget>
            Test GET /api/cost/budget/{id} returns 404 for non-existent budget.
        <Class TestTrendsEndpoint>
          Test suite for GET /api/cost/trends endpoint.
          <Function test_get_cost_trends_returns_time_series_data>
            Test GET /api/cost/trends returns time-series data.
          <Function test_get_cost_trends_supports_different_metrics>
            Test GET /api/cost/trends supports total_cost and token_usage metrics.
          <Function test_get_cost_trends_supports_different_periods>
            Test GET /api/cost/trends supports 7d, 30d, 90d periods.
        <Class TestExportEndpoint>
          Test suite for GET /api/cost/export endpoint.
          <Function test_export_cost_data_as_csv>
            Test GET /api/cost/export?format=csv exports CSV.
          <Function test_export_cost_data_as_json>
            Test GET /api/cost/export?format=json exports JSON.
          <Function test_export_cost_data_returns_400_for_unsupported_format>
            Test GET /api/cost/export returns 400 for unsupported format.
      <Module test_cost_tracker.py>
        Tests for Cost Tracking System

        Comprehensive test suite for LLM cost monitoring following TDD principles.

        Tests cover:
        - Token usage recording
        - Cost calculation
        - Cost aggregation by dimensions
        - Budget monitoring
        - Alert triggering
        - API endpoints
        - Edge cases
        <Function test_calculate_cost_for_anthropic_sonnet>
          Test cost calculation for Anthropic Claude 3.5 Sonnet.
        <Function test_calculate_cost_for_anthropic_haiku>
          Test cost calculation for Anthropic Claude 3.5 Haiku (cheaper model).
        <Function test_calculate_cost_for_openai_gpt4_turbo>
          Test cost calculation for OpenAI GPT-4 Turbo.
        <Function test_calculate_cost_for_google_gemini_flash>
          Test cost calculation for Google Gemini 2.5 Flash (free tier).
        <Function test_calculate_cost_with_zero_tokens_returns_zero>
          Test cost calculation with zero tokens.
        <Function test_calculate_cost_with_unknown_model_raises_key_error>
          Test cost calculation with unknown model raises KeyError.
        <Coroutine test_cost_metrics_collector_records_usage>
          Test CostMetricsCollector records token usage.
        <Coroutine test_cost_metrics_collector_calculates_cost_automatically>
          Test collector automatically calculates cost if not provided.
        <Coroutine test_cost_metrics_collector_increments_prometheus_counters>
          Test collector updates Prometheus metrics.
        <Coroutine test_cost_aggregator_sums_by_model>
          Test CostAggregator aggregates costs by model.
        <Coroutine test_cost_aggregator_sums_by_user>
          Test CostAggregator aggregates costs by user.
        <Coroutine test_cost_aggregator_sums_by_feature>
          Test CostAggregator aggregates costs by feature.
        <Coroutine test_cost_aggregator_calculates_total_cost>
          Test CostAggregator calculates total cost.
        <Class TestBudgetMonitor>
          Test suite for BudgetMonitor with memory safety pattern.
          <Coroutine test_budget_monitor_detects_75_percent_utilization>
            Test BudgetMonitor sends warning at 75% budget utilization.
          <Coroutine test_budget_monitor_detects_90_percent_utilization>
            Test BudgetMonitor sends critical alert at 90% budget utilization.
          <Coroutine test_budget_monitor_no_alert_below_threshold>
            Test BudgetMonitor does not alert below 75% utilization.
          <Coroutine test_budget_monitor_handles_budget_exceeded>
            Test BudgetMonitor handles budget being exceeded.
        <Function test_get_cost_summary_returns_aggregated_data>
          Test GET /api/cost/summary returns aggregated cost data.
        <Function test_get_cost_usage_filters_by_user>
          Test GET /api/cost/usage filters by user_id.
        <Function test_create_budget_creates_new_budget>
          Test POST /api/cost/budget creates new budget.
        <Function test_get_cost_trends_returns_time_series_data>
          Test GET /api/cost/trends returns time-series data.
        <Function test_export_cost_data_as_csv>
          Test GET /api/cost/export?format=csv exports CSV.
        <Coroutine test_end_to_end_cost_tracking_flow>
          Test complete flow: record usage → aggregate → check budget.
        <Coroutine test_cost_tracker_handles_concurrent_writes>
          Test CostMetricsCollector handles concurrent usage recording.
        <Function test_pricing_table_has_all_supported_models>
          Test pricing table includes all supported models.
      <Module test_prometheus_client.py>
        Tests for Prometheus Client

        Comprehensive test suite for Prometheus metric querying following TDD principles.

        Tests cover:
        - HTTP client initialization and cleanup
        - Instant queries and range queries
        - Result parsing (instant and range)
        - Uptime/downtime calculations
        - Percentile queries
        - Error rate calculations
        - SLA metrics aggregation
        - Edge cases and error handling
        <Class TestPrometheusClient>
          Test suite for PrometheusClient with memory safety pattern.
          <Coroutine test_initialize_creates_http_client>
            Test initialize() creates httpx.AsyncClient.
          <Coroutine test_initialize_is_idempotent>
            Test initialize() can be called multiple times safely.
          <Coroutine test_close_cleans_up_http_client>
            Test close() properly cleans up AsyncClient.
          <Function test_load_config_from_settings_uses_defaults>
            Test _load_config_from_settings() returns default config.
        <Function test_metric_value_from_prometheus_parses_correctly>
          Test MetricValue.from_prometheus() parses Prometheus data format.
        <Function test_query_result_get_latest_value_returns_last_value>
          Test QueryResult.get_latest_value() returns most recent value.
        <Function test_query_result_get_average_calculates_mean>
          Test QueryResult.get_average() calculates average of all values.
        <Function test_query_result_get_latest_value_returns_none_when_empty>
          Test QueryResult.get_latest_value() returns None for empty values.
        <Function test_query_result_get_average_returns_none_when_empty>
          Test QueryResult.get_average() returns None for empty values.
        <Class TestPrometheusQueries>
          Test suite for Prometheus query methods.
          <Coroutine test_query_executes_instant_query_successfully>
            Test query() executes instant query and parses response.
          <Coroutine test_query_range_executes_range_query_successfully>
            Test query_range() executes range query and parses time series.
          <Coroutine test_query_auto_initializes_if_not_initialized>
            Test query() auto-initializes client if not already initialized.
          <Coroutine test_query_raises_value_error_on_prometheus_error>
            Test query() raises ValueError when Prometheus returns error status.
          <Coroutine test_query_range_raises_value_error_when_client_not_initialized>
            Test query_range() raises ValueError if client is None.
        <Class TestPrometheusSLAQueries>
          Test suite for SLA-specific query methods.
          <Coroutine test_query_uptime_returns_percentage>
            Test query_uptime() returns uptime percentage.
          <Coroutine test_query_uptime_returns_100_when_no_data>
            Test query_uptime() returns 100% when no data available (optimistic).
          <Coroutine test_query_uptime_returns_99_on_error>
            Test query_uptime() returns 99% (conservative) on query error.
          <Coroutine test_query_downtime_calculates_seconds_from_uptime>
            Test query_downtime() calculates downtime in seconds.
          <Coroutine test_query_downtime_supports_multiple_timerange_formats>
            Test query_downtime() supports different timerange formats (d, h, m).
          <Coroutine test_query_downtime_raises_value_error_for_invalid_timerange>
            Test query_downtime() raises ValueError for unsupported timerange format.
          <Coroutine test_query_percentiles_returns_p50_p95_p99>
            Test query_percentiles() returns p50, p95, p99 percentiles.
          <Coroutine test_query_percentiles_returns_zero_on_query_error>
            Test query_percentiles() returns 0.0 for percentiles that fail to query.
          <Coroutine test_query_error_rate_calculates_percentage>
            Test query_error_rate() calculates error rate as percentage.
          <Coroutine test_query_error_rate_returns_zero_when_no_requests>
            Test query_error_rate() returns 0% when total requests is 0.
          <Coroutine test_get_sla_metrics_aggregates_all_metrics>
            Test get_sla_metrics() aggregates uptime, downtime, error rate, and response times.
        <Coroutine test_get_prometheus_client_returns_singleton>
          Test get_prometheus_client() returns global singleton instance.
    <Package property>
      <Module test_cache_properties.py>
        Property-based tests for cache module using Hypothesis.

        Tests cache invariants and edge cases:
        - Cache get/set roundtrip preservation
        - TTL expiration behavior
        - Key generation consistency
        - Multi-layer cache promotion/demotion
        - Concurrent access safety
        <Class TestCacheValuePreservation>
          Test that cache preserves values correctly
          <Function test_integer_values_preserved>
            Property: Integer values are preserved through cache
          <Function test_string_values_preserved>
            Property: String values are preserved through cache
          <Function test_list_values_preserved>
            Property: List values are preserved through cache
          <Function test_dict_values_preserved>
            Property: Dict values are preserved through cache
          <Function test_boolean_values_preserved>
            Property: Boolean values are preserved through cache
        <Class TestCacheKeyNormalization>
          Test cache key generation properties
          <Function test_key_generation_deterministic>
            Property: Same parts always generate same key
          <Function test_different_prefixes_different_keys>
            Property: Different prefixes generate different keys
          <Function test_different_versions_different_keys>
            Property: Different versions generate different keys
          <Function test_long_keys_are_hashed>
            Property: Long keys are hashed to prevent issues
        <Class TestCacheTTLProperties>
          Test TTL-related properties
          <Function test_ttl_lookup_consistent>
            Property: TTL lookup is consistent for valid cache types
          <Function test_unknown_cache_type_has_default_ttl>
            Property: Unknown cache types get default TTL
        <Class TestCacheStatisticsProperties>
          Test cache statistics invariants
          <Function test_statistics_counts_accumulate>
            Property: Statistics counts accumulate correctly
          <Function test_hit_rate_calculation_correct>
            Property: Hit rate is correctly calculated
          <Function test_delete_statistics_accurate>
            Property: Delete statistics are accurate
        <Class TestCacheConcurrencySafety>
          Test cache behavior under concurrent access
          <AsyncHypothesisTest test_concurrent_sets_dont_lose_data>
            Property: Concurrent sets don't lose data
          <AsyncHypothesisTest test_concurrent_reads_consistent>
            Property: Concurrent reads return consistent value
        <Class TestCacheDecoratorProperties>
          Test @cached decorator properties
          <AsyncHypothesisTest test_cached_decorator_memoization>
            Property: @cached decorator correctly memoizes results (avoiding falsy values)
          <AsyncHypothesisTest test_cached_decorator_different_args_cached_separately>
            Property: Different arguments are cached independently
        <Class TestCacheEdgeCases>
          Test cache edge cases and boundary conditions
          <Function test_cache_handles_none_value>
            Property: Cache can store and retrieve None without crashing
          <Function test_cache_handles_float_values>
            Property: Cache correctly handles float values
          <Function test_cache_respects_max_size>
            Property: Cache never exceeds maxsize
        <Class TestCacheTTLBehavior>
          Test TTL-related properties
          <Function test_cache_expires_after_ttl>
            Property: Cached values expire after TTL
          <Function test_ttl_from_key_prefix_consistent>
            Property: TTL determination is consistent for cache types
        <Class TestCacheStatisticsInvariants>
          Test cache statistics invariants
          <Function test_statistics_never_negative>
            Property: Cache statistics are never negative
          <Function test_hit_rate_between_zero_and_one>
            Property: Hit rate is always between 0 and 1
        <Class TestCacheStampedePrevention>
          Test cache stampede prevention properties
          <AsyncHypothesisTest test_stampede_prevention_calls_fetcher_once>
            Property: get_with_lock calls fetcher only once for concurrent requests
        <Class TestCacheDeleteProperties>
          Test cache deletion properties
          <Function test_delete_removes_keys>
            Property: Deleted keys are no longer retrievable
          <Function test_clear_removes_all_keys>
            Property: clear() removes all keys
        <Class TestCacheLevelIsolation>
          Test cache level isolation properties
          <Function test_l1_only_doesnt_affect_l2>
            Property: L1-only operations don't interact with L2
          <Function test_l2_writes_also_write_to_l1>
            Property: L2 writes are promoted to L1
      <Module test_event_loop_cleanup.py>
        Test event loop cleanup in property-based tests.

        MEMORY SAFETY: Ensure event loops are properly cleaned up in property tests
        to prevent file descriptor leaks and BaseEventLoop.__del__ errors.

        This test validates that the run_async helper in test_auth_properties.py
        properly cleans up event loops after each hypothesis example.
        <Class TestEventLoopCleanup>
          Test proper event loop cleanup in property-based tests
          <Function test_event_loop_cleanup_in_hypothesis>
            Event loops should be cleaned up properly in property tests
          <Function test_run_async_helper_closes_loop>
            The run_async helper should close event loops properly
          <Function test_multiple_event_loops_dont_leak>
            Creating multiple event loops should not leak file descriptors
      <Module test_resilience_properties.py>
        Property-based tests for resilience patterns using Hypothesis.

        Tests resilience invariants:
        - Circuit breakers prevent cascading failures
        - Retries respect exponential backoff
        - Timeouts enforce max duration
        - Bulkheads limit concurrency
        - Fallbacks provide degraded service
        <Class TestCircuitBreakerProperties>
          Property-based tests for circuit breaker
          <Function test_circuit_opens_after_consecutive_failures>
            Property: Circuit breaker opens after fail_max consecutive failures
          <Function test_circuit_stays_closed_on_success>
            Property: Circuit breaker stays closed when operations succeed
          <Function test_circuit_breaker_with_fallback_always_returns>
            Property: Circuit breaker with fallback always returns a value
        <Class TestRetryProperties>
          Property-based tests for retry logic
          <AsyncHypothesisTest test_retry_eventually_succeeds_or_exhausts>
            Property: Retry either succeeds within max_attempts or raises RetryExhaustedError
          <AsyncHypothesisTest test_retry_count_matches_configuration>
            Property: Retry attempts match max_attempts configuration
        <Class TestTimeoutProperties>
          Property-based tests for timeout enforcement
          <AsyncHypothesisTest test_timeout_enforced_correctly>
            Property: Timeout correctly determines if operation times out
          <AsyncHypothesisTest test_fast_operations_never_timeout>
            Property: Fast operations never timeout
        <Class TestBulkheadProperties>
          Property-based tests for bulkhead pattern
          <AsyncHypothesisTest test_bulkhead_limits_concurrency>
            Property: Bulkhead never exceeds max concurrent operations
          <AsyncHypothesisTest test_bulkhead_fail_fast_rejects_excess>
            Property: Bulkhead with wait=False rejects requests beyond capacity
        <Class TestFallbackProperties>
          Property-based tests for fallback strategies
          <AsyncHypothesisTest test_fallback_to_default_always_returns_default>
            Property: fallback_to_default always returns the default value on error
          <AsyncHypothesisTest test_fallback_not_used_on_success>
            Property: Fallback is not used when operation succeeds
          <AsyncHypothesisTest test_fail_open_returns_empty_list>
            Property: fail_open returns True on error (for boolean checks)
          <AsyncHypothesisTest test_fail_closed_raises_on_error>
            Property: fail_closed returns False on error (for boolean checks)
        <Class TestCacheProperties>
          Property-based tests for caching
          <Function test_cache_get_set_roundtrip>
            Property: Cache get/set roundtrip preserves value
          <Function test_cache_stores_different_values_independently>
            Property: Different cache keys store independent values
          <Function test_cache_miss_returns_none>
            Property: Cache miss always returns None
        <Class TestRetryExceptionHandling>
          Property-based tests for retry exception classification
          <Function test_client_errors_never_retry>
            Property: Client errors (non-retryable) should never be retried

            Note: TOKEN_EXPIRED is intentionally excluded as it has retry_policy=CONDITIONAL
            (can be retried with token refresh)
          <Function test_external_service_errors_always_retry>
            Property: External service errors should be retried
        <Class TestCacheKeyGenerationProperties>
          Property-based tests for cache key generation
          <Function test_cache_key_generation_is_deterministic>
            Property: Same inputs always generate same cache key
          <Function test_different_inputs_generate_different_keys>
            Property: Different inputs generate different cache keys
        <Class TestFallbackStrategyProperties>
          Property-based tests for fallback strategies
          <AsyncHypothesisTest test_default_fallback_returns_default>
            Property: Default fallback returns default value on any error
        <Class TestResilienceComposition>
          Property-based tests for composing resilience patterns
          <AsyncHypothesisTest test_retry_with_timeout_composition>
            Property: Retry + Timeout composition behaves correctly
          <AsyncHypothesisTest test_circuit_breaker_with_fallback_composition>
            Property: Circuit breaker + fallback provides degraded service
        <Class TestResilienceInvariants>
          Test resilience pattern invariants
          <AsyncHypothesisTest test_retry_backoff_increases>
            Property: Retry backoff time increases exponentially
          <Function test_circuit_breaker_state_is_consistent>
            Property: Circuit breaker state is consistent across calls
    <Package regression>
      <Module test_async_dependency_override_xdist.py>
        Regression Test: Async Dependency Override in pytest-xdist

        CODEX FINDING: FastAPI async dependencies overridden with sync lambdas fail in pytest-xdist
        DISCOVERY DATE: 2025-11-13
        SYMPTOM: 401 Unauthorized errors when overriding async dependencies with sync lambdas
        ROOT CAUSE: get_current_user is async but was being overridden with sync lambda: lambda: user

        CORRECT PATTERN (async override for async dependency):
            async def override_get_current_user():
                return {"user_id": "test"}
            app.dependency_overrides[get_current_user] = override_get_current_user

        WRONG PATTERN (sync override for async dependency):
            app.dependency_overrides[get_current_user] = lambda: {"user_id": "test"}  # FAILS in xdist!

        This regression test ensures:
        1. Sync lambda overrides fail as expected (demonstrates the bug)
        2. Async function overrides work correctly (demonstrates the fix)
        3. Pattern is validated by pre-commit hook

        See Also:
        - tests/PYTEST_XDIST_BEST_PRACTICES.md: FastAPI Auth Override Pattern
        - scripts/validation/validate_async_dependency_overrides.py: Pre-commit validation
        <Class TestAsyncDependencyOverridePattern>
          Regression tests for async dependency override pattern in pytest-xdist.

          These tests validate that async dependencies MUST be overridden with async
          functions, not sync lambdas, to work correctly in pytest-xdist workers.
          <Function test_sync_lambda_override_fails_with_async_dependency>
            REGRESSION: Sync lambda override of async dependency causes 401 errors

            This test demonstrates the BUG that was found in test_api_keys_endpoints.py.
            When an async dependency is overridden with a sync lambda, pytest-xdist
            workers fail to resolve the dependency properly, resulting in 401 errors.

            This test is EXPECTED TO FAIL initially (RED phase).
          <Function test_async_function_override_works_with_async_dependency>
            CORRECT PATTERN: Async function override works in pytest-xdist

            This test demonstrates the CORRECT way to override async dependencies.
            Using an async function instead of a sync lambda ensures that pytest-xdist
            workers can properly resolve the dependency.

            This test should PASS (GREEN phase).
          <Function test_mixed_override_pattern_works>
            Test that mixing sync and async overrides works when types match.

            Sync dependencies can be overridden with sync functions/lambdas.
            Async dependencies MUST be overridden with async functions.
          <Function test_demonstrate_async_override_bug_explicitly>
            DEMONSTRATION: This test explicitly shows the async override bug

            This test is SKIPPED by default because it's expected to fail.
            It demonstrates that sync lambda overrides of async dependencies
            cause failures in pytest-xdist workers.

            To see the failure, run with: pytest -k test_demonstrate_async_override_bug --no-skip
        <Class TestCodebaseAsyncOverrideCompliance>
          Validation tests to ensure all test files use correct async override pattern.

          These tests scan the codebase to find violations of the async override pattern
          and ensure the pre-commit hook catches them.
          <Function test_no_sync_lambda_overrides_for_get_current_user>
            Validate that no test files use sync lambda to override get_current_user.

            This meta-test ensures the regression doesn't happen again by scanning
            all test files for the buggy pattern.
          <Function test_pre_commit_hook_validates_async_overrides>
            Verify that pre-commit hook exists to validate async override pattern.

            This test checks that we have a pre-commit hook configured to catch
            sync lambda overrides of async dependencies.
      <Module test_auth_middleware_isolation.py>
        Regression tests for auth middleware isolation.

        This module contains tests to ensure that the global auth middleware
        is properly reset between tests to prevent pollution in pytest-xdist
        parallel execution.

        Background:
        -----------
        The `set_global_auth_middleware()` function modifies a module-level global
        variable `_global_auth_middleware` in the auth.middleware module. If this
        global is not reset between tests, it can pollute subsequent tests running
        in the same pytest-xdist worker, causing unexpected authentication behavior.

        Failure Scenario:
        -----------------
        1. Test A calls `set_global_auth_middleware(custom_auth)`
        2. Test A completes
        3. Test B runs in same worker, expects default auth middleware
        4. Test B sees `_global_auth_middleware` still set to custom_auth from Test A
        5. Test B fails due to unexpected auth behavior

        See Also:
        ---------
        - src/mcp_server_langgraph/auth/middleware.py
        - tests/conftest.py:268-291 (reset_dependency_singletons fixture)
        - tests/test_auth.py (tests that use set_global_auth_middleware)
        <Class TestAuthMiddlewareIsolation>
          Test auth middleware global state isolation.
          <Function test_global_auth_middleware_starts_none>
            Test that global auth middleware starts as None.

            This test validates that the `_global_auth_middleware` global
            is properly reset between tests.
          <Function test_set_global_auth_middleware_modifies_global>
            Test that set_global_auth_middleware() modifies the global variable.

            This test validates that the function works as expected.
          <Function test_global_auth_middleware_pollution_detection>
            Test that we can detect auth middleware pollution.

            This test simulates what would happen if a previous test set
            the global auth middleware and it wasn't cleaned up.

            This test should FAIL if the reset_dependency_singletons fixture
            is not properly resetting _global_auth_middleware.
          <Function test_multiple_tests_dont_interfere_with_each_other>
            Test that multiple tests setting global auth don't interfere.

            This test runs multiple scenarios in sequence to ensure proper isolation.
      <Module test_bearer_scheme_isolation.py>
        Regression Tests for bearer_scheme Module-Level Singleton Isolation

        These tests validate that the module-level bearer_scheme singleton in
        auth/middleware.py doesn't cause state pollution across pytest-xdist workers.

        This prevents regression of the bug where TestAPIKeyEndpointAuthorization
        tests (which don't override dependencies) run before TestCreateAPIKey tests
        on the same worker, causing intermittent 401 Unauthorized errors.

        Root Cause:
        -----------
        In src/mcp_server_langgraph/auth/middleware.py:816:
            bearer_scheme = HTTPBearer(auto_error=False)  # MODULE-LEVEL SINGLETON

        This singleton is shared across all app instances in the same pytest-xdist worker,
        causing state pollution when tests create FastAPI apps with and without dependency
        overrides.

        The Fix:
        --------
        Override bearer_scheme in ALL test fixtures that override get_current_user:
            app.dependency_overrides[bearer_scheme] = lambda: None

        This bypasses the singleton's auth check, allowing mocked get_current_user to work.

        See: tests/PYTEST_XDIST_BEST_PRACTICES.md for detailed explanation.
        <Class TestBearerSchemeIsolation>
          Regression tests for bearer_scheme module-level singleton isolation.

          Verifies that bearer_scheme state doesn't pollute across tests in pytest-xdist.
          <Function test_bearer_scheme_override_prevents_401_errors>
            ✅ CORRECT: Override bearer_scheme to bypass auth in mocked tests.

            This is the FIX for the 401 errors. When testing API endpoints with
            mocked authentication, we must override BOTH get_current_user AND bearer_scheme.

            UPDATED (Revision 7 - 2025-11-14):
            Now uses importlib.reload() pattern to ensure fresh module references.
            This is required because previous tests may have cached stale router references.
          <Function test_bearer_scheme_not_overridden_works_in_isolation>
            📝 DOCUMENTATION: In isolation, overriding get_current_user works without bearer_scheme.

            This test shows that the bug is INTERMITTENT and ORDER-DEPENDENT:
            - In isolation: Works fine (200 OK)
            - In pytest-xdist full suite: Fails with 401 when unlucky test order occurs

            The bug manifests when:
            1. TestAPIKeyEndpointAuthorization tests run first on a worker (no overrides)
            2. TestCreateAPIKey tests run next on same worker (with overrides)
            3. Module-level state from step 1 pollutes step 2

            Therefore: Always override bearer_scheme as defensive programming.
          <Function test_execution_order_documented>
            DOCUMENTATION TEST: Execution order matters in pytest-xdist.

            **NOTE**: This is a DOCUMENTATION test, not a validation test.
            It serves as living documentation for why bearer_scheme override is required.

            This test documents why bearer_scheme override is necessary as defensive
            programming against pytest-xdist test execution order variations.

            In pytest-xdist, tests run in different order on different workers:
            - Worker [gw0] might run: TestListAPIKeys → TestCreateAPIKey ✅ Works
            - Worker [gw5] might run: TestAPIKeyEndpointAuthorization → TestCreateAPIKey ❌ Fails

            When TestAPIKeyEndpointAuthorization.test_create_without_auth() runs BEFORE
            TestCreateAPIKey tests on the same worker, the module-level bearer_scheme
            singleton gets "primed" with no-auth state, causing subsequent tests to fail.

            The fix: Always override bearer_scheme in ALL API test fixtures that use
            get_current_user dependency.

            **Actual validation**: See test_bearer_scheme_override_prevents_401_errors
            which executes actual HTTP requests and verifies overrides work.

            References:
            - OpenAI Codex Finding: "Regression tests are inert documentation"
            - This pattern is validated by executable tests in this same file
        <Class TestNestedDependencyOverrides>
          Tests for FastAPI nested dependency override patterns.

          Documents that ALL nested dependencies must be overridden, not just top-level ones.
          <Function test_nested_dependency_override_pattern>
            Documentation test explaining nested dependency override requirements.

            When a dependency has nested Depends():

            async def parent_dep(
                nested_dep = Depends(some_function)
            ):
                return value

            You must override BOTH:
            - app.dependency_overrides[parent_dep] = mock_parent
            - app.dependency_overrides[some_function] = mock_nested  # ← Don't forget!

            In our case:
            - parent_dep = get_current_user
            - some_function = bearer_scheme

            Therefore:
            - app.dependency_overrides[get_current_user] = mock_user
            - app.dependency_overrides[bearer_scheme] = lambda: None  # ← REQUIRED!
        <Class TestCodexReloadScenario>
          Regression test for OpenAI Codex finding: importlib.reload() causing 401 errors.

          Codex Finding (2025-11-14):
          ---------------------------
          Running the suite in isolation (OTEL_SDK_DISABLED=true .venv/bin/pytest
          tests/api/test_service_principals_endpoints.py -n 8 --maxfail=1 -q) passes,
          but a full make test-unit captured intermittent 401s for all three delete tests.

          Reproducing the same failure is as simple as forcing a reload of the auth
          middleware before re-running the test:

              ENVIRONMENT=test .venv/bin/python - <<'PY'
              import importlib
              import mcp_server_langgraph.auth.middleware
              importlib.reload(mcp_server_langgraph.auth.middleware)
              pytest ...TestDeleteServicePrincipal::test_delete_service_principal_success ...
              PY

          Root Cause:
          -----------
          FastAPI router retains references to the original get_current_user function
          created when src/mcp_server_langgraph/api/service_principals.py is first imported.
          When any test reloads mcp_server_langgraph.auth.middleware, the module-level
          bearer_scheme and get_current_user are redefined. Test fixtures then override
          the NEW functions, but the router is still wired to the STALE functions.

          The Fix:
          --------
          Override bearer_scheme in ALL API test fixtures BEFORE include_router().
          This ensures the override applies regardless of reload timing.
          <Function test_reload_middleware_then_run_service_principal_test>
            TDD REGRESSION TEST: Validate fix for Codex reload scenario.

            GIVEN: Auth middleware has been reloaded (simulating smoke test pollution)
            WHEN: Running service principal delete test with proper overrides
            THEN: Test should PASS (not 401) because bearer_scheme override is in place

            This test simulates the exact scenario described in Codex findings:
            1. Reload auth middleware (creates new bearer_scheme/get_current_user)
            2. Reload service principals module to pick up new references
            3. Create test client with overrides using BOTH old and new references
            4. Call service principal endpoint
            5. Verify NO 401 errors occur

            Expected: PASS (fix is in place)
            Before fix: Would FAIL with 401 Unauthorized

            KEY INSIGHT: The fix is to override bearer_scheme in the test fixtures.
            This test validates that even after a reload, the pattern works correctly.
      <Module test_bearer_scheme_override_diagnostic.py>
        Diagnostic Test for Bearer Scheme Override Fix (Commit 05a54e1)

        This test verifies that the bearer_scheme override fix is present and working correctly.
        It can be run both locally and in Docker to diagnose authentication issues.

        Root Cause Fixed in Commit 05a54e1:
        - bearer_scheme singleton in auth/middleware.py was causing test pollution
        - Solution: Override bearer_scheme in all API test fixtures

        This test ensures that fix is present and functioning correctly.
        <Class TestBearerSchemeOverrideDiagnostic>
          Diagnostic tests for bearer_scheme override fix (Commit 05a54e1)

          These tests verify that the authentication override pattern is working correctly
          and can be used to diagnose issues when tests fail with 401 errors.
          <Function test_bearer_scheme_override_is_present_in_api_keys_fixture>
            Verify that api_keys_test_client fixture has bearer_scheme override

            This test reads the test_api_keys_endpoints.py file and verifies that:
            1. bearer_scheme is imported
            2. bearer_scheme override is set
            3. Override is set BEFORE app.include_router()

            If this test fails, the fix from commit 05a54e1 is missing.
          <Function test_bearer_scheme_override_actually_works>
            Functional test: Verify that bearer_scheme override prevents 401 errors

            This test creates a minimal FastAPI app with the same pattern as
            api_keys_test_client and verifies that it works correctly.

            UPDATED (Revision 7 - 2025-11-14):
            Now uses importlib.reload() pattern to ensure fresh module references.
            This is required because previous tests may have cached stale router references.
          <Function test_docker_image_has_fix>
            Diagnostic test: Check if running in Docker and verify fix is present

            This test helps identify if the Docker image is stale and needs rebuilding.
            It checks if we're running in Docker and validates the fix is present.
          <Function test_git_commit_is_recent_enough>
            Diagnostic test: Verify current codebase includes the fix commit

            This test checks if commit 05a54e1 is in the git history.
            If not, the codebase is too old and needs to be updated.

            NOTE: Skips in CI shallow clones (fetch-depth: 1) since historical
            commits aren't available. The actual fix validation happens via
            test_bearer_scheme_override_code_is_present() which checks source code.
        <Function test_bearer_scheme_override_documentation>
          Meta-test: Verify that bearer_scheme override pattern is documented

          This test ensures that the fix is properly documented in test files
          so future developers understand why it's necessary.
      <Module test_dev_dependencies.py>
        Validate that all test file imports are available in dev dependencies.

        Prevents ModuleNotFoundError in CI by ensuring test imports match
        the dependencies installed in test environments.

        This test follows TDD principles:
        - Written FIRST to catch the docker dependency issue
        - Fails initially (RED phase)
        - Passes after adding docker to dev deps (GREEN phase)
        - Prevents future regressions (REFACTOR phase)
        <Function test_test_imports_have_dev_dependencies>
          Ensure all test file imports are available in project dependencies.

          This test prevents CI failures from missing dependencies by validating
          that every package imported in tests/ is available either in:
          - Main dependencies (project.dependencies)
          - Dev dependencies (project.optional-dependencies.dev)
          - Or any other optional dependency group

          Test approach (TDD):
          - RED: Initially fails because some packages are in wrong dependency groups
          - GREEN: Passes after moving packages to appropriate dependency groups
          - REFACTOR: Prevents future regressions by running in CI

          Exceptions:
          - Standard library modules (sys, os, asyncio, etc.)
          - Local project modules (mcp_server_langgraph, tests, scripts, etc.)

          Failure mode:
          - Lists all packages imported by tests but not in any dependency group
          - Provides actionable fix: add missing packages to pyproject.toml
        <Function test_dev_dependencies_are_importable>
          Verify that all dev dependencies can be imported.

          This is a sanity check to ensure:
          - Dependencies are correctly installed
          - No broken package specifications
          - No conflicts in dependency resolution

          Note: Some packages may have different import names than package names.
          We skip import validation for known cases (e.g., pytest-* packages).
      <Module test_documentation_structure.py>
        Regression tests for documentation structure and organization.

        Prevents issues identified in documentation audit (2025-11-12):
        - Orphaned MDX files not in navigation
        - Duplicate ADR numbering
        - Broken internal links
        - Missing files referenced in navigation
        - Version inconsistencies

        Author: Documentation Audit Remediation
        Date: 2025-11-12
        Related: GitHub Issues #75-80
        <Class TestDocumentationNavigation>
          Prevent orphaned files and broken navigation references.
          <Function test_all_mdx_files_in_navigation>
            Ensure all MDX files (except excluded ones) are referenced in docs.json.

            Prevents: Orphaned documentation files (CRIT-001 from audit)
          <Function test_all_navigation_pages_exist>
            Ensure all pages referenced in navigation actually exist.

            Prevents: Missing files causing broken navigation
          <Function test_no_duplicate_pages_in_navigation>
            Ensure no page is referenced multiple times in navigation.

            Prevents: Duplicate navigation entries causing confusion
        <Class TestADRNumbering>
          Prevent duplicate ADR numbering.
          <Function test_no_duplicate_adr_numbers>
            Ensure no two ADRs have the same number.

            Prevents: Duplicate ADR numbering (WARN-002 from audit)
          <Function test_adr_source_and_mintlify_sync>
            Ensure ADRs in /adr match those in /docs/architecture.

            Prevents: ADR sync drift between source and Mintlify
          <Function test_adr_sequential_numbering>
            Warn about gaps in ADR numbering sequence.

            Note: This is a warning, not a failure. Gaps may be intentional.
        <Class TestVersionConsistency>
          Ensure version numbers are consistent across project.
          <Function test_version_consistency_in_deployment_files>
            Ensure deployment files reference consistent version.

            Prevents: Version drift in deployment configurations
          <Function test_readme_adr_badge_accuracy>
            Ensure README.md ADR badge count matches actual ADR count.

            Prevents: Incorrect badge counts (WARN-002 from audit)
        <Class TestDocumentationQuality>
          Additional quality checks for documentation.
          <Function test_no_todos_in_public_docs>
            Ensure public-facing documentation doesn't have TODO/FIXME comments.

            Prevents: Incomplete public documentation (WARN-003 from audit)

            This checks ALL public docs except:
            - Templates (intentionally have TODOs as scaffolding)
            - Internal docs (docs-internal/)
            - Archive docs

            TODOs in development/internal docs are acceptable for tracking.
          <Function test_mdx_files_have_frontmatter>
            Ensure all MDX files have proper frontmatter.

            Prevents: Missing metadata in Mintlify docs
          <Function test_no_broken_internal_links>
            Check for broken internal links in MDX files.

            Prevents: Broken internal links between documentation pages

            Validates:
            - Relative links to other .mdx files
            - Links to files in the repository
            - Anchor links within the same file
        <Class TestRootDocumentationFiles>
          Validate root-level documentation files.
          <Function test_essential_root_docs_exist>
            Ensure essential root documentation files exist.

            Prevents: Missing critical project documentation
          <Function test_changelog_not_too_large>
            Warn if CHANGELOG.md is excessively large.

            Prevents: Unwieldy changelog file (WARN-004 from audit)
      <Module test_fastapi_auth_override_sanity.py>
        Regression tests for FastAPI auth override sanity checks.

        PURPOSE:
        --------
        These tests serve as a "TDD backstop" to ensure FastAPI endpoint tests
        always properly override authentication dependencies.

        Per OpenAI Codex findings: "Document a TDD backstop: whenever we add a new
        FastAPI endpoint, write a tiny 'auth override sanity' test that fails if
        overrides go missing (assert 200 with mocked auth, 401 without). It keeps
        the async/sync contract visible."

        PATTERN:
        --------
        For each authenticated FastAPI endpoint, we test:
        1. ✅ Returns 200 with proper async auth override + bearer_scheme override
        2. ❌ Returns 401 without bearer_scheme override
        3. ❌ Returns error with sync lambda for async dependency

        This makes the authentication contract explicit and visible in tests.

        BENEFITS:
        ---------
        - Prevents async/sync override mismatch (causes intermittent 401s)
        - Prevents missing bearer_scheme override (causes singleton pollution)
        - Makes authentication requirements visible
        - Catches regressions immediately
        - Serves as living documentation

        References:
        -----------
        - OpenAI Codex Finding: Additional Improvements - TDD backstop
        - PYTEST_XDIST_BEST_PRACTICES.md: Dependency override patterns
        - Commit 079e82e: Fixed async/sync mismatch
        - tests/regression/test_bearer_scheme_isolation.py
        <Class TestGDPREndpointAuthOverrides>
          Sanity tests for GDPR endpoint authentication overrides.

          These tests validate the complete auth override pattern for GDPR endpoints.
          <Function test_user_data_endpoint_with_proper_auth_override_returns_200>
            🟢 GREEN: Test GET /api/v1/users/me/data with middleware-based auth.

            This test demonstrates the NEW middleware pattern:
            - Test middleware sets request.state.user directly
            - No dependency overrides needed
            - Mock GDPR storage to avoid global singleton pollution

            After refactoring to middleware-based auth,
            tests simply set request.state.user via test middleware.

            This test should PASS with the new middleware implementation.
          <Function test_user_data_endpoint_override_without_bearer_coupling>
            🟢 GREEN: Test GET /api/v1/users/me/data with middleware-based auth.

            After refactoring to middleware-based auth, there's no dependency coupling at all.
            Middleware sets request.state.user directly, and endpoints access it.

            This test validates that the new middleware pattern works correctly
            without any dependency injection complexity.
          <Function test_consent_endpoint_with_proper_auth_override_returns_200>
            🟢 GREEN: Test POST /api/v1/users/me/consent with middleware-based auth.

            Another example of the middleware pattern for a different endpoint.

            After refactoring to middleware-based auth,
            tests use test middleware to set request.state.user.
        <Function test_auth_override_sanity_pattern_documentation>
          📚 Document the auth override sanity test pattern.

          This test serves as living documentation for the TDD backstop pattern.
        <Class TestAuthOverrideSanityPattern>
          Tests demonstrating the auth override sanity pattern for various scenarios.
          <Function test_pattern_works_with_minimal_mock>
            🟢 GREEN: Demonstrate that sanity tests can use minimal mocking.

            Sanity tests should be SIMPLE and FAST:
            - Minimal mock user (just required fields)
            - No complex dependency mocking
            - Just verify 200 vs 401

            After refactoring to middleware-based auth,
            tests use simple test middleware.

            This keeps them maintainable and fast to run.
          <Function test_pattern_fails_fast_on_missing_override>
            🔴 RED: Demonstrate that missing overrides cause immediate 401.

            Without any auth override, endpoints should return 401.
            This is the baseline behavior that sanity tests protect against.
          <Function test_pattern_is_copy_paste_friendly>
            🟢 GREEN: Demonstrate that the pattern is easy to copy-paste.

            Sanity tests should be:
            - ~15 lines of code
            - Copy-paste from template
            - Just change endpoint path and method
            - No complex logic

            This makes them low-friction to add for every new endpoint.
        <Function test_auth_override_sanity_benefits>
          DOCUMENTATION TEST: Benefits of auth override sanity tests.

          **NOTE**: This is a DOCUMENTATION test that describes benefits and best practices.
          It serves as living documentation, not as executable validation.

          Benefits:
          ---------
          1. **Fast Feedback**: Catches auth issues immediately, not in production
          2. **Visible Contract**: Makes authentication requirements explicit
          3. **Prevents Regressions**: Hard to accidentally break auth
          4. **Low Maintenance**: Minimal code, easy to update
          5. **TDD-Friendly**: Write sanity test first when adding endpoint
          6. **Documentation**: Shows correct pattern for new developers

          Example Workflow:
          -----------------
          1. New endpoint: POST /api/v1/new-feature
          2. Write sanity test FIRST (TDD):
             - test_new_feature_with_proper_auth_override_returns_200()
          3. Test FAILS (endpoint doesn't exist yet)
          4. Implement endpoint
          5. Test PASSES
          6. Now protected against auth regressions!

          Cost-Benefit Analysis:
          ----------------------
          - Time to write: 2-3 minutes per endpoint
          - Time to run: < 100ms per test
          - Value: Prevents hours of debugging intermittent 401 errors
          - ROI: Very high!

          Comparison to Alternatives:
          ---------------------------
          - Full integration tests: Slower, more complex, still valuable
          - Manual testing: Error-prone, not automated
          - No testing: Regressions will happen
          - Sanity tests: Best balance of cost/benefit

          Recommendation:
          ---------------
          Make these sanity tests MANDATORY for all authenticated endpoints.
          Add to code review checklist:
          - [ ] New endpoint has auth override sanity test
          - [ ] Sanity test follows the pattern (bearer_scheme + async override)
          - [ ] Cleanup is present (dependency_overrides.clear())

          **Actual validation**: See TestGDPREndpointAuthOverrides and
          TestAuthOverrideSanityPattern classes which execute real HTTP requests.

          References:
          - OpenAI Codex Finding: "TDD backstop for auth overrides"
          - This file contains executable examples of the sanity pattern
      <Module test_gdpr_singleton_parallel_isolation.py>
        TDD Test: GDPR Singleton Parallel Execution Isolation

        PURPOSE:
        --------
        This test validates that GDPR storage does NOT suffer from cross-worker
        pollution when using pytest-xdist parallel execution.

        EXPECTED BEHAVIOR (TDD RED -> GREEN):
        --------------------------------------
        🔴 RED: This test should FAIL with the current global singleton implementation
                because multiple pytest-xdist workers share the same global _gdpr_storage
                variable, causing cross-worker state pollution.

        🟢 GREEN: After refactoring to request-scoped dependency injection, this test
                  should PASS consistently because each request gets its own storage
                  instance with no shared global state.

        CURRENT ISSUE (from CODEX report):
        -----------------------------------
        - Tests pass 100% in serial mode
        - Tests pass ~85% with parallel execution (8 workers)
        - Root cause: Global singleton `_gdpr_storage` in factory.py:170-228
        - Fixture resets only affect current worker, not other workers

        VALIDATION:
        -----------
        Run this test with different worker counts to reproduce the issue:

        # Serial (should PASS):
        pytest tests/regression/test_gdpr_singleton_parallel_isolation.py -v

        # Parallel 2 workers (should PASS ~95%):
        pytest tests/regression/test_gdpr_singleton_parallel_isolation.py -n 2 -v

        # Parallel 8 workers (should FAIL ~15% - current issue):
        pytest tests/regression/test_gdpr_singleton_parallel_isolation.py -n 8 -v

        SOLUTION:
        ---------
        Refactor src/mcp_server_langgraph/compliance/gdpr/factory.py to use
        request-scoped dependency injection instead of global singleton:

            from fastapi import Request, Depends

            async def get_gdpr_storage(request: Request) -> GDPRStorage:
                """Get GDPR storage from request state (no global)."""
                if not hasattr(request.state, "gdpr_storage"):
                    request.state.gdpr_storage = await create_gdpr_storage(
                        backend="memory"  # or from config
                    )
                return request.state.gdpr_storage

        Reference:
        ----------
        - CODEX_FINDINGS_VALIDATION_REPORT_2025-11-21.md
        - tests/regression/test_fastapi_auth_override_sanity.py (related flakiness)
        <Class TestGDPRSingletonParallelIsolation>
          TDD tests for GDPR singleton parallel execution isolation.

          These tests are designed to FAIL with the current global singleton
          implementation and PASS after refactoring to request-scoped state.
          <Coroutine test_singleton_isolation_worker_specific_data_does_not_leak>
            🔴 RED: This test should FAIL in parallel mode with global singleton.
            🟢 GREEN: Should PASS after refactoring to request-scoped storage.

            Test validates that data created by one worker doesn't leak to another.
          <Coroutine test_singleton_reset_only_affects_current_worker>
            🔴 RED: This test demonstrates the current issue - reset only affects current worker.
            🟢 GREEN: After refactoring, this test should show proper isolation.

            Test validates that reset_gdpr_storage() only resets the current worker's storage.
          <Coroutine test_parallel_execution_creates_independent_storage_instances>
            🔴 RED: This test should FAIL with global singleton (all workers share same instance).
            🟢 GREEN: Should PASS with request-scoped storage (each worker independent).

            Test validates that each pytest-xdist worker gets its own storage instance.
          <Coroutine test_concurrent_storage_operations_do_not_conflict>
            🔴 RED: May fail with global singleton due to race conditions.
            🟢 GREEN: Should pass with request-scoped storage (proper isolation).

            Test validates that concurrent operations on storage don't conflict.
        <Class TestGDPRSingletonStressTest>
          Stress tests to expose singleton issues more reliably.

          These tests are more aggressive and should consistently FAIL with
          the global singleton implementation when run in parallel.
          <Coroutine test_rapid_initialize_reset_cycles_do_not_corrupt_state>
            🔴 RED: Likely to fail with global singleton under parallel execution.
            🟢 GREEN: Should pass with proper isolation.

            Rapidly initialize and reset storage to expose race conditions.
      <Module test_langgraph_return_types.py>
        Regression test for LangGraph 1.0.3+ return type changes.

        OpenAI Codex Finding (2025-11-16):
        ==================================
        tests/patterns/test_supervisor.py:241 fails with AttributeError: 'dict' object has no attribute 'task'

        Root Cause:
        -----------
        LangGraph underwent a breaking API change between versions:
        - **0.6.10 and earlier**: compiled.invoke() returned Pydantic model instances (e.g., SupervisorState)
        - **1.0.3+**: compiled.invoke() returns plain Python dictionaries

        Impact:
        -------
        Tests that access result.task (attribute access) fail because result is now a dict,
        requiring result["task"] (key access) instead.

        This regression test validates the current LangGraph 1.0.3+ behavior and ensures
        our code correctly handles dict return types.

        Related Files:
        --------------
        - src/mcp_server_langgraph/patterns/supervisor.py - Already handles dicts correctly (lines 248+)
        - src/mcp_server_langgraph/patterns/swarm.py - Also handles dicts correctly
        - tests/patterns/test_supervisor.py:241 - Needs update to use dict access
        - tests/patterns/test_swarm.py - May need similar update

        References:
        -----------
        - pyproject.toml: langgraph>=1.0.3 (line 29)
        - Migration from 0.2.28 → 0.6.10 → 1.0.3 (breaking changes)
        <Class TestLangGraphReturnTypes>
          Test suite validating LangGraph 1.0.3+ return type behavior.

          These tests document the breaking API change and ensure our patterns
          handle both dict and Pydantic model types correctly.
          <Function test_langgraph_1_0_3_returns_dict_not_pydantic_model>
            Validate that LangGraph 1.0.3+ compiled.invoke() returns dict, not Pydantic model.

            This is the DOCUMENTED behavior we expect in LangGraph 1.0.3+.
            If this test fails, LangGraph may have reverted the API change.
          <Function test_supervisor_pattern_handles_dict_return>
            Validate that Supervisor pattern correctly handles dict returns from compiled.invoke().

            The Supervisor.invoke() method (lines 248+) already treats results as dicts.
            This test ensures that implementation is correct.
          <Function test_swarm_pattern_handles_dict_return>
            Validate that Swarm pattern correctly handles dict returns from compiled.invoke().

            Similar to Supervisor, Swarm should handle dict returns correctly.
          <Function test_dict_to_pydantic_conversion>
            Demonstrate how to convert dict results back to Pydantic models if needed.

            This pattern can be used in tests that need type safety from Pydantic models.
          <Function test_return_type_consistent_with_and_without_checkpointer[True]>
            Validate that return type is consistent regardless of checkpointer usage.

            Both checkpointed and non-checkpointed invocations should return dicts in 1.0.3+.
          <Function test_return_type_consistent_with_and_without_checkpointer[False]>
            Validate that return type is consistent regardless of checkpointer usage.

            Both checkpointed and non-checkpointed invocations should return dicts in 1.0.3+.
        <Class TestBackwardCompatibilityPatterns>
          Test patterns for writing code that works with both dict and Pydantic returns.

          These patterns help maintain backward compatibility if LangGraph changes behavior again.
          <Function test_safe_dict_access_pattern>
            Demonstrate safe access pattern that works for both dict and Pydantic models.
          <Function test_helper_function_for_safe_access>
            Demonstrate helper function for safe attribute/key access.
      <Module test_litellm_cleanup_warnings.py>
        Regression test for litellm async client cleanup warnings.

        OpenAI Codex Finding (2025-11-17 - RESOLVED):
        - RuntimeWarning from litellm: coroutine 'close_litellm_async_clients' was never awaited
        - Warning appeared during test teardown when event loop closes
        - Source: /litellm/llms/custom_httpx/async_client_cleanup.py:78

        Root Cause:
        - litellm registers an atexit handler at import time (__init__.py:105)
        - The handler calls loop.create_task() when loop.is_running() is True
        - During pytest shutdown (especially with pytest-xdist workers), the task is created but never awaited
        - The warning is emitted by Python's C code in asyncio, bypassing warnings filters
        - Python 3.12+ removed atexit._exithandlers, making handler unregistration difficult

        Investigation (2025-11-15 to 2025-11-17):
        - Upgraded litellm from 1.78.5 → 1.79.3 (latest stable) - warning persisted
        - GitHub issues #13970 and #9817 claim fixes, but warning still appeared
        - pytest_sessionfinish hook cleanup was correct but atexit handler still ran
        - Python 3.12.12 doesn't have atexit._exithandlers (removed earlier than expected)

        Solution (2025-11-17 - IMPLEMENTED):
        - Monkey-patch atexit.register at import time (conftest.py:7-46)
        - Intercept and filter out litellm's cleanup_wrapper before registration
        - Prevent atexit handler from ever being registered
        - Manual cleanup in pytest_sessionfinish still runs (belt-and-suspenders)
        - Clear litellm's in-memory cache to ensure no handlers trigger

        Current Status (RESOLVED):
        - ✅ Zero RuntimeWarnings in regression tests (125 passed, 9 skipped, 1 xfailed)
        - ✅ All 4 litellm cleanup tests passing
        - ✅ Confirmed working with Python 3.12.12
        - ✅ Works across pytest-xdist workers (8 workers tested)
        - ✅ No resource leaks detected

        Impact:
        - BEFORE: 9 RuntimeWarnings per regression test run
        - AFTER: 0 RuntimeWarnings (100% elimination)
        <Class TestLitellmCleanupWarnings>
          Test suite for litellm async client cleanup regression.
          <Coroutine test_litellm_acompletion_cleanup_no_warning>
            Verify that using litellm's acompletion does not produce cleanup warnings.

            This test:
            1. Uses litellm.acompletion (creates async HTTP clients)
            2. Verifies cleanup happens properly (no RuntimeWarning)
            3. Ensures event loop closes cleanly

            Status (GREEN - PASSING):
            - ✅ No RuntimeWarnings detected
            - ✅ Clean async cleanup via monkey-patched atexit.register
            - ✅ Manual cleanup in pytest_sessionfinish ensures proper resource cleanup
          <Function test_litellm_sync_completion_cleanup_no_warning>
            Verify that using litellm's sync completion does not produce cleanup warnings.

            Synchronous calls should not create async cleanup issues, but we test
            to ensure no regression.
          <Coroutine test_litellm_cleanup_fixture_integration>
            Test that the cleanup_litellm_clients fixture properly cleans up async clients.

            This test:
            1. Uses the autouse fixture (cleanup_litellm_clients from conftest.py)
            2. Makes async LLM calls
            3. Verifies cleanup happens automatically

            Expected behavior:
            - Fixture ensures all litellm async clients are closed
            - No manual cleanup needed in tests
            - No RuntimeWarning during teardown
          <Coroutine test_multiple_litellm_calls_cleanup>
            Test that multiple litellm calls in sequence don't accumulate unclosed clients.

            This simulates realistic test scenarios where multiple LLM calls are made.
      <Module test_performance_regression.py>
        Performance Regression Tests

        Tracks performance over time and alerts on regressions.
        Metrics are compared against baseline_metrics.json.
        <Class TestAgentPerformance>
          Performance regression tests for agent
          <Function test_agent_response_time_p95>
            Agent response p95 should be under 5 seconds.

            FIXED: Now uses SerializableLLMMock which is msgpack-compatible.
          <Function test_message_formatting_performance>
            Message formatting should be fast (<2ms p95)
        <Class TestAuthPerformance>
          Performance regression tests for auth/authz
          <Coroutine test_jwt_validation_performance>
            JWT validation should be very fast (<2ms p95)
          <Coroutine test_authorization_check_performance>
            Authorization checks should be fast (<50ms p95)
        <Class TestLLMPerformance>
          Performance regression tests for LLM calls
          <Coroutine test_llm_call_latency>
            LLM calls should complete reasonably fast
        <Class TestRegressionReporting>
          Tests for regression detection and reporting
          <Function test_regression_detection_threshold_exceeded>
            Regression detected when threshold exceeded
          <Function test_regression_detection_significant_slowdown>
            Regression detected when slowdown exceeds alert percentage
          <Function test_no_regression_within_baseline>
            No regression when within baseline
          <Function test_improvement_detected>
            Performance improvements are recorded
        <Class TestBaselineMetrics>
          Tests for baseline metrics structure
          <Function test_baseline_metrics_loadable>
            Baseline metrics file should be valid JSON
          <Function test_all_metrics_have_required_fields>
            All metrics should have required fields
          <Function test_thresholds_are_reasonable>
            Thresholds should be >= p99
      <Module test_precommit_hook_dependencies.py>
        Regression Tests for Pre-commit Hook Dependencies

        Prevents recurrence of pre-commit hook dependency failures from 2025-11-12:
        - validate-workflow-test-deps hook failed with "ModuleNotFoundError: No module named 'yaml'"
        - Hook was using language: system instead of language: python
        - Missing pyyaml in additional_dependencies

        ## Failure Scenario (2025-11-12)
        - Pre-commit hook: validate-workflow-test-deps
        - Entry point: python3 scripts/validation/validate_workflow_test_deps.py
        - Error: "ModuleNotFoundError: No module named 'yaml'" (line 30: import yaml)
        - Root cause: Hook used `language: system` assuming system Python has pyyaml
        - Fix: Changed to `language: python` with `additional_dependencies: ['pyyaml>=6.0.0']`

        ## Prevention Strategy
        1. Test all pre-commit hooks have proper dependencies declared
        2. Verify Python hooks use language: python (not system)
        3. Ensure scripts' imports match hook dependencies
        4. Validate hook configuration is consistent

        ## Related
        - Similar to test_dev_dependencies.py (validates package dependencies)
        - This focuses on pre-commit hook infrastructure dependencies
        <Class TestPreCommitHookDependencies>
          Regression tests for pre-commit hook dependency configuration.

          Prevents recurrence of hook dependency failures.
          <Function test_python_hooks_use_language_python_not_system>
            Test: Python hooks should use language: python, not language: system.

            RED (Before Fix - 2025-11-12):
            - validate-workflow-test-deps hook used language: system
            - Assumed system Python had pyyaml installed
            - Failed in CI environment where pyyaml wasn't available

            GREEN (After Fix - 2025-11-12):
            - Changed to language: python
            - Added additional_dependencies: ['pyyaml>=6.0.0']
            - Pre-commit creates isolated environment with dependencies

            REFACTOR:
            - This test ensures all Python hooks use proper configuration
            - Catches any new hooks that might use language: system incorrectly

            EXCEPTION (2025-11-13):
            - Hooks using `uv run` or `bash -c '... source .venv ...'` SHOULD use language: system
            - uv/venv manage their own dependencies, not pre-commit's isolated environment
            - This provides better integration with project dependencies
          <Function test_python_script_imports_match_hook_dependencies>
            Test: Python scripts referenced by hooks must have their imports satisfied.

            Ensures all imports in hook entry scripts are covered by additional_dependencies.

            RED (Before Fix - 2025-11-12):
            - scripts/validation/validate_workflow_test_deps.py imported yaml
            - Hook had no additional_dependencies
            - Runtime error: ModuleNotFoundError

            GREEN (After Fix - 2025-11-12):
            - Hook added additional_dependencies: ['pyyaml>=6.0.0']
            - yaml import satisfied by pyyaml package

            REFACTOR:
            - This test validates import/dependency alignment
            - Catches mismatches before CI runs
          <Function test_validate_workflow_test_deps_hook_has_pyyaml>
            Test: validate-workflow-test-deps hook must have pyyaml dependency.

            Specific regression test for the exact issue from 2025-11-12.

            RED (Before Fix):
            - Hook had no additional_dependencies
            - Script imported yaml at line 30
            - ModuleNotFoundError in CI

            GREEN (After Fix):
            - Hook has additional_dependencies: ['pyyaml>=6.0.0']
            - yaml import satisfied

            REFACTOR:
            - Permanent regression check
            - Fails immediately if dependency removed
          <Function test_validate_workflow_test_deps_script_imports_yaml>
            Test: Verify the script actually imports yaml (validation that test is correct).

            This ensures our regression test is testing the right thing.
        <Class TestPreCommitHookConfiguration>
          General pre-commit hook configuration best practices.
          <Function test_all_hooks_have_descriptions>
            Test: All hooks should have descriptions for maintainability.

            Helps developers understand what each hook does and why it exists.
          <Function test_python_hooks_specify_pass_filenames_explicitly>
            Test: Python hooks should explicitly set pass_filenames to avoid surprises.

            Makes hook behavior clear and intentional.
        <Function test_precommit_config_exists>
          Test: .pre-commit-config.yaml file exists in repository root.
        <Function test_pyyaml_in_dev_dependencies>
          Test: pyyaml should be in dev dependencies for local testing.

          While pre-commit installs it in isolated env, developers need it locally too.
      <Module test_pytest_xdist_environment_pollution.py>
        Regression tests for pytest-xdist environment pollution and dependency override leaks.

        PROBLEMS:
        ---------
        1. **Environment Pollution**: tests/integration/test_gdpr_endpoints.py:44-45
           - Direct os.environ mutation without cleanup
           - Environment variables leak to other tests in xdist workers

        2. **Dependency Override Leaks**: Multiple files
           - tests/integration/test_gdpr_endpoints.py:57 - No cleanup
           - tests/test_gdpr.py:397-399 - Sync lambdas for async deps + no cleanup
           - Missing bearer_scheme override causes singleton pollution

        3. **Environment Hygiene**: tests/unit/core/test_cache_isolation.py:27
           - Direct os.environ mutation in test (should use monkeypatch)

        SOLUTIONS:
        ----------
        1. Use monkeypatch.setenv() instead of direct os.environ assignment
        2. Add app.dependency_overrides.clear() in fixture teardown
        3. Use async overrides for async dependencies
        4. Override bearer_scheme when overriding get_current_user

        This test demonstrates:
        1. ❌ Current implementation: Environment and overrides leak
        2. ✅ Fixed implementation: Proper cleanup and isolation

        References:
        -----------
        - OpenAI Codex Findings: test_gdpr_endpoints.py:40, test_gdpr.py:397
        - PYTEST_XDIST_BEST_PRACTICES.md: Dependency override cleanup
        - Commit 079e82e: Fixed async/sync override mismatch
        <Class TestEnvironmentPollution>
          Tests demonstrating environment variable pollution issues.
          <Function test_direct_environ_mutation_pollutes_other_tests>
            🔴 RED: Demonstrate that direct os.environ mutation leaks to other tests.

            This test sets an environment variable directly. In pytest-xdist, this
            will pollute other tests running in the same worker process.

            This test PASSES now (detecting pollution) and should FAIL after fix
            (when proper cleanup is implemented).
          <Function test_monkeypatch_provides_automatic_cleanup>
            🟢 GREEN: Demonstrate that monkeypatch provides automatic cleanup.

            This test shows the CORRECT pattern using monkeypatch.setenv().
            Environment variables are automatically restored after the test.

            This test PASSES now and continues to PASS after fix.
          <Function test_environment_restored_after_monkeypatch_test>
            🟢 GREEN: Verify that environment is clean after monkeypatch test.

            This test runs AFTER test_monkeypatch_provides_automatic_cleanup.
            If monkeypatch worked correctly, environment should be clean.

            This test PASSES both before and after fix.
        <Class TestDependencyOverrideLeaks>
          Tests demonstrating FastAPI dependency override leak issues.
          <Function test_dependency_overrides_without_cleanup_leak>
            🔴 RED: Demonstrate that dependency overrides without cleanup leak.

            This simulates tests/integration/test_gdpr_endpoints.py:57 pattern.
            Overrides are set but never cleared, leaking to subsequent tests.

            This test PASSES now (detecting leak) and should FAIL after fix.
          <Function test_sync_lambda_for_async_dependency_is_incorrect>
            🔴 RED: Demonstrate that sync lambdas for async deps are incorrect.

            This simulates tests/test_gdpr.py:397-399 pattern.
            Uses sync lambda for async dependency, which causes issues in xdist.

            This test PASSES now (detecting incorrect pattern) and should FAIL after fix.
          <Function test_async_override_for_async_dependency_is_correct>
            🟢 GREEN: Demonstrate correct async override pattern.

            This shows the CORRECT pattern: async function for async dependency.

            This test will FAIL initially (showing incorrect pattern in code)
            and PASS after fix.
          <Function test_bearer_scheme_override_is_required>
            🔴 RED: Demonstrate that bearer_scheme override is required.

            Per PYTEST_XDIST_BEST_PRACTICES.md, when overriding get_current_user,
            must also override bearer_scheme to prevent singleton pollution.

            This test will FAIL initially (bearer_scheme not overridden in test files)
            and PASS after fix.
          <Function test_complete_correct_override_pattern>
            🟢 GREEN: Demonstrate the complete correct override pattern.

            This test shows ALL the requirements:
            1. ✅ Async override for async dependency
            2. ✅ bearer_scheme override
            3. ✅ Cleanup with app.dependency_overrides.clear()

            This is the pattern that test_gdpr_endpoints.py should follow.
        <Class TestEnvironmentHygiene>
          Tests for environment variable hygiene in individual tests.
          <Function test_cache_isolation_should_use_monkeypatch>
            🔴 RED: Demonstrate test_cache_isolation.py:27 pattern issue.

            The test directly mutates os.environ without cleanup.
            Should use monkeypatch instead.

            This test PASSES now (detecting issue) and should FAIL after fix.
      <Module test_pytest_xdist_isolation.py>
        Regression Tests for Pytest-xdist Test Isolation

        These tests validate that FastAPI dependency overrides work correctly
        in parallel test execution (pytest-xdist). They prevent regression of
        the bug fixed in commit 079e82e where async dependency overrides were
        not properly handled, causing intermittent 401 authentication failures.

        See: MEMORY_SAFETY_GUIDELINES.md and pytest-xdist documentation.

        Key Learnings:
        1. Async dependencies MUST be overridden with async functions
        2. Sync dependencies MUST be overridden with sync functions
        3. Mixing async/sync causes FastAPI to ignore the override
        4. Use @pytest.mark.xdist_group to group related tests in same worker
        5. Always clear dependency_overrides in fixture teardown
        <Class TestPytestXdistIsolation>
          Regression tests for pytest-xdist test isolation.

          These tests validate the fix for the async/sync dependency override bug.
          <Function test_async_dependency_override_with_async_function>
            ✅ CORRECT: Override async dependency with async function.

            This is the correct pattern and should work in pytest-xdist.
          <Function test_async_dependency_override_with_sync_lambda_documentation>
            ❌ INCORRECT PATTERN: Override async dependency with sync lambda.

            This is the BUG that was fixed in commit 079e82e.

            IMPORTANT: This bug only manifests in pytest-xdist parallel execution!
            - When run in isolation (pytest -xvs): Works correctly (200 OK)
            - When run in parallel (pytest -n auto): Fails intermittently (401)

            The intermittent nature makes it hard to test directly, but this
            test documents the incorrect pattern for future reference.

            Always use async function to override async dependency!
          <Function test_sync_dependency_override_with_sync_function>
            ✅ CORRECT: Override sync dependency with sync function.

            This is the correct pattern for sync dependencies.
          <Function test_dependency_override_cleanup_prevents_pollution>
            Test that dependency_overrides.clear() prevents state pollution.

            Without cleanup, overrides from one test could leak to another test
            in the same pytest-xdist worker, causing intermittent failures.
          <Function test_xdist_group_marker_keeps_tests_in_same_worker>
            DOCUMENTATION TEST: Documents @pytest.mark.xdist_group marker behavior.

            **WARNING**: This is an INERT test that only documents expected behavior.
            It cannot fail on regression because it always passes (assert True).

            All tests in this class should run in the same worker because
            they have the same xdist_group marker.

            This is important for tests that share state or fixtures.

            **TODO**: Convert to executable test that parses pytest-xdist output.
            Proper implementation would:
            1. Run pytest with -n 2 in subprocess
            2. Parse output for [gw#] worker identifiers
            3. Verify all tests in same xdist_group run on same worker
            4. Use pytest's pytester plugin for subprocess execution

            References:
            - OpenAI Codex Finding: "Regression tests are inert documentation"
            - pytest-xdist documentation: Worker assignment and grouping
        <Class TestFastAPIPatterns>
          Test real FastAPI patterns used in the codebase.

          These tests validate the patterns used in:
          - tests/api/test_api_keys_endpoints.py
          - tests/api/test_service_principals_endpoints.py
          <Function test_api_keys_pattern_works_in_xdist>
            Test the exact pattern used in test_api_keys_endpoints.py.

            This validates the fix for the TestCreateAPIKey failures.
        <Function test_dependency_override_documentation>
          Documentation test that serves as a reference for correct patterns.

          ✅ DO THIS:
          ---------
          async def get_user():
              # ... async code ...

          async def mock_user():
              return {"user_id": "test"}

          app.dependency_overrides[get_user] = mock_user  # ✅ Async -> Async


          ❌ DON'T DO THIS:
          ----------------
          async def get_user():
              # ... async code ...

          app.dependency_overrides[get_user] = lambda: {"user_id": "test"}  # ❌ Async -> Sync


          ALWAYS CLEANUP:
          --------------
          try:
              # ... test code ...
          finally:
              app.dependency_overrides.clear()  # ✅ Always clear

          OR use yield in fixture:
          -----------------------
          @pytest.fixture
          def client():
              app.dependency_overrides[dep] = mock
              yield TestClient(app)
              app.dependency_overrides.clear()  # ✅ Cleanup after yield
      <Module test_pytest_xdist_port_conflicts.py>
        Regression tests for pytest-xdist port allocation in test infrastructure.

        ORIGINAL PROBLEM (Codex Finding):
        -----------------------------------
        The test_infrastructure_ports fixture (conftest.py:582) calculated worker-specific
        port offsets (gw0=+0, gw1=+100, etc.), but docker-compose.test.yml used fixed ports.
        This caused a mismatch where:
        - test_infrastructure_ports returned 9432 (gw0) or 9532 (gw1)
        - docker-compose always bound to 9432
        - Workers gw1+ would wait for port 9532, but nothing listened there

        ARCHITECTURAL DECISION:
        -----------------------
        We chose SINGLE SHARED INFRASTRUCTURE over per-worker infrastructure:
        - ONE docker-compose instance on FIXED ports (9432, 9379, 9080, etc.)
        - ALL workers connect to the SAME ports
        - Isolation via logical mechanisms (PostgreSQL schemas, Redis DBs, OpenFGA stores)

        This approach is:
        ✅ Simpler (no dynamic port allocation or env-var templating)
        ✅ Faster (single infrastructure startup, not N instances)
        ✅ More reliable (matches existing database isolation patterns)
        ✅ Session-scoped (aligns with session-scoped fixtures)

        TESTS:
        ------
        These tests validate the CORRECT behavior (fixed ports with logical isolation)
        and prevent regression to broken multi-instance attempts.

        References:
        -----------
        - OpenAI Codex Finding: conftest.py:583 port conflicts (RESOLVED)
        - tests/meta/test_infrastructure_singleton.py (validates architecture)
        - ADR: Single shared test infrastructure with logical isolation
        <Function test_ports_are_intentionally_fixed>
          ✅ Validate that ports are INTENTIONALLY fixed (not a bug).

          All workers should use the same base ports, connecting to a single
          shared infrastructure instance. This is BY DESIGN, not a problem.
        <Function test_all_workers_share_same_ports>
          ✅ Validate that ALL workers use the same ports (single shared infrastructure).

          This is the architectural decision: ONE infrastructure instance,
          shared across all workers, with logical isolation.
        <Function test_no_worker_based_port_offsets>
          ✅ Validate that there are NO worker-based port offsets.

          The old (broken) approach calculated port_offset = worker_num * 100.
          The new (correct) approach uses fixed ports for all workers.

          This test prevents regression to the broken multi-instance pattern.
        <Function test_ports_match_docker_compose>
          ✅ Validate that test_infrastructure_ports matches docker-compose.test.yml.

          Ensures consistency between the fixture and actual infrastructure configuration.
        <Function test_logical_isolation_mechanisms_exist>
          ✅ Validate that logical isolation mechanisms exist for shared infrastructure.

          Since all workers share the same ports, they MUST use logical isolation:
          - PostgreSQL schemas (test_worker_gw0, test_worker_gw1, ...)
          - Redis DB indices (1, 2, 3, ...)
          - OpenFGA stores (test_store_gw0, test_store_gw1, ...)

          This test validates that the infrastructure supports these mechanisms.
        <Function test_fixture_documentation_explains_architecture>
          ✅ Validate that the fixture documents the single-instance architecture.

          The fixture should have a docstring explaining why ports are fixed
          and how isolation is achieved.
        <Function test_regression_documentation>
          📚 Document the regression and its resolution.

          This test serves as living documentation for the Codex finding
          and the architectural decision made to resolve it.
      <Module test_pytest_xdist_worker_database_isolation.py>
        Regression tests for pytest-xdist worker database isolation issues.

        PROBLEMS:
        ---------
        The "clean" wrapper fixtures in conftest.py wrap session-scoped resources
        but clean up with operations that affect ALL workers:

        1. **postgres_connection_clean** (conftest.py:1042)
           - Uses TRUNCATE TABLE on shared connection
           - Worker A's TRUNCATE wipes Worker B's in-progress test data
           - Race conditions and flaky tests

        2. **redis_client_clean** (conftest.py:1092)
           - Uses FLUSHDB on shared Redis instance
           - Worker A's flushdb() wipes Worker B's test data
           - Causes intermittent failures

        3. **openfga_client_clean** (conftest.py:1116)
           - Deletes tuples from shared OpenFGA store
           - Worker A's deletion affects Worker B's tests
           - Authorization test failures

        SOLUTIONS:
        ----------
        1. **Postgres**: Worker-scoped schemas
           - Worker gw0: CREATE SCHEMA test_worker_gw0; SET search_path TO test_worker_gw0;
           - Worker gw1: CREATE SCHEMA test_worker_gw1; SET search_path TO test_worker_gw1;
           - Cleanup: DROP SCHEMA {worker_schema} CASCADE;

        2. **Redis**: Worker-scoped DB indexes
           - Worker gw0: SELECT 1 (DB index 1)
           - Worker gw1: SELECT 2 (DB index 2)
           - Worker gw2: SELECT 3 (DB index 3)
           - Cleanup: FLUSHDB (safe because each worker has its own DB)

        3. **OpenFGA**: Worker-scoped store names
           - Worker gw0: store name = "test_store_gw0"
           - Worker gw1: store name = "test_store_gw1"
           - Cleanup: Delete worker-specific store

        This test demonstrates:
        1. ❌ Current implementation: Shared resources cause conflicts
        2. ✅ Fixed implementation: Worker-scoped isolation prevents conflicts

        References:
        -----------
        - OpenAI Codex Findings: conftest.py:1042, 1092, 1116
        - PYTEST_XDIST_BEST_PRACTICES.md: Worker isolation patterns
        <Class TestPostgresWorkerIsolation>
          Tests demonstrating PostgreSQL worker isolation issues.
          <Function test_worker_scoped_schemas_would_provide_isolation>
            🟢 GREEN: Test that worker-scoped schemas provide proper isolation.

            This test will FAIL initially (worker schemas not implemented)
            and PASS after implementation.

            Expected behavior:
            - Worker gw0: Uses schema test_worker_gw0
            - Worker gw1: Uses schema test_worker_gw1
            - TRUNCATE in gw0's schema doesn't affect gw1's schema
          <Function test_worker_schema_naming_convention[gw0-test_worker_gw0]>
            🟢 GREEN: Test worker schema naming convention.

            This test will FAIL initially and PASS after implementation.

            Convention: test_worker_{worker_id}
          <Function test_worker_schema_naming_convention[gw1-test_worker_gw1]>
            🟢 GREEN: Test worker schema naming convention.

            This test will FAIL initially and PASS after implementation.

            Convention: test_worker_{worker_id}
          <Function test_worker_schema_naming_convention[gw2-test_worker_gw2]>
            🟢 GREEN: Test worker schema naming convention.

            This test will FAIL initially and PASS after implementation.

            Convention: test_worker_{worker_id}
          <Function test_worker_schema_naming_convention[gw3-test_worker_gw3]>
            🟢 GREEN: Test worker schema naming convention.

            This test will FAIL initially and PASS after implementation.

            Convention: test_worker_{worker_id}
        <Class TestRedisWorkerIsolation>
          Tests demonstrating Redis worker isolation issues.
          <Function test_worker_scoped_redis_databases_would_provide_isolation>
            🟢 GREEN: Test that worker-scoped Redis DBs provide proper isolation.

            This test will FAIL initially and PASS after implementation.

            Expected behavior:
            - Worker gw0: Uses Redis DB 1
            - Worker gw1: Uses Redis DB 2
            - Worker gw2: Uses Redis DB 3
            - FLUSHDB in one DB doesn't affect other DBs
          <Function test_worker_redis_db_index_calculation[gw0-1]>
            🟢 GREEN: Test worker Redis DB index calculation.

            This test will FAIL initially and PASS after implementation.

            Formula: db_index = worker_num + 1
            - gw0 → worker_num=0 → DB 1
            - gw1 → worker_num=1 → DB 2
            - gw2 → worker_num=2 → DB 3

            Note: DB 0 is reserved for non-xdist usage.
          <Function test_worker_redis_db_index_calculation[gw1-2]>
            🟢 GREEN: Test worker Redis DB index calculation.

            This test will FAIL initially and PASS after implementation.

            Formula: db_index = worker_num + 1
            - gw0 → worker_num=0 → DB 1
            - gw1 → worker_num=1 → DB 2
            - gw2 → worker_num=2 → DB 3

            Note: DB 0 is reserved for non-xdist usage.
          <Function test_worker_redis_db_index_calculation[gw2-3]>
            🟢 GREEN: Test worker Redis DB index calculation.

            This test will FAIL initially and PASS after implementation.

            Formula: db_index = worker_num + 1
            - gw0 → worker_num=0 → DB 1
            - gw1 → worker_num=1 → DB 2
            - gw2 → worker_num=2 → DB 3

            Note: DB 0 is reserved for non-xdist usage.
          <Function test_worker_redis_db_index_calculation[gw3-4]>
            🟢 GREEN: Test worker Redis DB index calculation.

            This test will FAIL initially and PASS after implementation.

            Formula: db_index = worker_num + 1
            - gw0 → worker_num=0 → DB 1
            - gw1 → worker_num=1 → DB 2
            - gw2 → worker_num=2 → DB 3

            Note: DB 0 is reserved for non-xdist usage.
          <Function test_worker_redis_db_index_calculation[gw4-5]>
            🟢 GREEN: Test worker Redis DB index calculation.

            This test will FAIL initially and PASS after implementation.

            Formula: db_index = worker_num + 1
            - gw0 → worker_num=0 → DB 1
            - gw1 → worker_num=1 → DB 2
            - gw2 → worker_num=2 → DB 3

            Note: DB 0 is reserved for non-xdist usage.
          <Function test_redis_has_enough_databases_for_workers>
            🟢 GREEN: Verify Redis has enough databases for expected workers.

            Redis default configuration has 16 databases (0-15).
            This allows for 15 xdist workers (DB 1-15, with DB 0 reserved).

            This test PASSES both before and after fix.
        <Class TestOpenFGAWorkerIsolation>
          Tests demonstrating OpenFGA worker isolation issues.
          <Function test_worker_scoped_openfga_stores_would_provide_isolation>
            🟢 GREEN: Test that worker-scoped stores provide proper isolation.

            This test will FAIL initially and PASS after implementation.

            Expected behavior:
            - Worker gw0: Uses store "test_store_gw0"
            - Worker gw1: Uses store "test_store_gw1"
            - Tuple deletion in one store doesn't affect other stores
          <Function test_worker_openfga_store_naming_convention[gw0-test_store_gw0]>
            🟢 GREEN: Test worker OpenFGA store naming convention.

            This test will FAIL initially and PASS after implementation.

            Convention: test_store_{worker_id}
          <Function test_worker_openfga_store_naming_convention[gw1-test_store_gw1]>
            🟢 GREEN: Test worker OpenFGA store naming convention.

            This test will FAIL initially and PASS after implementation.

            Convention: test_store_{worker_id}
          <Function test_worker_openfga_store_naming_convention[gw2-test_store_gw2]>
            🟢 GREEN: Test worker OpenFGA store naming convention.

            This test will FAIL initially and PASS after implementation.

            Convention: test_store_{worker_id}
          <Function test_worker_openfga_store_naming_convention[gw3-test_store_gw3]>
            🟢 GREEN: Test worker OpenFGA store naming convention.

            This test will FAIL initially and PASS after implementation.

            Convention: test_store_{worker_id}
        <Class TestWorkerIsolationIntegration>
          Integration tests for complete worker isolation pattern.
          <Function test_all_backends_should_be_worker_scoped>
            🟢 GREEN: Test that all three backends use worker isolation.

            This is an integration test that validates the complete solution.
            All three "clean" fixtures should use worker-scoped resources.

            This test will FAIL initially and PASS after all fixes are implemented.
          <Function test_cleanup_operations_are_safe_with_worker_isolation>
            🟢 GREEN: Test that cleanup operations are safe with worker isolation.

            After implementing worker-scoped resources, cleanup operations
            that were previously dangerous become safe:

            - TRUNCATE TABLE: Safe (only affects worker's schema)
            - FLUSHDB: Safe (only affects worker's Redis DB)
            - Delete tuples: Safe (only affects worker's OpenFGA store)

            This test will FAIL initially and PASS after implementation.
      <Module test_service_principal_test_isolation.py>
        Regression Tests for Service Principal Test Isolation in pytest-xdist

        Prevents recurrence of 401 Unauthorized errors in service principal unit tests
        when running with pytest-xdist parallel execution.

        ## Failure Scenario (2025-11-12)
        - Test: test_create_service_principal_success
        - Expected: 201 Created
        - Actual: 401 Unauthorized
        - Root cause: FastAPI dependency override state pollution in pytest-xdist workers
        - Context: Tests use TestClient with dependency_overrides for mocking

        ## Technical Details

        ### Problem
        1. FastAPI app instances share module-level state across test workers
        2. bearer_scheme (HTTPBearer) is a module-level singleton
        3. dependency_overrides dict can pollute across parallel test execution
        4. Mock state accumulates without proper cleanup between tests

        ### Symptoms
        - Tests pass when run sequentially
        - Tests fail intermittently with 401 when run in parallel (-n auto)
        - Failures are non-deterministic (different tests fail on different runs)
        - Error message: "assert 401 == 201" (Unauthorized instead of Created)

        ### Fix Strategy
        1. Create fresh FastAPI app instance per test (scope="function")
        2. Override bearer_scheme to bypass authentication
        3. Clear dependency_overrides after each test
        4. Force garbage collection to prevent mock accumulation

        ## Prevention Strategy
        These tests validate that the isolation mechanisms are in place and working.
        <Class TestServicePrincipalTestIsolation>
          Regression tests for pytest-xdist state isolation in service principal tests.

          Validates that the fix from 2025-11-12 prevents 401 errors in parallel execution.
          <Function test_bearer_scheme_override_prevents_401>
            Test: Overriding bearer_scheme allows tests to bypass authentication.

            RED (Before Fix - 2025-11-12):
            - TestClient requests required actual bearer tokens
            - Mock authentication not working in parallel execution
            - Tests failed with 401 Unauthorized

            GREEN (After Fix - 2025-11-12):
            - bearer_scheme overridden to return None
            - Authentication bypassed in test environment
            - Mock current_user injected directly

            REFACTOR:
            - This test validates the fix stays in place
            - Ensures bearer override pattern is working
          <Function test_fresh_app_instance_per_test_prevents_pollution>
            Test: Each test gets fresh FastAPI app instance (scope="function").

            RED (Before Fix):
            - App instances shared across tests
            - dependency_overrides accumulated
            - Tests interfered with each other

            GREEN (After Fix):
            - Each test creates fresh app instance
            - Clean slate for dependency_overrides
            - No cross-test interference

            REFACTOR:
            - Validates fixture scoping is correct
            - This test simulates multiple test runs
          <Function test_dependency_overrides_cleared_after_test>
            Test: dependency_overrides are cleared in fixture cleanup.

            Ensures no state pollution between tests.
          <Function test_garbage_collection_prevents_mock_accumulation>
            Test: Garbage collection prevents mock object accumulation.

            **NOTE**: This test validates the GC pattern exists but doesn't measure
            actual memory cleanup (CPython's reference counting makes this difficult).

            RED (Before Fix):
            - Mock objects accumulated in memory
            - AsyncMock instances not cleaned up
            - Memory usage grew with parallel test execution

            GREEN (After Fix):
            - gc.collect() called in teardown_method
            - Mocks properly cleaned up
            - Stable memory usage

            REFACTOR:
            - Validates gc pattern is in place

            **TODO**: For executable validation, use pytest-memray or memory_profiler
            to measure actual memory usage before/after gc.collect().

            References:
            - MEMORY_SAFETY_GUIDELINES.md: Documents the GC pattern
            - OpenAI Codex Finding: Tests should validate behavior, not just patterns
        <Class TestServicePrincipalFixtureConfiguration>
          Tests for service principal fixture configuration best practices.

          Validates fixtures follow the patterns that prevent pytest-xdist issues.
          <Function test_sp_test_client_fixture_exists_in_test_file>
            Test: Service principal tests have sp_test_client fixture.

            Validates the fixture that implements the fix is present.
          <Function test_service_principal_tests_use_function_scoped_fixture>
            Test: Service principal tests use function-scoped fixtures.

            Ensures tests don't share app instances across parallel execution.
          <Function test_teardown_method_includes_gc_collect>
            Test: Test classes have teardown_method with gc.collect().

            Prevents mock accumulation in pytest-xdist workers.
        <Function test_service_principal_tests_have_xdist_groups>
          Test: Service principal tests use xdist_group marker for isolation.

          Prevents concurrent execution of tests that might interfere.
      <Module test_terraform_configuration.py>
        Regression Tests for Terraform Configuration

        Prevents recurrence of Terraform validation failures from 2025-11-12:
        1. Duplicate required_providers blocks
        2. Invalid cross-variable references in validations
        3. Provider version constraint conflicts
        4. Terraform version incompatibilities

        ## Failure Scenarios (2025-11-12)

        ### Issue 1: Duplicate required_providers
        - File: terraform/backend-setup-gcp/main.tf
        - Error: "Duplicate required providers configuration"
        - Two separate terraform{} blocks with required_providers
        - Fix: Merged into single block

        ### Issue 2: Invalid variable validation cross-references
        - Files: terraform/backend-setup-gcp/variables.tf, terraform/modules/eks/variables.tf
        - Error: "Invalid reference in variable validation"
        - Validations referenced other variables (var.enable_cmek, var.environment)
        - Fix: Removed cross-variable checks (Terraform limitation)

        ### Issue 3: Provider version conflicts
        - File: terraform/modules/github-actions-wif/versions.tf
        - Error: "no available releases match the given constraints ~> 5.0, ~> 6.0"
        - Module required ~> 5.0, parent required ~> 6.0
        - Fix: Aligned to ~> 6.0

        ### Issue 4: Terraform version incompatibility
        - File: terraform/modules/azure-database/versions.tf
        - Error: "This configuration does not support Terraform version 1.6.6"
        - Required >= 1.7, CI had 1.6.6
        - Fix: Lowered to >= 1.5

        ## Prevention Strategy
        These tests validate Terraform configurations are properly structured
        and compatible across all modules and environments.
        <Class TestTerraformDuplicateBlocks>
          Regression tests for duplicate Terraform configuration blocks.

          Prevents recurrence of duplicate required_providers issue.
          <Function test_no_duplicate_terraform_blocks_in_same_file>
            Test: No file should have multiple terraform{} blocks.

            RED (Before Fix - 2025-11-12):
            - terraform/backend-setup-gcp/main.tf had 2 terraform{} blocks
            - Line 16-25: First block with google provider
            - Line 167-174: Second block with random provider
            - Error: "Duplicate required providers configuration"

            GREEN (After Fix - 2025-11-12):
            - Merged into single terraform{} block with both providers
            - All providers declared in one required_providers section

            REFACTOR:
            - This test prevents regression
            - Checks all .tf files for duplicate terraform{} blocks

            REFACTOR (2025-11-14 - Codex finding):
            - Replaced naive regex with HCL parser to avoid false positives
            - Previous regex matched "terraform {" in heredoc strings (outputs.tf)
            - HCL parser correctly identifies actual terraform configuration blocks only
          <Function test_no_duplicate_required_providers_blocks>
            Test: No terraform{} block should have multiple required_providers{} sections.

            Ensures providers are all declared in one place.
        <Class TestTerraformVariableValidations>
          Regression tests for Terraform variable validations.

          Prevents cross-variable references in validation conditions.
          <Function test_variable_validations_only_reference_self>
            Test: Variable validation conditions must only reference the variable itself.

            RED (Before Fix - 2025-11-12):
            - backend-setup-gcp/variables.tf line 63:
              condition = !var.enable_cmek || (var.enable_cmek && var.kms_key_name != "")
              Referenced var.enable_cmek in kms_key_name validation
            - modules/eks/variables.tf line 81:
              condition = !contains(var.cluster_endpoint_public_access_cidrs, "0.0.0.0/0") || var.environment == "dev"
              Referenced var.environment in cluster_endpoint validation

            GREEN (After Fix - 2025-11-12):
            - Removed cross-variable validations
            - Added comments to move validation to resource-level preconditions
            - Terraform only allows variable.validation to reference itself

            REFACTOR:
            - This test catches any new cross-variable validation attempts
        <Class TestTerraformProviderVersions>
          Regression tests for provider version constraints.

          Ensures compatible provider versions across modules and environments.
          <Function test_no_conflicting_google_provider_versions>
            Test: Google provider versions must be compatible across modules.

            RED (Before Fix - 2025-11-12):
            - modules/github-actions-wif/versions.tf: version = "~> 5.0"
            - environments/gcp-staging-wif-only/main.tf: version = "~> 6.0"
            - Error: "no available releases match the given constraints ~> 5.0, ~> 6.0"

            GREEN (After Fix - 2025-11-12):
            - All updated to ~> 6.0
            - Compatible versions across all usages

            REFACTOR:
            - Test ensures versions stay aligned
            - Allows flexibility for minor version bumps
          <Function test_terraform_version_compatible_with_ci>
            Test: Terraform version requirements must be compatible with CI environment.

            RED (Before Fix - 2025-11-12):
            - modules/azure-database/versions.tf: required_version = ">= 1.7"
            - CI environment: Terraform 1.6.6
            - Error: "This configuration does not support Terraform version 1.6.6"

            GREEN (After Fix - 2025-11-12):
            - Updated to required_version = ">= 1.5"
            - Compatible with CI environment

            REFACTOR:
            - Test ensures version requirements stay compatible
            - Assumes CI runs Terraform >= 1.5 and < 2.0
        <Function test_terraform_validate_script_exists>
          Test: Terraform validation is part of CI pipeline.

          Ensures the terraform validate check runs in CI to catch these issues early.
      <Module test_uv_lockfile_sync.py>
        Regression Test: UV Lockfile Synchronization

        REGRESSION INCIDENT: Commit 67f8942 (2025-11-12)
        SEVERITY: CRITICAL - Blocked all CI/CD workflows

        ## What Happened

        Commit 67f8942 modified pyproject.toml to add pytest markers (lines 425-429)
        but didn't regenerate uv.lock, causing lockfile desynchronization.

        This caused ALL Python-based CI workflows to fail with:
        ```
        uv.lock is out of date with pyproject.toml
        Run 'uv lock' locally and commit the updated lockfile
        ```

        ## Impact

        - ❌ CI/CD Pipeline failed on Python 3.10, 3.11, 3.12
        - ❌ Quality Tests failed
        - ❌ E2E Tests failed
        - ❌ Coverage Trend Tracking failed
        - ❌ All development blocked for ~2 hours

        ## Root Cause

        Developer modified pyproject.toml without running `uv lock` to update uv.lock.

        ## Prevention Measures

        1. **Pre-commit Hook** (.pre-commit-config.yaml:74-91)
           - Runs `uv lock --check` on pyproject.toml or uv.lock changes
           - Fails commit if lockfile is out of sync
           - Prevents issue from reaching CI

        2. **CI Fail-Fast Check** (.github/workflows/ci.yaml:172-185)
           - Validates lockfile BEFORE running tests
           - Provides clear error message with remediation steps
           - Saves CI minutes by failing early

        3. **This Regression Test** (tests/regression/test_uv_lockfile_sync.py)
           - Documents the incident
           - Provides testable validation
           - Runs in CI to catch regressions

        ## Testing Strategy

        This test validates that:
        1. uv.lock exists
        2. uv.lock is in sync with pyproject.toml (uv lock --check passes)
        3. Pre-commit hook exists for lockfile validation
        4. CI workflow has lockfile validation step

        ## References

        - Incident Commit: 67f8942
        - Fix Commits: 709adda (test fixes), c193936 (CI Python version), ba5296f (uv sync --python)
        - CI Run: https://github.com/vishnu2kmohan/mcp-server-langgraph/actions/runs/19306810940
        <Class TestUVLockfileSynchronization>
          Regression tests for UV lockfile synchronization issues.

          Prevents recurrence of the 2025-11-12 incident where out-of-sync
          lockfile blocked all CI/CD workflows.
          <Function test_uv_lock_file_exists>
            Verify uv.lock exists in project root
          <Function test_uv_lockfile_is_synchronized_with_pyproject>
            CRITICAL: Verify uv.lock is in sync with pyproject.toml

            This is the EXACT check that failed in commit 67f8942.
            If this test fails, run: uv lock
          <Function test_pre_commit_hook_validates_lockfile>
            Verify pre-commit hook exists to catch lockfile sync issues
          <Function test_ci_workflow_validates_lockfile_before_tests>
            Verify CI workflow has fail-fast lockfile validation
          <Function test_lockfile_validation_error_message_is_helpful>
            Verify lockfile validation provides clear remediation steps
        <Class TestCIPythonVersionMatrix>
          Regression tests for CI Python version matrix configuration.

          Prevents recurrence of the issue where all matrix jobs ran Python 3.12
          instead of their configured versions (3.10, 3.11, 3.12).
          <Function test_ci_workflow_uses_uv_sync_with_python_flag>
            CRITICAL: Verify uv sync uses --python flag to prevent venv recreation

            REGRESSION: Without this flag, uv sync removed Python 3.11 venv and
            recreated it with Python 3.12, causing all matrix jobs to run 3.12.

            Evidence from CI logs:
            ```
            Using CPython 3.11.14 - Creating virtual environment at: .venv
            Using CPython 3.12.3 - Removed virtual environment at: .venv
            Creating virtual environment at: .venv
            ```
          <Function test_ci_workflow_verifies_python_version_after_install>
            Verify CI workflow checks Python version after venv creation
        <Class TestFastAPIDependencyOverridesPattern>
          Regression tests for FastAPI dependency mocking pattern.

          Documents the correct way to mock dependencies in FastAPI tests
          to avoid parameter name collisions and pytest-xdist issues.
          <Function test_service_principals_fixture_uses_dependency_overrides>
            Verify service principals tests use dependency_overrides pattern

            REGRESSION: Using monkeypatch + reload caused FastAPI parameter collision:
            - Endpoint: request: CreateServicePrincipalRequest
            - Mock: request: Request
            - FastAPI got confused → 422 error looking for 'request' in body

            FIX: Use app.dependency_overrides instead of monkeypatch
          <Function test_api_keys_fixture_uses_dependency_overrides>
            Verify API keys tests use dependency_overrides pattern
      <Module test_worker_safe_test_ids.py>
        Regression Test: Worker-Safe Test ID Generation

        This test validates that the worker-safe ID generation utilities prevent
        database record pollution when tests run in parallel with pytest-xdist.

        Root Cause Being Prevented:
        - Hardcoded user IDs like "user:alice" cause database constraint violations
          when multiple pytest-xdist workers create records simultaneously
        - Solution: Generate worker-scoped IDs that include worker ID

        Golden Rules Being Enforced:
        1. "Each test should be an island" - Worker-scoped IDs ensure test isolation
        2. "Deterministic test data" - Seed with worker ID for reproducibility
        3. "No shared state between tests" - Each worker gets unique namespace

        CRITICAL: This test must run BEFORE implementing the utilities (RED phase).
        Expected to FAIL initially, proving the test works correctly.

        After implementing utilities, test should PASS (GREEN phase).

        Reference: /tmp/pollution_audit_summary.md
        <Class TestWorkerSafeIDGeneration>
          Test suite for worker-safe ID generation utilities

          These utilities prevent database record pollution in pytest-xdist parallel execution
          by generating unique IDs per worker.
          <Function test_get_worker_scoped_user_id_with_default_worker>
            🔴 RED: Test worker-scoped user ID generation (default worker)

            When: No PYTEST_XDIST_WORKER env var is set (single-worker mode)
            Then: Should generate ID with default worker 'gw0'
          <Function test_get_worker_scoped_user_id_with_multiple_workers>
            🔴 RED: Test worker-scoped user ID generation (multiple workers)

            When: PYTEST_XDIST_WORKER is set to different worker IDs
            Then: Should generate unique IDs per worker
          <Function test_get_worker_scoped_user_id_uniqueness_across_workers>
            🔴 RED: Test that different workers generate different IDs for same user

            When: Multiple workers (gw0, gw1, gw2) all create "alice" user
            Then: Each worker gets a unique ID to prevent database conflicts
          <Function test_get_worker_scoped_email_with_default_worker>
            🔴 RED: Test worker-scoped email generation (default worker)

            When: No PYTEST_XDIST_WORKER env var is set
            Then: Should generate email with default worker 'gw0'
          <Function test_get_worker_scoped_email_with_multiple_workers>
            🔴 RED: Test worker-scoped email generation (multiple workers)

            When: PYTEST_XDIST_WORKER is set to different worker IDs
            Then: Should generate unique emails per worker
          <Function test_get_worker_scoped_service_principal_id_with_default_worker>
            🔴 RED: Test worker-scoped service principal ID generation

            When: No PYTEST_XDIST_WORKER env var is set
            Then: Should generate service principal ID with default worker 'gw0'
          <Function test_get_worker_scoped_service_principal_id_with_multiple_workers>
            🔴 RED: Test worker-scoped service principal ID generation (multiple workers)

            When: PYTEST_XDIST_WORKER is set to different worker IDs
            Then: Should generate unique service principal IDs per worker
          <Function test_worker_safe_utilities_prevent_database_conflicts>
            🔴 RED: Integration test - verify worker-safe IDs prevent collisions

            Scenario: Simulate 8 pytest-xdist workers all creating "alice" user simultaneously
            Expected: Each worker gets unique ID namespace, no database conflicts

            This is the CRITICAL test that validates the entire approach.
        <Function test_worker_safe_id_documentation>
          Meta-test: Verify that worker-safe ID utilities are properly documented

          This ensures future developers understand why these utilities exist
          and when to use them.
    <Package resilience>
      <Module test_bulkhead.py>
        Unit tests for bulkhead isolation pattern.

        Tests concurrency limiting, wait/fail-fast modes, and metrics.
        <Class TestBulkheadBasics>
          Test basic bulkhead functionality
          <Coroutine test_bulkhead_limits_concurrency>
            Test that bulkhead limits concurrent executions
          <Coroutine test_bulkhead_sequential_calls_not_limited>
            Test that sequential calls are not limited
        <Class TestBulkheadFailFast>
          Test bulkhead fail-fast mode
          <Coroutine test_fail_fast_rejects_when_full>
            Test that fail-fast mode rejects when bulkhead is full
          <Coroutine test_wait_mode_queues_requests>
            Test that wait mode queues requests instead of rejecting
        <Class TestBulkheadContextManager>
          Test bulkhead context manager
          <Coroutine test_bulkhead_context_manager>
            Test BulkheadContext as context manager
          <Coroutine test_bulkhead_context_fail_fast>
            Test BulkheadContext in fail-fast mode
        <Class TestBulkheadStatistics>
          Test bulkhead statistics
          <Coroutine test_get_bulkhead_stats>
            Test get_bulkhead_stats function
          <Function test_get_bulkhead_stats_empty>
            Test stats for non-existent bulkhead
        <Class TestBulkheadConfiguration>
          Test bulkhead configuration
          <Function test_get_bulkhead_with_default_limit>
            Test bulkhead creation with default limit from config
          <Function test_get_bulkhead_with_custom_limit>
            Test bulkhead creation with custom limit
          <Function test_reset_bulkhead>
            Test manual bulkhead reset
        <Class TestBulkheadMetrics>
          Test bulkhead metrics emission
          <Coroutine test_rejection_metric>
            Test that rejection metric is emitted
          <Coroutine test_active_operations_metric>
            Test that active operations metric is set
        <Class TestBulkheadEdgeCases>
          Test bulkhead edge cases
          <Coroutine test_bulkhead_with_exceptions>
            Test that bulkhead releases slot even on exception
          <Function test_bulkhead_decorator_type_check>
            Test that bulkhead only works with async functions
          <Coroutine test_different_resource_types_independent>
            Test that different resource types have independent bulkheads
      <Module test_circuit_breaker.py>
        Unit tests for circuit breaker pattern.

        Tests circuit breaker behavior, state transitions, fallback, and metrics.
        <Class TestCircuitBreakerBasics>
          Test basic circuit breaker functionality
          <Function test_get_circuit_breaker_creates_new>
            Test that get_circuit_breaker creates a new breaker
          <Function test_get_circuit_breaker_returns_same_instance>
            Test that get_circuit_breaker returns same instance for same name
          <Function test_circuit_breaker_default_state_is_closed>
            Test that circuit breaker starts in CLOSED state
          <Function test_get_circuit_breaker_state>
            Test get_circuit_breaker_state function
          <Function test_get_all_circuit_breaker_states>
            Test get_all_circuit_breaker_states function
        <Class TestCircuitBreakerStateTransitions>
          Test circuit breaker state transitions
          <Coroutine test_breaker_opens_after_max_failures>
            Test that breaker opens after reaching max failures
          <Coroutine test_breaker_fails_fast_when_open>
            Test that breaker fails fast when in OPEN state
          <Coroutine test_breaker_transitions_to_half_open>
            Test that breaker transitions to HALF_OPEN after timeout
        <Class TestCircuitBreakerWithFallback>
          Test circuit breaker with fallback functions
          <Coroutine test_fallback_called_when_circuit_open>
            Test that fallback is called when circuit is open
          <Coroutine test_static_fallback_value>
            Test circuit breaker with static fallback value
        <Class TestCircuitBreakerMetrics>
          Test circuit breaker metrics emission
          <Coroutine test_success_metric_emitted>
            Test that success metric is emitted on successful call
          <Coroutine test_failure_metric_emitted>
            Test that failure metric is emitted on failed call
        <Class TestCircuitBreakerReset>
          Test circuit breaker manual reset
          <Coroutine test_reset_circuit_breaker>
            Test manual circuit breaker reset
        <Class TestCircuitBreakerConfiguration>
          Test circuit breaker configuration
          <Function test_custom_fail_max>
            Test custom fail_max configuration
          <Function test_custom_timeout_duration>
            Test custom timeout_duration configuration
        <Class TestCircuitBreakerEdgeCases>
          Test circuit breaker edge cases
          <Coroutine test_concurrent_calls_during_open_state>
            Test concurrent calls when circuit is open
          <Coroutine test_successful_call_after_recovery>
            Test successful call after circuit recovers
          <Coroutine test_circuit_breaker_with_different_exception_types>
            Test that circuit breaker tracks all exception types
      <Module test_circuit_breaker_decorator_isolation.py>
        Regression tests for circuit breaker decorator closure isolation.

        This module contains tests to ensure that circuit breaker decorators
        maintain correct references to their circuit breaker instances even
        after registry operations like reset_all_circuit_breakers().

        Background:
        -----------
        The circuit breaker decorator creates a closure over the circuit breaker
        instance at decoration time. If reset_all_circuit_breakers() clears the
        registry, subsequent calls to get_circuit_breaker() may return a different
        instance than the one captured in the decorator's closure, leading to
        state inspection failures in tests.

        Failure Scenario:
        -----------------
        1. Function decorated with @circuit_breaker at module import time
        2. Decorator closure captures circuit breaker instance A
        3. Test calls reset_all_circuit_breakers() which clears registry
        4. Test calls get_circuit_breaker() which creates new instance B
        5. Test inspects instance B state, but decorator uses instance A
        6. Assertions fail because inspecting wrong circuit breaker object

        See Also:
        ---------
        - src/mcp_server_langgraph/resilience/circuit_breaker.py:395-416
        - tests/test_openfga_client.py:518, :559, :600 (original failures)
        - docs-internal/adr/ADR-0053-circuit-breaker-decorator-closure-isolation.md
        <Class TestCircuitBreakerDecoratorIsolation>
          Test circuit breaker decorator closure isolation.
          <Coroutine test_decorator_closure_persists_after_registry_clear>
            Test that decorator closure doesn't go stale after registry clear.

            This test validates the fix for the circuit breaker closure isolation bug
            where reset_all_circuit_breakers() was clearing the registry, causing
            get_circuit_breaker() to return a different instance than the one the
            decorator was using.

            Expected behavior (after fix):
            - reset_all_circuit_breakers() resets state without clearing registry
            - Decorator continues working with same circuit breaker instance
            - State inspections reflect actual circuit breaker state
            - No stale closure references
          <Coroutine test_reset_circuit_breaker_preserves_instance_identity>
            Test that reset_circuit_breaker() preserves instance identity.

            reset_circuit_breaker() should reset the state without creating
            a new instance, preserving decorator closure integrity.
          <Coroutine test_multiple_decorators_with_same_name_share_instance>
            Test that multiple decorators with the same name share the same
            circuit breaker instance.

            This validates that the get_circuit_breaker() registry works correctly
            and that all decorators using the same name reference the same instance.
      <Module test_fallback.py>
        Unit tests for fallback strategies.

        Tests fallback behavior, strategies, and metrics.
        <Class TestBasicFallback>
          Test basic fallback functionality
          <Coroutine test_static_fallback_value>
            Test fallback with static value
          <Coroutine test_function_fallback>
            Test fallback with function
          <Coroutine test_no_fallback_on_success>
            Test that fallback is not used when function succeeds
        <Class TestFallbackStrategies>
          Test different fallback strategies
          <Function test_default_value_fallback>
            Test DefaultValueFallback strategy
          <Function test_cached_value_fallback>
            Test CachedValueFallback strategy
          <Function test_function_fallback_strategy>
            Test FunctionFallback strategy
          <Function test_stale_data_fallback_within_limit>
            Test StaleDataFallback returns data within staleness limit
          <Function test_stale_data_fallback_exceeds_limit>
            Test StaleDataFallback rejects too-stale data
        <Class TestConvenienceDecorators>
          Test convenience fallback decorators
          <Coroutine test_fail_open_decorator>
            Test fail_open decorator (allow on error)
          <Coroutine test_fail_closed_decorator>
            Test fail_closed decorator (deny on error)
          <Coroutine test_return_empty_on_error_list>
            Test return_empty_on_error with list return type
          <Coroutine test_return_empty_on_error_dict>
            Test return_empty_on_error with dict return type
        <Class TestFallbackWithSpecificExceptions>
          Test fallback with specific exception types
          <Coroutine test_fallback_only_on_specified_exceptions>
            Test that fallback only triggers for specified exceptions
        <Class TestFallbackMetrics>
          Test fallback metrics emission
          <Coroutine test_fallback_used_metric>
            Test that fallback used metric is emitted
        <Class TestFallbackEdgeCases>
          Test fallback edge cases
          <Coroutine test_fallback_with_function_arguments>
            Test that fallback function receives original arguments
          <Function test_fallback_requires_one_strategy>
            Test that exactly one fallback strategy must be provided
          <Coroutine test_async_fallback_function>
            Test async fallback function
        <Class TestFallbackComposition>
          Test fallback with other resilience patterns
          <Coroutine test_fallback_with_retry>
            Test fallback combined with retry
      <Module test_retry.py>
        Unit tests for retry logic with exponential backoff.

        Tests retry behavior, backoff calculation, and exception handling.
        <Class TestRetryBasics>
          Test basic retry functionality
          <Coroutine test_retry_on_failure>
            Test that function is retried on failure
          <Coroutine test_retry_exhausted_raises_error>
            Test that RetryExhaustedError is raised after max attempts
          <Coroutine test_no_retry_on_immediate_success>
            Test that successful calls don't trigger retries
        <Class TestExponentialBackoff>
          Test exponential backoff timing
          <Coroutine test_exponential_backoff_timing>
            Test that backoff increases exponentially.

            Performance: Uses mocked asyncio.sleep to validate backoff calculation
            without actually sleeping. Reduces test time from 14s to <1s (93% faster).
          <Coroutine test_backoff_respects_max_limit>
            Test that backoff doesn't exceed max limit.

            Performance: Uses mocked asyncio.sleep to validate max limit enforcement
            without actually sleeping. Instant test instead of ~6s real sleeps.
        <Class TestRetryWithSpecificExceptions>
          Test retry with specific exception types
          <Coroutine test_retry_only_on_specified_exceptions>
            Test that retry only happens for specified exception types
          <Coroutine test_retry_on_multiple_exception_types>
            Test retry with multiple exception types
        <Class TestShouldRetryException>
          Test should_retry_exception logic
          <Function test_should_not_retry_validation_errors>
            Test that validation errors are not retried
          <Function test_should_not_retry_authorization_errors>
            Test that authorization errors are not retried
          <Function test_should_retry_external_service_errors>
            Test that external service errors are retried
          <Function test_should_retry_network_errors>
            Test that network errors are retried
        <Class TestRetryStrategies>
          Test different retry strategies
          <Coroutine test_exponential_strategy>
            Test exponential backoff strategy
        <Class TestRetryMetrics>
          Test retry metrics emission
          <Coroutine test_retry_attempt_metric>
            Test that retry attempt metric is emitted
          <Coroutine test_retry_exhausted_metric>
            Test that retry exhausted metric is emitted
        <Class TestRetryWithOtherPatterns>
          Test retry composition with other resilience patterns
          <Coroutine test_retry_with_timeout>
            Test retry combined with timeout
          <Coroutine test_retry_with_circuit_breaker>
            Test retry combined with circuit breaker
        <Class TestRedisOptionalDependency>
          Test that redis is an optional dependency for retry logic
          <Function test_should_retry_exception_works_without_redis>
            Test that should_retry_exception works even when redis is not installed
          <Function test_should_retry_exception_handles_redis_import_error>
            Test that ImportError from redis module doesn't crash the retry logic
      <Module test_timeout.py>
        Unit tests for timeout enforcement.

        Tests timeout behavior, context managers, and metrics.
        <Class TestTimeoutBasics>
          Test basic timeout functionality
          <Coroutine test_timeout_enforced>
            Test that timeout is enforced
          <Coroutine test_no_timeout_for_fast_function>
            Test that fast functions don't timeout
          <Coroutine test_timeout_custom_value>
            Test custom timeout value
        <Class TestTimeoutByOperationType>
          Test timeout by operation type
          <Coroutine test_llm_timeout>
            Test LLM operation timeout (60s)
          <Coroutine test_auth_timeout>
            Test auth operation timeout (5s)
          <Coroutine test_db_timeout>
            Test DB operation timeout (10s)
          <Function test_get_timeout_for_operation>
            Test get_timeout_for_operation helper
        <Class TestTimeoutContextManager>
          Test timeout context manager
          <Coroutine test_timeout_context_manager>
            Test TimeoutContext as context manager
          <Coroutine test_timeout_context_manager_enforced>
            Test that TimeoutContext enforces timeout
          <Coroutine test_timeout_context_by_operation_type>
            Test TimeoutContext with operation type
        <Class TestTimeoutMetrics>
          Test timeout metrics emission
          <Coroutine test_timeout_metric_on_violation>
            Test that metric is emitted on timeout violation
        <Class TestTimeoutEdgeCases>
          Test timeout edge cases
          <Function test_timeout_only_for_async_functions>
            Test that timeout decorator only works with async functions
          <Coroutine test_timeout_with_zero_duration>
            Test timeout with zero duration (should timeout immediately)
          <Coroutine test_multiple_concurrent_timeouts>
            Test multiple concurrent operations with different timeouts
    <Dir scripts>
      <Module test_async_mocks_validator.py>
        Unit tests for AsyncMock validation library.

        This test file validates the shared validation logic used by:
        - scripts/check_async_mock_configuration.py (pre-commit hook)
        - scripts/check_async_mock_usage.py (pre-commit hook)
        - tests/meta/test_async_mock_configuration.py (meta-tests)

        Following TDD: These tests are written FIRST, before the implementation.
        <Class TestAsyncMockConfigurationValidator>
          Unit tests for AsyncMock configuration validation logic.
          <Function test_detects_unconfigured_async_mock>
            Test detection of AsyncMock without return_value or side_effect.
          <Function test_allows_async_mock_with_return_value>
            Test that AsyncMock with return_value is allowed.
          <Function test_allows_async_mock_with_constructor_kwargs>
            Test that AsyncMock with constructor kwargs is allowed.
          <Function test_allows_async_mock_with_spec>
            Test that AsyncMock with spec parameter is considered configured.
          <Function test_respects_noqa_comment>
            Test that # noqa: async-mock-config comment suppresses warnings.
          <Function test_scopes_to_function_boundaries>
            Test that configuration checking is scoped to function boundaries.
        <Class TestAsyncMockUsageValidator>
          Unit tests for AsyncMock usage validation logic (async methods must use AsyncMock).
          <Function test_detects_async_method_mocked_without_async_mock>
            Test detection of async method mocked without AsyncMock.
          <Function test_allows_async_method_with_new_callable_async_mock>
            Test that async methods mocked with new_callable=AsyncMock are allowed.
          <Function test_uses_static_analysis_when_module_path_available>
            Test that validator uses static analysis to determine if function is async.
          <Function test_detects_async_patterns_in_method_names>
            Test detection of async naming patterns (send_, async_, fetch_).
          <Function test_uses_whitelist_for_known_sync_functions>
            Test that whitelisted synchronous functions are not flagged.
          <Function test_respects_noqa_comment_for_usage>
            Test that # noqa: async-mock comment suppresses usage warnings.
          <Function test_skips_xfail_tests>
            Test that tests marked with @pytest.mark.xfail are skipped.
        <Class TestAsyncMocksHelpers>
          Test helper functions in async_mocks validator.
          <Function test_is_async_function_detects_async_def>
            Test that is_async_function_in_source correctly identifies async functions.
          <Function test_handles_syntax_errors_gracefully>
            Test that validators handle files with syntax errors gracefully.
      <Module test_fix_missing_pytestmarks.py>
        Unit tests for scripts/fix_missing_pytestmarks.py

        These tests validate that the script correctly handles multi-line import statements
        and does not insert pytestmark declarations inside import blocks.

        Regression prevention for commit a57fcc95 (2025-11-20) where pytestmark was
        incorrectly inserted inside multi-line imports, causing SyntaxError in 16 test files.

        Test ID: TEST-SCR-001
        <Class TestModuleLevelInsertPosition>
          Test get_module_level_insert_position handles multi-line imports correctly.
          <Function test_insert_after_single_line_imports>
            Test insertion after single-line imports (basic case).

            Test ID: TEST-SCR-001-01
          <Function test_insert_after_multiline_import_last>
            Test insertion after multi-line import as last import (BUG CASE).

            This is the exact scenario that caused the bug in commit a57fcc95.
            The script used node.lineno (line 3) instead of node.end_lineno (line 6),
            causing pytestmark to be inserted inside the import parentheses.

            Test ID: TEST-SCR-001-02
          <Function test_insert_after_multiline_import_middle>
            Test insertion after multi-line import with more imports after.

            Test ID: TEST-SCR-001-03
          <Function test_insert_after_multiple_multiline_imports>
            Test insertion after multiple multi-line import statements.

            Test ID: TEST-SCR-001-04
          <Function test_insert_after_docstring_and_imports>
            Test insertion after module docstring and imports.

            Test ID: TEST-SCR-001-05
          <Function test_insert_with_no_imports>
            Test insertion when file has no imports (edge case).

            Test ID: TEST-SCR-001-06
          <Function test_insert_with_very_long_multiline_import>
            Test insertion after very long multi-line import (many items).

            Test ID: TEST-SCR-001-07
        <Class TestPytestmarkIntegration>
          Integration tests for complete pytestmark insertion workflow.
          <Function test_generated_code_is_syntactically_valid_multiline_import>
            Integration test: ensure generated code with multi-line imports is valid.

            This is the critical test that would have caught the bug in commit a57fcc95.

            Test ID: TEST-SCR-001-08
          <Function test_generated_code_preserves_existing_code>
            Test that adding pytestmark doesn't corrupt existing code.

            Test ID: TEST-SCR-001-09
          <Function test_multiple_multiline_imports_generates_valid_syntax>
            Test complex case with multiple multi-line imports.

            Test ID: TEST-SCR-001-10
          <Function test_handles_inline_comments_in_multiline_import>
            Test multi-line import with inline comments.

            Test ID: TEST-SCR-001-11
        <Class TestEdgeCases>
          Test edge cases and error conditions.
          <Function test_file_with_syntax_error_uses_fallback>
            Test that files with syntax errors use fallback method.

            Test ID: TEST-SCR-001-12
          <Function test_empty_file>
            Test handling of empty file.

            Test ID: TEST-SCR-001-13
          <Function test_only_docstring_no_imports>
            Test file with only module docstring, no imports.

            Test ID: TEST-SCR-001-14
      <Module test_memory_safety_validator.py>
        Unit tests for memory safety validation library.

        This test file validates the shared validation logic used by:
        - scripts/check_test_memory_safety.py (pre-commit hook)
        - tests/meta/test_pytest_xdist_enforcement.py (meta-tests)

        Following TDD: These tests are written FIRST, before the implementation.
        <Class TestMemorySafetyValidator>
          Unit tests for memory safety validation logic.
          <Function test_detects_missing_xdist_group_marker>
            Test detection of test class using mocks without xdist_group marker.
          <Function test_detects_missing_teardown_method>
            Test detection of test class missing teardown_method with gc.collect().
          <Function test_allows_valid_memory_safety_pattern>
            Test that valid memory safety pattern passes validation.
          <Function test_ignores_classes_not_using_mocks>
            Test that classes not using AsyncMock/MagicMock are not flagged.
          <Function test_detects_performance_test_missing_xdist_skipif>
            Test detection of performance test missing skipif for xdist.
          <Function test_allows_performance_test_with_xdist_skipif>
            Test that performance test with skipif passes validation.
          <Function test_violation_dataclass_structure>
            Test that Violation objects have expected structure.
          <Function test_handles_syntax_errors_gracefully>
            Test that validator handles files with syntax errors gracefully.
          <Function test_detects_async_mock_instantiation_not_references>
            Test that only AsyncMock() instantiation is detected, not name references.
          <Function test_detects_magic_mock_in_addition_to_async_mock>
            Test that both AsyncMock and MagicMock are detected.
        <Class TestMemorySafetyHelpers>
          Test helper functions in memory safety validator.
          <Function test_find_test_files_returns_sorted_list>
            Test that find_test_files returns sorted list of test files.
          <Function test_find_test_files_handles_missing_directory>
            Test that find_test_files handles missing directory gracefully.
          <Function test_print_violations_shows_helpful_output>
            Test that print_violations produces helpful output.
          <Function test_print_violations_shows_success_when_no_violations>
            Test that print_violations shows success message when no violations.
      <Module test_test_ids_validator.py>
        Unit tests for test ID pollution prevention validator.

        This test file validates the shared validation logic used by:
        - scripts/validate_test_ids.py (pre-commit hook)
        - tests/meta/test_id_pollution_prevention.py (meta-tests)

        Following TDD: These tests are written FIRST, before the implementation.
        <Class TestIDsValidator>
          Unit tests for test ID pollution prevention logic.
          <Function test_detects_hardcoded_user_id>
            Test detection of hardcoded user IDs like 'user:alice'.
          <Function test_detects_hardcoded_apikey_id>
            Test detection of hardcoded API key IDs.
          <Function test_allows_openfga_format_assertions>
            Test that OpenFGA format assertions are allowed.
          <Function test_allows_mock_configurations>
            Test that Mock/AsyncMock configurations are allowed.
          <Function test_allows_docstring_examples>
            Test that docstring examples are allowed.
          <Function test_allows_comments>
            Test that comments with IDs are allowed.
          <Function test_is_file_exempt_for_conftest>
            Test that conftest.py is exempt.
          <Function test_is_unit_test_with_inmemory_detects_correctly>
            Test that unit tests with InMemory backends are correctly identified.
          <Function test_is_unit_test_with_inmemory_rejects_integration>
            Test that integration tests are NOT identified as InMemory unit tests.
          <Function test_validate_test_file_returns_true_for_valid>
            Test that validate_test_file returns True for valid files.
          <Function test_validate_test_file_returns_false_for_violations>
            Test that validate_test_file returns False when violations found.
          <Function test_check_worker_safe_usage_detects_helpers>
            Test detection of worker-safe helper usage.
          <Function test_check_worker_safe_usage_returns_false_when_no_helpers>
            Test that check_worker_safe_usage returns False when no helpers used.
          <Function test_violation_tuple_structure>
            Test that violations are tuples with expected structure.
        <Class TestIDsValidatorHelpers>
          Test helper functions in test IDs validator.
          <Function test_legitimate_patterns_match_correctly>
            Test that legitimate usage patterns are correctly identified.
          <Function test_exempt_files_list_includes_conftest>
            Test that exempt files list includes expected files.
    <Package security>
      <Module test_deployment_security_regression.py>
        Deployment Security Regression Tests

        These tests validate that previously-fixed security issues in deployment
        configurations remain fixed. They prevent regression of critical security
        findings from Codex and security audits.

        Test Coverage:
        1. Qdrant deployment has proper securityContext (readOnlyRootFilesystem)
        2. OpenFGA ServiceAccount has Workload Identity annotations
        3. Qdrant health checks use grpc_health_probe (not wget/curl)
        <Class TestQdrantSecurityContext>
          Regression tests for Qdrant security context configuration
          <Function test_qdrant_has_readonly_root_filesystem>
            Test that Qdrant deployment has readOnlyRootFilesystem: true.

            This prevents the container from writing to its root filesystem,
            which is a security best practice. Previously flagged by Trivy.

            File: deployments/overlays/staging-gke/qdrant-patch.yaml:12
            Finding: Trivy flagged missing securityContext.readOnlyRootFilesystem
          <Function test_qdrant_has_run_as_non_root>
            Test that Qdrant runs as non-root user.

            This prevents privilege escalation attacks.
          <Function test_qdrant_drops_all_capabilities>
            Test that Qdrant drops all Linux capabilities.

            This follows the principle of least privilege.
        <Class TestOpenFGAWorkloadIdentity>
          Regression tests for OpenFGA Workload Identity configuration
          <Function test_openfga_has_workload_identity_annotation>
            Test that OpenFGA ServiceAccount has Workload Identity annotation.

            This enables secure authentication to GCP services without
            using service account keys.

            File: deployments/overlays/staging-gke/serviceaccount-openfga.yaml:6
            Finding: Missing iam.gke.io/gcp-service-account annotation
          <Function test_openfga_serviceaccount_has_minimal_permissions_comment>
            Test that OpenFGA ServiceAccount documents minimal permissions.

            This ensures the principle of least privilege is documented.
        <Class TestQdrantHealthCheckSecurity>
          Regression tests for Qdrant health check security
          <Function test_qdrant_uses_tcp_health_check>
            Test that Qdrant health check uses TCP-based check (not wget/curl/grpc_health_probe).

            Qdrant v1.15.1 image does not include wget, curl, or grpc_health_probe binaries.
            For security and reliability, use TCP port check via /dev/tcp instead.

            File: docker-compose.test.yml:232
            Finding: Must use TCP-based health check as no HTTP clients exist in base image
          <Function test_qdrant_health_check_has_proper_timing>
            Test that Qdrant health check has proper timing configuration.

            This ensures the health check doesn't timeout prematurely.
        <Class TestHelmPlaceholderSecurity>
          Regression tests for Helm value placeholder security
          <Function test_no_unresolved_project_id_placeholders_in_production>
            Test that production Helm values don't have unresolved PROJECT_ID placeholders.

            Unresolved placeholders could cause deployment failures or security issues
            if accidentally deployed with test/placeholder values.

            Files:
            - deployments/helm/values-staging.yaml:108
            - deployments/helm/values-production.yaml:139
          <Function test_helm_values_are_valid_yaml>
            Test that Helm values files are valid YAML.

            This ensures deployment configurations can be parsed.
      <Module test_injection_attacks.py>
        Security regression tests for injection attacks

        Tests defense against OWASP Top 10 and code execution exploits.
        Following TDD best practices - these tests should FAIL until implementation is complete.
        <Class TestCommandInjection>
          Test defenses against command injection attacks
          <Function test_reject_os_system_command_injection>
            Test blocking os.system() command injection
          <Function test_reject_subprocess_injection>
            Test blocking subprocess command injection
          <Function test_reject_shell_execution_patterns>
            Test blocking shell execution patterns
        <Class TestCodeInjection>
          Test defenses against code injection attacks
          <Function test_reject_eval_injection>
            Test blocking eval() injection
          <Function test_reject_exec_injection>
            Test blocking exec() injection
          <Function test_reject_compile_injection>
            Test blocking compile() injection
          <Function test_reject_dynamic_import_injection>
            Test blocking __import__() injection
        <Class TestDeserializationAttacks>
          Test defenses against deserialization attacks
          <Function test_reject_pickle_deserialization>
            Test blocking pickle deserialization (RCE risk)
          <Function test_reject_marshal_deserialization>
            Test blocking marshal deserialization
          <Function test_reject_yaml_unsafe_load>
            Test blocking YAML module (can be unsafe)
        <Class TestPathTraversal>
          Test defenses against path traversal attacks
          <Function test_reject_file_open_with_traversal>
            Test blocking open() with path traversal
          <Function test_reject_os_path_operations>
            Test blocking os.path operations (path traversal risk)
        <Class TestPrivilegeEscalation>
          Test defenses against privilege escalation
          <Function test_reject_setuid_operations>
            Test blocking setuid operations
          <Function test_reject_ptrace_operations>
            Test blocking ptrace (debugging other processes)
        <Class TestReflectionAndIntrospection>
          Test defenses against reflection/introspection attacks
          <Function test_reject_globals_access>
            Test blocking globals() access
          <Function test_reject_locals_access>
            Test blocking locals() access
          <Function test_reject_vars_access>
            Test blocking vars() access
          <Function test_reject_getattr_abuse>
            Test blocking getattr() abuse
          <Function test_reject_setattr_abuse>
            Test blocking setattr() abuse
          <Function test_reject_delattr_abuse>
            Test blocking delattr() abuse
          <Function test_reject_class_manipulation>
            Test blocking class manipulation attacks
        <Class TestNetworkAttacks>
          Test defenses against network-based attacks
          <Function test_reject_socket_operations>
            Test blocking socket operations
          <Function test_reject_urllib_without_whitelist>
            Test blocking urllib (network access)
          <Function test_reject_requests_library>
            Test blocking requests library (network access)
        <Class TestResourceExhaustion>
          Test detection of resource exhaustion patterns
          <Function test_warn_infinite_loop>
            Test warning on obvious infinite loops
          <Function test_warn_large_recursion>
            Test warning on deep recursion patterns
          <Function test_warn_memory_allocation_bomb>
            Test warning on obvious memory bombs
        <Class TestOWASPTop10>
          Test defenses against OWASP Top 10 vulnerabilities
          <Function test_injection_prevention>
            A01:2021 - Injection prevention
          <Function test_insecure_deserialization_prevention>
            A08:2021 - Insecure deserialization prevention
          <Function test_ssrf_prevention>
            Test SSRF (Server-Side Request Forgery) prevention
      <Module test_permission_inheritance.py>
        Tests for Permission Inheritance via acts_as Relationship

        Following TDD principles - these tests define the expected behavior
        of service principals inheriting permissions from associated users.
        <Class TestDirectPermissions>
          Test direct permission checks (without inheritance)
          <Coroutine test_user_has_direct_permission>
            Test user with direct permission to resource
          <Coroutine test_user_without_permission_denied>
            Test user without permission is denied
        <Class TestServicePrincipalInheritedPermissions>
          Test permission inheritance via acts_as relationship
          <Coroutine test_service_principal_inherits_user_permission>
            Test service principal inherits permission from associated user
          <Coroutine test_service_principal_without_acts_as_denied>
            Test service principal without acts_as relationship is denied
          <Coroutine test_service_principal_acts_as_user_without_permission>
            Test service principal acts as user who also lacks permission
          <Coroutine test_service_principal_acts_as_multiple_users>
            Test service principal acting as multiple users (first with permission wins)
        <Class TestServicePrincipalDirectPermissions>
          Test service principals can also have direct permissions
          <Coroutine test_service_principal_direct_permission_without_acts_as>
            Test service principal with direct permission (no user association needed)
          <Coroutine test_service_principal_prefers_direct_over_inherited>
            Test that direct permission check happens before inheritance lookup
        <Class TestRegularUsersUnaffected>
          Test that regular users are unaffected by acts_as logic
          <Coroutine test_regular_user_permission_check_unchanged>
            Test that regular user permission checks don't trigger acts_as lookup
        <Class TestPermissionInheritanceLogging>
          Test that inherited access is logged for audit trail
          <Coroutine test_inherited_permission_logs_both_identities>
            Test that when permission is inherited, both service and user are logged
        <Class TestPermissionInheritanceCaching>
          Test caching of acts_as relationships for performance
          <Coroutine test_acts_as_relationships_cached>
            Test that acts_as relationships are cached to reduce OpenFGA calls
      <Module test_security_report.py>
        Test security report generation.

        Following TDD principles - these tests define the expected behavior
        before implementation.
        <Function test_generate_report_creates_markdown_file>
          Test that generate_report creates a markdown report file.
        <Function test_generate_report_includes_severity_summary>
          Test that report includes severity counts for all findings.
        <Function test_generate_report_includes_vulnerability_details>
          Test that report includes detailed vulnerability information.
        <Function test_generate_report_handles_missing_files_gracefully>
          Test that generate_report handles missing scan files gracefully.
        <Function test_generate_report_includes_timestamp>
          Test that report includes generation timestamp.
        <Function test_parse_bandit_report_extracts_findings>
          Test that Bandit report parsing extracts all findings correctly.
        <Function test_parse_safety_report_extracts_vulnerabilities>
          Test that Safety report parsing extracts vulnerabilities correctly.
        <Function test_parse_pip_audit_report_extracts_vulnerabilities>
          Test that pip-audit report parsing extracts vulnerabilities correctly.
        <Function test_format_markdown_report_structure>
          Test that markdown report has correct structure.
        <Function test_categorize_by_severity_groups_findings>
          Test that findings are correctly grouped by severity.
    <Dir smoke>
      <Module test_ci_startup_smoke.py>
        CI/CD Smoke Tests for Application Startup

        These tests run in CI/CD pipelines to catch critical startup failures before
        deployment. They validate that the application can start successfully with
        various configurations.

        Design Principles:
        - Fast execution (< 30 seconds total)
        - No external dependencies required
        - Catches critical startup failures
        - Validates dependency injection wiring
        - Prevents deployment of broken configurations

        These tests would have caught ALL 5 bugs from OpenAI Codex review.
        <Class TestCriticalStartupValidation>
          Critical smoke tests that must pass before deployment
          <Function test_import_core_modules>
            CRITICAL: All core modules must import without errors.

            Failure Modes Caught:
            - Syntax errors
            - Missing dependencies
            - Circular import errors
            - Module initialization failures
          <Function test_settings_initialize_with_defaults>
            CRITICAL: Settings must initialize with default values.

            Failure Modes Caught:
            - Missing required environment variables
            - Invalid default values
            - Type validation errors
          <Function test_keycloak_client_factory_with_minimal_config>
            CRITICAL: Keycloak client must initialize with minimal config.

            BUG CAUGHT: Missing admin credentials (Bug #1)
          <Function test_openfga_client_returns_none_when_disabled>
            CRITICAL: OpenFGA must return None when config incomplete.

            BUG CAUGHT: Always creating client with None store_id (Bug #2)

            Note: Skipped if observability not initialized (acceptable for smoke tests)
          <Function test_service_principal_manager_handles_none_openfga>
            CRITICAL: ServicePrincipalManager must not crash with None OpenFGA.

            BUG CAUGHT: AttributeError when OpenFGA disabled (Bug #3)
          <Function test_cache_service_accepts_redis_credentials>
            CRITICAL: CacheService must accept Redis password and SSL.

            BUG CAUGHT: Ignoring redis_password and redis_ssl (Bug #4)
          <Function test_fastapi_app_can_be_created>
            CRITICAL: FastAPI app must be creatable.

            Failure Modes Caught:
            - Dependency injection errors
            - Route registration errors
            - Middleware initialization errors
        <Class TestConfigurationValidation>
          Validate configuration handling
          <Function test_production_config_validation>
            Test that production config validation works correctly.

            This ensures we don't deploy with insecure defaults.
          <Function test_development_config_allows_inmemory>
            Test that development environment allows inmemory providers.
        <Class TestDependencyInjectionSmoke>
          Smoke tests for dependency injection system
          <Function test_all_dependency_singletons_initialize>
            COMPREHENSIVE: All dependency singletons must initialize.

            This is the ultimate smoke test - validates entire DI system.
        <Class TestGracefulDegradationSmoke>
          Smoke tests for graceful degradation
          <Function test_system_works_without_external_services>
            CRITICAL: System must work when external services are unavailable.

            This validates graceful degradation for:
            - Redis (L2 cache fallback to L1)
            - OpenFGA (return None, no crash)
            - Keycloak (client created, connectivity tested lazily)
    <Dir terraform>
      <Module test_no_placeholders.py>
        Test suite to validate that Terraform configurations contain no placeholder values.

        These tests ensure that sensitive configuration values like ACCOUNT_ID, PROJECT_ID,
        and ENVIRONMENT are replaced with actual values before deployment.

        Following TDD Red-Green-Refactor:
        - RED: These tests should FAIL initially (placeholders exist)
        - GREEN: After implementing variable substitution, tests should PASS
        - REFACTOR: Improve validation logic as needed
        <Class TestTerraformPlaceholders>
          Test that Terraform files contain no placeholder values.
          <Function test_no_placeholders_in_terraform_modules>
            Test that terraform/modules contains no placeholder values.

            RED: Should FAIL - modules have ACCOUNT_ID, PROJECT_ID placeholders
            GREEN: Should PASS - after variable substitution implemented
          <Function test_no_placeholders_in_terraform_environments>
            Test that terraform/environments contains no placeholder values.

            RED: Should FAIL - environments have placeholder defaults
            GREEN: Should PASS - after real values configured
        <Class TestEKSEndpointSecurity>
          Test that EKS API endpoints are not publicly accessible from 0.0.0.0/0.
          <Function test_eks_endpoint_not_open_to_internet>
            Test that EKS cluster endpoints are not accessible from 0.0.0.0/0.

            RED: Should FAIL - currently defaults to ["0.0.0.0/0"]
            GREEN: Should PASS - after restricting to specific CIDR blocks
          <Function test_eks_endpoint_cidr_validation_exists>
            Test that EKS module has validation preventing 0.0.0.0/0 in production.

            RED: Should FAIL - no validation currently exists
            GREEN: Should PASS - after adding validation blocks
        <Class TestGKENetworkSecurity>
          Test that GKE control plane has proper network security.
          <Function test_gke_prod_private_endpoint_enabled>
            Test that GKE production uses private endpoint.

            RED: Should FAIL - currently enable_private_endpoint = false
            GREEN: Should PASS - after enabling private endpoint
          <Function test_gke_prod_authorized_networks_restricted>
            Test that GKE authorized networks are restricted to specific VPC CIDRs.

            RED: Should FAIL - currently allows entire 10.0.0.0/8 space
            GREEN: Should PASS - after restricting to specific /16 or /24 ranges
          <Function test_gke_module_authorized_networks_validation>
            Test that GKE module validates non-empty CIDRs when enabled.

            RED: Should FAIL - no validation currently exists
            GREEN: Should PASS - after adding validation
    <Module test_constants.py>
      Test that validates centralized test constants are used consistently.

      This test ensures that JWT secrets and other test constants are synchronized
      across all test environments (local, Docker, CI/CD).

      OpenAI Codex Finding (2025-11-16):
      - Integration tests failed due to JWT secret mismatch
      - conftest.py used "test-secret-key"
      - docker-compose.test.yml used "test-secret-key-for-integration-tests"
      - CI workflows used "test-secret-key-for-ci"
      - Auth middleware validated with settings.jwt_secret_key, causing token rejection

      Solution:
      - Centralized test constants in tests/constants.py
      - All test fixtures, Docker configs, and CI workflows use same constant
      - This test validates consistency is maintained
      <Function test_jwt_secret_constant_exists>
        Verify TEST_JWT_SECRET constant exists in tests/constants.py.
      <Function test_mock_jwt_token_uses_constant>
        Verify mock_jwt_token fixture uses TEST_JWT_SECRET.
      <Function test_docker_compose_uses_correct_jwt_secret>
        Verify docker-compose.test.yml uses the correct JWT secret.
      <Function test_github_workflows_use_correct_jwt_secret>
        Verify GitHub workflow files use the correct JWT secret.
      <Function test_jwt_secret_not_in_production_config>
        Verify test JWT secret is not accidentally used in production config.
    <Dir tools>
      <Module test_tool_discovery.py>
        Tests for Tool Discovery Module

        Tests progressive tool discovery functionality including search, filtering,
        and formatting at different detail levels following Anthropic's MCP best practices.
        <Class TestSearchToolsInput>
          Tests for SearchToolsInput schema
          <Function test_search_tools_input_default_values>
            Test SearchToolsInput with default values
          <Function test_search_tools_input_with_query>
            Test SearchToolsInput with query
          <Function test_search_tools_input_with_category>
            Test SearchToolsInput with category
          <Function test_search_tools_input_with_detail_level>
            Test SearchToolsInput with different detail levels
        <Class TestFilterToolsByCategory>
          Tests for _filter_tools_by_category
          <Function test_filter_by_calculator_category>
            Test filtering tools by calculator category
          <Function test_filter_by_search_category>
            Test filtering tools by search category
          <Function test_filter_by_filesystem_category>
            Test filtering tools by filesystem category
          <Function test_filter_by_execution_category>
            Test filtering tools by execution category
          <Function test_filter_by_unknown_category_returns_all_tools>
            Test filtering with unknown category returns all tools
          <Function test_filter_by_case_insensitive_category>
            Test category filtering is case-insensitive
        <Class TestFilterToolsByQuery>
          Tests for _filter_tools_by_query
          <Function test_filter_by_query_matches_name>
            Test query filtering matches tool names
          <Function test_filter_by_query_matches_description>
            Test query filtering matches tool descriptions
          <Function test_filter_by_query_case_insensitive>
            Test query filtering is case-insensitive
          <Function test_filter_by_query_no_matches>
            Test query filtering with no matches
          <Function test_filter_by_query_partial_match>
            Test query filtering with partial matches
        <Class TestToolFormatting>
          Tests for tool formatting functions
          <Function test_format_tool_minimal>
            Test minimal tool formatting
          <Function test_format_tool_standard>
            Test standard tool formatting
          <Function test_format_tool_standard_without_args_schema>
            Test standard formatting for tool without args_schema
          <Function test_format_tool_full>
            Test full tool formatting
          <Function test_format_tool_full_shows_required_fields>
            Test full formatting shows required vs optional fields
          <Function test_format_tool_full_without_args_schema>
            Test full formatting for tool without args_schema
        <Class TestFormatToolResults>
          Tests for _format_tool_results
          <Function test_format_results_minimal>
            Test formatting results with minimal detail
          <Function test_format_results_standard>
            Test formatting results with standard detail
          <Function test_format_results_full>
            Test formatting results with full detail
          <Function test_format_results_empty_list>
            Test formatting with empty tool list
        <Class TestSearchTools>
          Tests for search_tools function
          <Function test_search_tools_no_parameters>
            Test search_tools with no parameters returns all tools
          <Function test_search_tools_by_category>
            Test search_tools filtering by category
          <Function test_search_tools_by_query>
            Test search_tools filtering by query
          <Function test_search_tools_with_query_and_category>
            Test search_tools with both query and category
          <Function test_search_tools_minimal_detail>
            Test search_tools with minimal detail level
          <Function test_search_tools_standard_detail>
            Test search_tools with standard detail level
          <Function test_search_tools_full_detail>
            Test search_tools with full detail level
          <Function test_search_tools_no_matches>
            Test search_tools with query that matches nothing
          <Function test_search_tools_no_matches_with_category>
            Test search_tools with non-matching query and category
          <Function test_search_tools_case_insensitive_query>
            Test search_tools query is case-insensitive
          <Function test_search_tools_returns_string>
            Test search_tools always returns a string
          <Function test_search_tools_invalid_category_returns_all>
            Test search_tools with invalid category returns all tools
        <Class TestToolDiscoveryIntegration>
          Integration tests for tool discovery
          <Function test_progressive_disclosure_workflow>
            Test complete progressive disclosure workflow
          <Function test_search_then_filter_workflow>
            Test search followed by filter workflow
          <Function test_token_efficiency_minimal_vs_full>
            Test that minimal detail uses fewer tokens than full
    <Dir unit>
      <Dir auth>
        <Module test_api_key_manager.py>
          Tests for API Key Manager

          Following TDD principles - these tests are written BEFORE implementation.
          Tests cover:
          - API key generation and creation
          - bcrypt hashing and validation
          - Key rotation and revocation
          - Expiration handling
          - Listing keys per user
          - Keycloak attribute storage
          <Class TestAPIKeyGeneration>
            Test API key generation
            <Function test_generate_api_key_format>
              Test that generated API keys have correct format
            <Function test_generate_api_key_uniqueness>
              Test that generated keys are cryptographically unique
            <Function test_generate_api_key_test_prefix>
              Test generating API key with test prefix
          <Class TestAPIKeyCreation>
            Test API key creation
            <Coroutine test_create_api_key_success>
              Test successful API key creation
            <Coroutine test_create_api_key_returns_created_timestamp>
              REGRESSION TEST for Codex finding: API key creation must return 'created' timestamp.

              The CreateAPIKeyResponse schema requires 'created' field, but create_api_key()
              was only returning key_id, api_key, name, and expires_at, causing the API
              endpoint to fill 'created' with an empty string.

              This violates the API contract and breaks clients expecting the created timestamp.
            <Coroutine test_create_api_key_enforces_max_limit>
              Test that creating more than max keys raises error
            <Coroutine test_create_api_key_stores_metadata>
              Test that API key metadata is stored correctly
          <Class TestAPIKeyValidation>
            Test API key validation
            <Coroutine test_validate_api_key_success>
              Test successful API key validation
            <Coroutine test_validate_api_key_invalid_key>
              Test that invalid API key returns None
            <Coroutine test_validate_api_key_expired_key>
              Test that expired API key is rejected
            <Coroutine test_validate_api_key_updates_last_used>
              Test that successful validation updates last_used timestamp
          <Class TestAPIKeyRevocation>
            Test API key revocation
            <Coroutine test_revoke_api_key_success>
              Test successful API key revocation
            <Coroutine test_revoke_nonexistent_key_no_error>
              Test revoking non-existent key doesn't raise error
          <Class TestAPIKeyListing>
            Test listing API keys
            <Coroutine test_list_api_keys_success>
              Test listing API keys for a user
            <Coroutine test_list_api_keys_empty>
              Test listing API keys when user has none
          <Class TestAPIKeyRotation>
            Test API key rotation
            <Coroutine test_rotate_api_key_success>
              Test successful API key rotation
            <Coroutine test_rotate_nonexistent_key_raises_error>
              Test rotating non-existent key raises error
          <Class TestBcryptHashing>
            Test bcrypt hashing functionality
            <Function test_hash_api_key>
              Test that API keys are hashed with bcrypt
            <Function test_verify_api_key_hash>
              Test API key hash verification
          <Class TestAPIKeyValidationPagination>
            Test API key validation with pagination (>100 users)

            Regression test for Finding #4 from OpenAI Codex security audit.
            <Coroutine test_validate_api_key_beyond_first_page>
              Test that API key validation works for users beyond first 100
            <Coroutine test_validate_api_key_not_found_after_pagination>
              Test that validation returns None after checking all pages
            <Coroutine test_validate_api_key_pagination_stops_on_match>
              Test that pagination stops immediately when key is found
          <Class TestAPIKeyRedisCache>
            Test Redis caching for API key validation (performance optimization)
            <Coroutine test_cache_hit_skips_keycloak_lookup>
              Test that cache hit avoids expensive Keycloak pagination
            <Coroutine test_cache_miss_falls_back_to_keycloak>
              Test that cache miss falls back to Keycloak and caches result
            <Coroutine test_revoke_invalidates_cache>
              Test that revoking API key invalidates Redis cache
            <Coroutine test_cache_disabled_skips_redis>
              Test that disabling cache skips Redis operations
          <Class TestRedisAPICacheConfiguration>
            TDD RED phase tests for Redis API key cache configuration (OpenAI Codex Finding #5).

            ISSUE: get_api_key_manager() in core/dependencies.py creates APIKeyManager
            without passing redis_client, even though settings.api_key_cache_enabled=True
            and settings.api_key_cache_ttl are configured.

            IMPACT: API key cache is disabled, causing performance degradation and
            unnecessary Keycloak queries on every API key validation.

            These tests will FAIL until dependencies.py wires Redis client.
            <Function test_get_api_key_manager_uses_redis_when_enabled>
              Test that get_api_key_manager passes Redis client when caching is enabled.
            <Function test_get_api_key_manager_respects_cache_ttl_from_settings>
              Test that cache TTL from settings is passed to APIKeyManager.
            <Function test_get_api_key_manager_uses_correct_redis_database>
              Test that Redis client connects to correct database number.
            <Function test_get_api_key_manager_disables_cache_when_redis_url_missing>
              Test that caching is disabled when redis_url is not configured.

              GREEN: This should already work (graceful degradation)
            <Coroutine test_api_key_validation_uses_redis_cache>
              Integration test: Verify API key validation actually uses Redis cache.
        <Module test_auth.py>
          Unit tests for auth.py - Authentication and Authorization
          <Class TestAuthMiddleware>
            Test AuthMiddleware class
            <Function test_init>
              Test AuthMiddleware initialization
            <Coroutine test_authenticate_success>
              Test successful user authentication with password
            <Coroutine test_authenticate_missing_password>
              Test authentication without password fails
            <Coroutine test_authenticate_invalid_password>
              Test authentication with wrong password fails
            <Coroutine test_authenticate_user_not_found>
              Test authentication with non-existent user
            <Coroutine test_authenticate_inactive_user>
              Test authentication with inactive user
            <Coroutine test_authorize_with_openfga_success>
              Test authorization with OpenFGA returns True
            <Coroutine test_authorize_with_openfga_denied>
              Test authorization with OpenFGA returns False
            <Coroutine test_authorize_with_openfga_error>
              Test authorization fails closed on OpenFGA error
            <Coroutine test_authorize_fallback_admin_access>
              Test fallback authorization grants admin full access
            <Coroutine test_authorize_fallback_premium_user>
              Test fallback authorization for premium user
            <Coroutine test_authorize_fallback_standard_user>
              Test fallback authorization for standard user
            <Coroutine test_authorize_fallback_viewer_access>
              Test fallback authorization for viewer relation on default conversation
            <Coroutine test_authorize_fallback_editor_access_default>
              Test fallback authorization for editor relation on default conversation
            <Coroutine test_authorize_fallback_editor_access_owned>
              Test fallback authorization allows access to user-owned conversations
            <Coroutine test_authorize_fallback_editor_access_denied_other_user>
              Test fallback authorization DENIES access to conversations owned by other users.

              SECURITY: This is a critical regression test. The old fallback logic granted
              access to ANY conversation:* resource. This test ensures users can only access
              their own conversations.
            <Coroutine test_authorize_fallback_viewer_access_denied_other_user>
              Test fallback authorization DENIES viewer access to other users' conversations
            <Coroutine test_authorize_fallback_unknown_user>
              Test fallback authorization denies unknown user
            <Coroutine test_list_accessible_resources_success>
              Test listing accessible resources with OpenFGA
            <Coroutine test_list_accessible_resources_no_openfga>
              Test listing resources without OpenFGA returns empty list (post Finding #1 fix)
            <Coroutine test_list_accessible_resources_error>
              Test listing resources handles OpenFGA errors
            <Function test_create_token_success>
              Test JWT token creation
            <Function test_create_token_expiration>
              Test JWT token expiration is set correctly
            <Function test_create_token_user_not_found>
              Test token creation fails for non-existent user
            <Coroutine test_verify_token_success>
              Test successful token verification
            <Coroutine test_verify_token_expired>
              Test verification of expired token
            <Coroutine test_verify_token_invalid>
              Test verification of invalid token
            <Coroutine test_verify_token_wrong_secret>
              Test verification with wrong secret key
          <Class TestRequireAuthDecorator>
            Test require_auth decorator
            <Coroutine test_require_auth_success>
              Test decorator allows authorized request
            <Coroutine test_require_auth_no_credentials>
              Test decorator blocks request without credentials
            <Coroutine test_require_auth_invalid_user>
              Test decorator blocks invalid user
            <Coroutine test_require_auth_with_authorization>
              Test decorator with authorization check
            <Coroutine test_require_auth_authorization_denied>
              Test decorator blocks unauthorized request
          <Class TestStandaloneVerifyToken>
            Test standalone verify_token function
            <Coroutine test_standalone_verify_token_success>
              Test standalone token verification succeeds
            <Coroutine test_standalone_verify_token_default_secret>
              Test standalone verification with default secret
            <Coroutine test_standalone_verify_token_invalid>
              Test standalone verification of invalid token
          <Class TestGetCurrentUser>
            Test get_current_user FastAPI dependency for bearer token authentication
            <Coroutine test_get_current_user_with_valid_bearer_token>
              Test get_current_user authenticates with valid JWT bearer token.

              SECURITY: This is a critical test to ensure bearer token authentication works.
              Without proper dependency injection, bearer tokens are never validated.
            <Coroutine test_get_current_user_with_invalid_bearer_token>
              Test get_current_user rejects invalid JWT bearer token
            <Coroutine test_get_current_user_with_expired_bearer_token>
              Test get_current_user rejects expired JWT bearer token
            <Coroutine test_get_current_user_without_credentials_raises_401>
              Test get_current_user requires authentication when no credentials provided
            <Coroutine test_get_current_user_uses_request_state_if_set>
              Test get_current_user returns user from request.state if already authenticated
            <Coroutine test_get_current_user_prefers_preferred_username_over_sub>
              Test get_current_user uses preferred_username instead of sub for Keycloak compatibility.

              SECURITY: Keycloak JWTs use UUID in 'sub' field, but OpenFGA tuples use user:username format.
              We must extract 'preferred_username' to ensure authorization works correctly.
            <Coroutine test_get_current_user_falls_back_to_sub_if_no_preferred_username>
              Test get_current_user falls back to sub if preferred_username is missing.

              This handles legacy JWTs or non-Keycloak identity providers.
            <Coroutine test_get_current_user_normalizes_username_format>
              Test get_current_user normalizes user_id to user:username format for OpenFGA.

              This ensures consistent format regardless of JWT structure.
          <Class TestAuthFallbackWithExternalProviders>
            Test authorization fallback logic with external providers (Keycloak, etc.)

            SECURITY: This addresses the critical bug where Keycloak users were denied
            all access when OpenFGA was down, because the fallback logic only checked
            InMemoryUserProvider's users_db (which is empty for Keycloak).

            The fix ensures fallback authorization queries the user provider for roles
            instead of relying on in-memory user database.
            <Coroutine test_authorize_fallback_keycloak_admin_user>
              Test fallback authorization grants admin access for Keycloak users.

              REGRESSION TEST: Previously failed because users_db was empty for Keycloak.
            <Coroutine test_authorize_fallback_keycloak_premium_user>
              Test fallback authorization for Keycloak premium user.

              REGRESSION TEST: Previously denied access because users_db was empty.
            <Coroutine test_authorize_fallback_keycloak_standard_user>
              Test fallback authorization for Keycloak standard user.
            <Coroutine test_authorize_fallback_keycloak_user_not_found>
              Test fallback authorization denies access for non-existent Keycloak user.

              REGRESSION TEST: Previously failed with same denial, but now logs correctly.
            <Coroutine test_authorize_fallback_keycloak_provider_error>
              Test fallback authorization fails closed when provider lookup fails.

              SECURITY: When we can't verify user roles, deny access (fail closed).
            <Coroutine test_authorize_fallback_keycloak_conversation_ownership>
              Test fallback authorization respects conversation ownership for Keycloak users.

              SECURITY: Users should only access their own conversations in fallback mode.
            <Coroutine test_authorize_fallback_keycloak_user_without_required_role>
              Test fallback authorization denies access when user lacks required role.
            <Coroutine test_authorize_inmemory_provider_still_works>
              Test that InMemoryUserProvider fallback still works (fast path).

              REGRESSION TEST: Ensure the fix didn't break existing InMemory behavior.
            <Coroutine test_authorize_openfga_takes_precedence_over_fallback>
              Test that OpenFGA is used when available (fallback only when OpenFGA down).

              SECURITY: Fallback should ONLY be used when OpenFGA is unavailable.
          <Class TestAuthMiddlewareProductionControls>
            CRITICAL P0: Test production environment security controls

            In production, fallback should be disabled to prevent security bypasses
            <Coroutine test_authorize_fallback_disabled_in_production>
              SECURITY P0: authorize() should NOT use fallback in production

              CWE-863: Incorrect Authorization
        <Module test_auth_factory.py>
          Comprehensive tests for auth factory module

          Tests the factory functions for creating AuthMiddleware, UserProvider, and SessionStore
          based on configuration settings.
          <Class TestCreateUserProvider>
            Test create_user_provider factory function
            <Function test_create_inmemory_provider>
              Test creating InMemoryUserProvider
            <Function test_create_inmemory_provider_with_openfga>
              Test creating InMemoryUserProvider with OpenFGA client
            <Function test_create_inmemory_provider_missing_jwt_secret>
              Test error when JWT secret missing for InMemory provider
            <Function test_create_keycloak_provider>
              Test creating KeycloakUserProvider
            <Function test_create_keycloak_provider_missing_client_secret>
              Test error when Keycloak client secret missing
            <Function test_create_keycloak_provider_missing_admin_password>
              Test error when Keycloak admin password missing
            <Function test_create_provider_invalid_type>
              Test error for unknown provider type
            <Function test_create_provider_case_insensitive>
              Test provider type is case-insensitive
          <Class TestProductionEnvironmentGuards>
            Test production environment guards for InMemoryUserProvider.

            TDD RED phase: Tests written FIRST to define security requirements.
            These tests will FAIL until environment guards are implemented.
            <Function test_inmemory_provider_blocked_in_production>
              Test InMemoryUserProvider is blocked in production environment.

              RED: Will fail until environment guard is implemented in factory.py
            <Function test_inmemory_provider_blocked_in_staging>
              Test InMemoryUserProvider is blocked in staging environment
            <Function test_inmemory_provider_allowed_in_development>
              Test InMemoryUserProvider is allowed in development
            <Function test_inmemory_provider_allowed_in_test>
              Test InMemoryUserProvider is allowed in test environment
            <Function test_inmemory_provider_default_environment_is_development>
              Test default environment is development (safe fallback)
            <Function test_keycloak_provider_allowed_in_production>
              Test KeycloakUserProvider is allowed in production
            <Function test_environment_validation_case_insensitive>
              Test environment names are case-insensitive
          <Class TestCreateSessionStore>
            Test create_session_store factory function
            <Function test_create_session_store_disabled_for_token_auth>
              Test session store returns None when auth_mode is token
            <Function test_create_memory_session_store>
              Test creating in-memory session store
            <Function test_create_redis_session_store>
              Test creating Redis session store
            <Function test_create_redis_session_store_missing_url>
              Test error when Redis URL missing
            <Function test_create_session_store_invalid_backend>
              Test error for unknown session backend
            <Function test_create_session_store_case_insensitive>
              Test session backend is case-insensitive
          <Class TestCreateAuthMiddleware>
            Test create_auth_middleware factory function
            <Function test_create_auth_middleware_inmemory>
              Test creating AuthMiddleware with InMemory provider
            <Function test_create_auth_middleware_with_openfga>
              Test creating AuthMiddleware with OpenFGA client
            <Function test_create_auth_middleware_keycloak>
              Test creating AuthMiddleware with Keycloak provider
            <Function test_create_auth_middleware_with_sessions>
              Test creating AuthMiddleware with session store
            <Function test_create_auth_middleware_token_mode_no_sessions>
              Test AuthMiddleware has no session store in token mode
            <Function test_create_auth_middleware_missing_jwt_secret>
              Test error propagates from user provider creation
            <Function test_create_auth_middleware_missing_keycloak_secret>
              Test error propagates from Keycloak provider creation
          <Class TestFactoryIntegration>
            Integration tests for factory module
            <Function test_full_middleware_creation_flow>
              Test complete middleware creation with all components
            <Function test_full_production_setup>
              Test production-like setup with Keycloak and Redis
          <Class TestRedisSessionStoreSignatureFix>
            TDD RED phase tests for Redis session store signature mismatch.

            These tests use the REAL RedisSessionStore class (not mocked) to expose
            the bug where factory.py passes parameters that RedisSessionStore.__init__()
            doesn't accept.

            Expected behavior: These tests will FAIL until the signature is fixed.
            <Function test_redis_session_store_accepts_password_parameter>
              Test that RedisSessionStore accepts password parameter.

              RED: Will fail with TypeError: __init__() got unexpected keyword argument 'password'
            <Function test_redis_session_store_accepts_ttl_seconds_parameter>
              Test that RedisSessionStore accepts ttl_seconds parameter (as used by factory).

              RED: Will fail because factory uses 'ttl_seconds' but class expects 'default_ttl_seconds'
            <Function test_factory_creates_redis_store_with_real_class>
              Test that factory can actually instantiate RedisSessionStore with real signature.

              RED: Will fail with TypeError due to signature mismatch between factory and class.
          <Class TestRedisMetadataSerializationFix>
            TDD RED phase tests for Redis metadata serialization bug.

            These tests verify that metadata roundtrips correctly through Redis
            (stored as JSON, retrieved as JSON).

            Expected behavior: These tests will FAIL until str() is replaced with json.dumps().
            <Coroutine test_redis_metadata_roundtrip_preserves_data>
              Test that session metadata survives roundtrip to Redis storage.

              RED: Will fail because str() produces non-JSON format that json.loads() cannot parse.
            <Coroutine test_redis_nested_metadata_serializes_correctly>
              Test that nested metadata objects serialize correctly in Redis.

              RED: Will fail with JSON decode error on complex nested structures.
          <Class TestMemorySessionStoreFix>
            TDD RED phase tests for memory session store factory bug.

            Tests verify that factory returns a functional InMemorySessionStore
            instead of None.

            Expected behavior: These tests will FAIL until factory returns InMemorySessionStore().
            <Function test_factory_creates_memory_session_store>
              Test that factory creates InMemorySessionStore for memory backend.

              RED: Will fail because factory returns None instead of InMemorySessionStore().
            <Coroutine test_memory_sessions_work_with_session_auth_mode>
              Test that memory sessions are functional for session auth mode.

              RED: Will fail because factory returns None, making sessions non-functional.
            <Function test_memory_session_store_uses_configured_ttl>
              Test that memory session store uses TTL from settings.

              RED: Will fail because factory doesn't pass settings to InMemorySessionStore.
          <Class TestSessionStoreRegistration>
            TDD tests for session store registration bug (OpenAI Codex Finding #3).

            Validates that create_auth_middleware() registers the session store globally
            via set_session_store() so GDPR/session APIs use the configured store.
            <Function test_create_auth_middleware_registers_memory_session_store_globally>
              Test that memory session store is registered globally after middleware creation
            <Function test_create_auth_middleware_registers_redis_session_store_globally>
              Test that Redis session store is registered globally
            <Coroutine test_session_persistence_across_middleware_and_gdpr_endpoints>
              Test session created via middleware is accessible via GDPR endpoints
        <Module test_auth_metrics.py>
          Tests for authentication metrics

          Verifies that all authentication metrics are properly defined
          and helper functions work correctly.
          <Class TestMetricDefinitions>
            Test that all metrics are properly defined
            <Function test_login_metrics_defined>
              Test login-related metrics are defined
            <Function test_token_metrics_defined>
              Test token-related metrics are defined
            <Function test_jwks_metrics_defined>
              Test JWKS cache metrics are defined
            <Function test_session_metrics_defined>
              Test session management metrics are defined
            <Function test_user_provider_metrics_defined>
              Test user provider metrics are defined
            <Function test_openfga_metrics_defined>
              Test OpenFGA integration metrics are defined
            <Function test_authorization_metrics_defined>
              Test authorization metrics are defined
            <Function test_role_mapping_metrics_defined>
              Test role mapping metrics are defined
            <Function test_concurrent_session_metrics_defined>
              Test concurrent session metrics are defined
          <Class TestRecordLoginAttempt>
            Test record_login_attempt helper function
            <Function test_record_login_attempt_success>
              Test recording successful login attempt
            <Function test_record_login_attempt_failure>
              Test recording failed login attempt
          <Class TestRecordTokenVerification>
            Test record_token_verification helper function
            <Function test_record_token_verification_success>
              Test recording successful token verification
            <Function test_record_token_verification_expired>
              Test recording expired token verification
            <Function test_record_token_verification_default_provider>
              Test token verification with default provider
          <Class TestRecordSessionOperation>
            Test record_session_operation helper function
            <Function test_record_session_create_success>
              Test recording successful session creation
            <Function test_record_session_create_failure>
              Test recording failed session creation
            <Function test_record_session_retrieve>
              Test recording session retrieval
            <Function test_record_session_refresh>
              Test recording session refresh
            <Function test_record_session_revoke_success>
              Test recording successful session revocation
          <Class TestRecordJWKSOperation>
            Test record_jwks_operation helper function
            <Function test_record_jwks_cache_hit>
              Test recording JWKS cache hit
            <Function test_record_jwks_cache_miss>
              Test recording JWKS cache miss
            <Function test_record_jwks_refresh>
              Test recording JWKS refresh
            <Function test_record_jwks_without_duration>
              Test recording JWKS operation without duration
          <Class TestRecordOpenFGASync>
            Test record_openfga_sync helper function
            <Function test_record_openfga_sync_success>
              Test recording successful OpenFGA sync
            <Function test_record_openfga_sync_failure>
              Test recording failed OpenFGA sync
          <Class TestRecordRoleMapping>
            Test record_role_mapping helper function
            <Function test_record_role_mapping>
              Test recording role mapping operation
            <Function test_record_role_mapping_with_zero_rules>
              Test recording role mapping with no rules applied
          <Class TestMetricAttributes>
            Test that metrics can be called with proper attributes
            <Function test_login_metrics_accept_attributes>
              Test that login metrics accept proper attributes
            <Function test_session_active_counter_up_down>
              Test that sessions_active counter can increment and decrement
        <Module test_auth_middleware.py>
          Tests for FastAPI Auth Middleware

          Following TDD: These tests are written FIRST, before implementation.
          They define the expected behavior of the auth middleware.

          The auth middleware should:
          1. Extract Bearer tokens from Authorization headers
          2. Verify JWT tokens
          3. Set request.state.user for authenticated requests
          4. Allow public endpoints to pass through
          5. Return 401 for invalid/missing tokens on protected endpoints
          <Class TestAuthRequestMiddleware>
            Test FastAPI request middleware for auth
            <Coroutine test_middleware_extracts_bearer_token_and_sets_request_state>
              Middleware should extract Bearer token from headers and set request.state.user
            <Coroutine test_middleware_handles_missing_token_gracefully>
              Middleware should NOT return 401 for missing tokens - let endpoints decide
            <Coroutine test_middleware_handles_invalid_token>
              Middleware should NOT set request.state.user for invalid tokens
            <Coroutine test_middleware_extracts_keycloak_user_info>
              Middleware should properly extract Keycloak user information from token
            <Coroutine test_middleware_handles_malformed_authorization_header>
              Middleware should handle malformed Authorization headers gracefully
            <Coroutine test_middleware_preserves_worker_safe_user_ids>
              Middleware should preserve worker-safe user IDs from pytest-xdist
        <Module test_keycloak.py>
          Comprehensive tests for Keycloak integration

          Tests KeycloakClient, TokenValidator, and role synchronization.
          Uses mocking to avoid requiring live Keycloak instance.
          <Class TestKeycloakConfig>
            Test KeycloakConfig model
            <Function test_realm_url>
              Test realm URL construction
            <Function test_admin_url>
              Test admin API URL construction
            <Function test_token_endpoint>
              Test token endpoint URL
            <Function test_userinfo_endpoint>
              Test userinfo endpoint URL
            <Function test_jwks_uri>
              Test JWKS URI
            <Function test_well_known_url>
              Test OpenID configuration URL
          <Class TestKeycloakUser>
            Test KeycloakUser model
            <Function test_user_id_property>
              Test user_id property formats correctly
            <Function test_full_name_with_both_names>
              Test full_name with first and last name
            <Function test_full_name_with_first_name_only>
              Test full_name with only first name
            <Function test_full_name_fallback_to_username>
              Test full_name falls back to username
          <Class TestTokenValidator>
            Test TokenValidator class
            <Coroutine test_get_jwks_success>
              Test successful JWKS retrieval
            <Coroutine test_get_jwks_caching>
              Test JWKS caching works
            <Coroutine test_get_jwks_force_refresh>
              Test forced JWKS refresh
            <Coroutine test_get_jwks_http_error>
              Test JWKS retrieval handles HTTP errors
            <Coroutine test_verify_token_success>
              Test successful token verification
            <Coroutine test_verify_token_expired>
              Test expired token verification fails
            <Coroutine test_verify_token_missing_kid>
              Test token without kid in header fails
            <Coroutine test_verify_token_key_not_found>
              Test token with unknown kid triggers JWKS refresh
          <Class TestKeycloakClient>
            Test KeycloakClient class
            <Coroutine test_authenticate_user_success>
              Test successful user authentication
            <Coroutine test_authenticate_user_invalid_credentials>
              Test authentication with invalid credentials
            <Coroutine test_refresh_token_success>
              Test successful token refresh
            <Coroutine test_get_userinfo_success>
              Test getting user info
            <Coroutine test_get_admin_token_success>
              Test getting admin token
            <Coroutine test_get_admin_token_caching>
              Test admin token caching
            <Coroutine test_get_user_by_username_success>
              Test getting user by username
            <Coroutine test_get_user_by_username_not_found>
              Test getting non-existent user
          <Class TestRoleSynchronization>
            Test sync_user_to_openfga function
            <Coroutine test_sync_admin_role>
              Test syncing user with admin role
            <Coroutine test_sync_group_memberships>
              Test syncing group memberships
            <Coroutine test_sync_premium_role>
              Test syncing premium role
            <Coroutine test_sync_no_roles>
              Test syncing user with no special roles
            <Coroutine test_sync_error_handling>
              Test error handling during sync
          <Class TestKeycloakPrivateMethods>
            Test private helper methods for comprehensive coverage
            <Coroutine test_get_user_realm_roles_success>
              Test retrieving user's realm roles
            <Coroutine test_get_user_realm_roles_http_error>
              Test realm roles retrieval handles HTTP errors
            <Coroutine test_get_user_client_roles_success>
              Test retrieving user's client roles
            <Coroutine test_get_user_client_roles_http_error>
              Test client roles retrieval handles HTTP errors
            <Coroutine test_get_user_groups_success>
              Test retrieving user's group memberships
            <Coroutine test_get_user_groups_http_error>
              Test groups retrieval handles HTTP errors
          <Class TestTokenValidatorErrorPaths>
            Test error paths in TokenValidator for higher coverage
            <Coroutine test_verify_token_generic_exception>
              Test generic exception handling during token verification
          <Class TestKeycloakAdminClientManagement>
            Test Keycloak Admin API client management methods.

            These tests follow TDD principles - RED phase.
            Tests written BEFORE implementation to ensure proper behavior.
            <Coroutine test_create_client_success>
              Test creating a Keycloak client via Admin API.

              RED: This test will FAIL initially because create_client raises NotImplementedError.
            <Coroutine test_create_client_http_error>
              Test create_client handles HTTP errors gracefully
            <Coroutine test_delete_client_success>
              Test deleting a Keycloak client via Admin API
            <Coroutine test_delete_client_not_found>
              Test delete_client handles 404 Not Found
          <Class TestKeycloakAdminUserManagement>
            Test Keycloak Admin API user management methods
            <Coroutine test_create_user_success>
              Test creating a Keycloak user via Admin API
            <Coroutine test_delete_user_success>
              Test deleting a Keycloak user via Admin API
          <Class TestKeycloakUserAttributes>
            Test Keycloak user attribute management (for API keys)
            <Coroutine test_get_user_attributes_success>
              Test retrieving user attributes via Admin API
            <Coroutine test_update_user_attributes_success>
              Test updating user attributes via Admin API
          <Class TestKeycloakSCIMUserMethods>
            Test SCIM 2.0 user management methods (TDD RED phase).

            These tests are written FIRST to define expected behavior.
            All tests will FAIL initially because methods raise NotImplementedError.
            <Coroutine test_update_user_success>
              Test updating a user via Admin API
            <Coroutine test_update_user_http_error>
              Test update_user handles HTTP errors
            <Coroutine test_set_user_password_success>
              Test setting user password via Admin API
            <Coroutine test_set_user_password_temporary>
              Test setting temporary user password
            <Coroutine test_get_user_success>
              Test getting user by ID via Admin API
            <Coroutine test_get_user_not_found>
              Test get_user returns None for non-existent user
            <Coroutine test_search_users_success>
              Test searching users via Admin API
            <Coroutine test_search_users_with_email_filter>
              Test searching users by email
            <Coroutine test_get_users_success>
              Test getting all users via Admin API
          <Class TestKeycloakSCIMGroupMethods>
            Test SCIM 2.0 group management methods (TDD RED phase)
            <Coroutine test_create_group_success>
              Test creating a group via Admin API
            <Coroutine test_create_group_http_error>
              Test create_group handles HTTP errors
            <Coroutine test_get_group_success>
              Test getting group by ID via Admin API
            <Coroutine test_get_group_not_found>
              Test get_group returns None for non-existent group
            <Coroutine test_get_group_members_success>
              Test getting group members via Admin API
            <Coroutine test_get_group_members_empty>
              Test getting members of empty group
            <Coroutine test_add_user_to_group_success>
              Test adding user to group via Admin API
            <Coroutine test_add_user_to_group_http_error>
              Test add_user_to_group handles HTTP errors
          <Class TestKeycloakSCIMClientMethods>
            Test SCIM 2.0 client/service principal methods (TDD RED phase)
            <Coroutine test_update_client_attributes_success>
              Test updating client attributes via Admin API
            <Coroutine test_update_client_secret_success>
              Test updating client secret via Admin API
            <Coroutine test_get_clients_success>
              Test getting all clients via Admin API
            <Coroutine test_get_clients_with_query>
              Test getting clients with query filter
            <Coroutine test_get_client_success>
              Test getting client by ID via Admin API
            <Coroutine test_get_client_not_found>
              Test get_client returns None for non-existent client
          <Class TestKeycloakTokenIssuance>
            Test Keycloak token issuance methods for API key → JWT exchange (TDD RED phase)
            <Coroutine test_issue_token_for_user_success>
              Test issuing JWT token for user (for API key exchange)
            <Coroutine test_issue_token_for_user_with_client_id>
              Test issuing token with specific client_id
            <Coroutine test_issue_token_for_user_http_error>
              Test issue_token_for_user handles HTTP errors
        <Module test_role_mapper.py>
          Comprehensive tests for Role Mapping Engine

          Tests cover:
          - Simple role mappings
          - Group pattern matching
          - Conditional mappings
          - Role hierarchies
          - YAML configuration loading
          - Validation
          <Class TestSimpleRoleMapping>
            Tests for SimpleRoleMapping
            <Function test_realm_role_mapping>
              Test mapping realm role to OpenFGA tuple
            <Function test_realm_role_not_matching>
              Test realm role that doesn't match
            <Function test_client_role_mapping>
              Test mapping client role to OpenFGA tuple
          <Class TestGroupMapping>
            Tests for GroupMapping
            <Function test_top_level_group_mapping>
              Test mapping top-level group to organization
            <Function test_nested_group_mapping>
              Test mapping nested groups to teams
            <Function test_group_pattern_no_match>
              Test group pattern that doesn't match
          <Class TestConditionalMapping>
            Tests for ConditionalMapping
            <Function test_equality_condition>
              Test conditional mapping with equality operator
            <Function test_inequality_condition>
              Test conditional mapping with != operator
            <Function test_in_condition>
              Test conditional mapping with 'in' operator
            <Function test_greater_equal_condition>
              Test conditional mapping with >= operator
            <Function test_condition_not_met>
              Test condition not met
            <Function test_missing_attribute>
              Test conditional mapping with missing attribute
          <Class TestRoleMapper>
            Tests for RoleMapper
            <Coroutine test_default_config>
              Test RoleMapper with default configuration
            <Coroutine test_yaml_config_loading>
              Test loading configuration from YAML file
            <Coroutine test_dict_config>
              Test loading configuration from dictionary
            <Coroutine test_role_hierarchies>
              Test role hierarchy expansion
            <Coroutine test_deduplication>
              Test that duplicate tuples are deduplicated
            <Coroutine test_multiple_rule_types>
              Test combining multiple rule types
            <Function test_add_rule_dynamically>
              Test adding rules dynamically
            <Function test_validate_config_valid>
              Test validation of valid configuration
            <Function test_validate_config_circular_hierarchy>
              Test validation detects circular hierarchy
            <Function test_validate_config_invalid_hierarchy_type>
              Test validation detects invalid hierarchy type
          <Class TestRoleMapperIntegration>
            Integration tests for role mapper
            <Coroutine test_realistic_enterprise_scenario>
              Test realistic enterprise role mapping scenario
        <Module test_service_principal_manager.py>
          Tests for Service Principal Manager

          Following TDD principles - these tests are written BEFORE implementation.
          Tests cover:
          - Service principal creation (both authentication modes)
          - User association and permission inheritance
          - Secret rotation
          - Listing and deletion
          - OpenFGA tuple synchronization
          <Class TestServicePrincipalCreation>
            Test service principal creation with different authentication modes
            <Coroutine test_create_service_principal_client_credentials_mode>
              Test creating service principal using client credentials flow
            <Coroutine test_create_service_principal_service_account_user_mode>
              Test creating service principal using service account user mode
            <Coroutine test_create_service_principal_with_user_association>
              Test creating service principal associated with user for permission inheritance
            <Coroutine test_create_service_principal_invalid_mode_raises_error>
              Test that invalid authentication mode raises ValueError
          <Class TestServicePrincipalUserAssociation>
            Test associating service principals with users
            <Coroutine test_associate_with_user>
              Test associating existing service principal with user
            <Coroutine test_associate_without_permission_inheritance>
              Test associating user without permission inheritance
          <Class TestServicePrincipalSecretRotation>
            Test secret rotation for service principals
            <Coroutine test_rotate_secret>
              Test rotating service principal secret
            <Coroutine test_rotate_secret_generates_different_secret>
              Test that rotation generates cryptographically unique secrets
          <Class TestServicePrincipalListing>
            Test listing service principals
            <Coroutine test_list_all_service_principals>
              Test listing all service principals
            <Coroutine test_list_service_principals_filtered_by_owner>
              Test listing service principals filtered by owner
          <Class TestServicePrincipalRetrieval>
            Test retrieving service principals (regression test for Codex finding)
            <Coroutine test_get_service_principal_service_account_user_uses_correct_method>
              Test that get_service_principal uses get_user_by_username for service account users.

              REGRESSION TEST for Codex finding: get_user() expects UUID, not username.
              Service account usernames like 'svc_my-service' are NOT UUIDs, so we must
              use get_user_by_username() instead of get_user().
          <Class TestServicePrincipalDeletion>
            Test deleting service principals
            <Coroutine test_delete_service_principal>
              Test deleting service principal removes from Keycloak and OpenFGA
          <Class TestServicePrincipalDataModel>
            Test ServicePrincipal data model
            <Function test_service_principal_dataclass>
              Test ServicePrincipal dataclass structure
            <Function test_service_principal_optional_fields>
              Test ServicePrincipal with optional fields omitted
        <Module test_user_provider.py>
          Comprehensive tests for UserProvider implementations

          Tests InMemoryUserProvider, KeycloakUserProvider, and the factory function.
          <Class TestInMemoryUserProvider>
            Test InMemoryUserProvider implementation
            <Function test_initialization>
              Test provider initialization (post Finding #2 fix: no default users)
            <Coroutine test_authenticate_success>
              Test successful authentication
            <Coroutine test_authenticate_user_not_found>
              Test authentication with non-existent user
            <Coroutine test_authenticate_inactive_user>
              Test authentication with inactive user
            <Coroutine test_get_user_by_id>
              Test getting user by ID
            <Coroutine test_get_user_by_id_not_found>
              Test getting non-existent user by ID
            <Coroutine test_get_user_by_username>
              Test getting user by username
            <Coroutine test_get_user_by_username_not_found>
              Test getting non-existent user by username
            <Coroutine test_list_users>
              Test listing all users
            <Function test_create_token_success>
              Test creating JWT token
            <Function test_create_token_user_not_found>
              Test creating token for non-existent user
            <Function test_create_token_expiration>
              Test token expiration is set correctly
            <Coroutine test_verify_token_success>
              Test successful token verification
            <Coroutine test_verify_token_expired>
              Test verification of expired token
            <Coroutine test_verify_token_invalid>
              Test verification of invalid token
            <Coroutine test_verify_token_wrong_secret>
              Test verification with wrong secret
          <Class TestKeycloakUserProvider>
            Test KeycloakUserProvider implementation
            <Function test_initialization>
              Test provider initialization
            <Function test_initialization_with_openfga>
              Test initialization with OpenFGA client
            <Coroutine test_authenticate_success>
              Test successful authentication
            <Coroutine test_authenticate_no_password>
              Test authentication without password
            <Coroutine test_authenticate_with_openfga_sync>
              Test authentication with OpenFGA synchronization
            <Coroutine test_authenticate_openfga_sync_failure>
              Test authentication continues even if OpenFGA sync fails
            <Coroutine test_authenticate_keycloak_error>
              Test authentication handles Keycloak errors
            <Coroutine test_get_user_by_id>
              Test getting user by ID
            <Coroutine test_get_user_by_username>
              Test getting user by username
            <Coroutine test_get_user_by_username_not_found>
              Test getting non-existent user
            <Coroutine test_get_user_error_handling>
              Test error handling when fetching user
            <Coroutine test_verify_token_success>
              Test successful token verification
            <Coroutine test_verify_token_invalid>
              Test verification of invalid token
            <Coroutine test_refresh_token_success>
              Test successful token refresh
            <Coroutine test_refresh_token_failure>
              Test token refresh failure
            <Coroutine test_list_users_not_implemented>
              Test list_users returns empty list (not implemented)
          <Class TestCreateUserProvider>
            Test create_user_provider factory function
            <Function test_create_inmemory_provider>
              Test creating InMemoryUserProvider
            <Function test_create_inmemory_provider_default>
              Test creating InMemoryUserProvider with defaults
            <Function test_create_keycloak_provider>
              Test creating KeycloakUserProvider
            <Function test_create_keycloak_provider_with_openfga>
              Test creating KeycloakUserProvider with OpenFGA client
            <Function test_create_keycloak_provider_missing_config>
              Test creating KeycloakUserProvider without config fails
            <Function test_create_provider_unknown_type>
              Test creating provider with unknown type
            <Function test_create_provider_case_insensitive>
              Test provider type is case-insensitive
          <Class TestUserProviderInterface>
            Test UserProvider abstract interface
            <Function test_cannot_instantiate_abstract_class>
              Test UserProvider cannot be instantiated directly
            <Coroutine test_inmemory_implements_interface>
              Test InMemoryUserProvider implements all abstract methods
            <Coroutine test_keycloak_implements_interface>
              Test KeycloakUserProvider implements all abstract methods
      <Dir config>
        <Module test_code_execution_config.py>
          Unit tests for code execution configuration settings

          Tests Settings class code execution configuration following TDD best practices.
          These tests should FAIL until config.py is updated with execution settings.
          <Class TestCodeExecutionSettings>
            Test code execution configuration settings
            <Function test_code_execution_disabled_by_default>
              Test that code execution is disabled by default (security)
            <Function test_code_execution_backend_default>
              Test default code execution backend
            <Function test_code_execution_timeout_default>
              Test default code execution timeout
            <Function test_code_execution_memory_limit_default>
              Test default memory limit
            <Function test_code_execution_cpu_quota_default>
              Test default CPU quota
            <Function test_code_execution_disk_quota_default>
              Test default disk quota
            <Function test_code_execution_max_processes_default>
              Test default max processes
            <Function test_code_execution_network_mode_default>
              Test default network mode
            <Function test_code_execution_allowed_domains_default>
              Test default allowed domains
            <Function test_code_execution_allowed_imports_default>
              Test default allowed imports
            <Function test_docker_image_default>
              Test default Docker image
            <Function test_docker_socket_path_default>
              Test default Docker socket path
            <Function test_kubernetes_namespace_default>
              Test default Kubernetes namespace
            <Function test_kubernetes_job_ttl_default>
              Test default Kubernetes job TTL
          <Class TestCodeExecutionSettingsFromEnv>
            Test code execution settings from environment variables
            <Function test_enable_code_execution_from_env>
              Test enabling code execution via environment
            <Function test_code_execution_backend_from_env>
              Test setting backend via environment
            <Function test_code_execution_timeout_from_env>
              Test setting timeout via environment
            <Function test_code_execution_memory_limit_from_env>
              Test setting memory limit via environment
            <Function test_code_execution_allowed_domains_from_env>
              Test setting allowed domains via environment (JSON format)
            <Function test_code_execution_allowed_imports_from_env>
              Test setting allowed imports via environment (JSON format)
          <Class TestCodeExecutionSettingsValidation>
            Test code execution settings validation
            <Function test_network_mode_validation>
              Test network mode must be valid value
            <Function test_backend_validation>
              Test backend must be valid value
            <Function test_timeout_must_be_positive>
              Test timeout validation
            <Function test_memory_limit_must_be_positive>
              Test memory limit validation
          <Class TestCodeExecutionSettingsIntegration>
            Test code execution settings integration with ResourceLimits
            <Function test_settings_compatible_with_resource_limits>
              Test that settings can be used to create ResourceLimits
            <Function test_settings_compatible_with_code_validator>
              Test that settings can be used to create CodeValidator
          <Class TestCodeExecutionSettingsDefaults>
            Test code execution default settings for different environments
            <Function test_development_defaults>
              Test development environment defaults
            <Function test_production_defaults>
              Test production environment defaults
            <Function test_test_defaults>
              Test testing environment defaults
        <Module test_config_validation.py>
          Tests for configuration validation

          Validates that fallback models have proper credentials configured
          and that missing credentials are detected at startup.
          <Class TestFallbackModelValidation>
            Test fallback model credential validation.
            <Function test_validate_fallback_credentials_all_present>
              Test that validation passes when all credentials are present.
            <Function test_validate_fallback_credentials_missing_anthropic>
              Test that missing Anthropic credentials are detected.
            <Function test_validate_fallback_credentials_missing_openai>
              Test that missing OpenAI credentials are detected.
            <Function test_validate_fallback_credentials_no_fallback_enabled>
              Test that validation is skipped when fallback is disabled.
            <Function test_validate_fallback_credentials_empty_list>
              Test that validation is skipped with empty fallback list.
            <Function test_validate_fallback_credentials_multiple_missing>
              Test detection of multiple missing credentials.
            <Function test_fallback_models_use_latest_stable>
              Test that default fallback models use latest stable production versions.
          <Class TestCORSValidation>
            Test CORS configuration validation.
            <Function test_cors_validation_wildcard_in_production>
              Test that wildcard CORS is rejected in production.
            <Function test_cors_validation_wildcard_in_development>
              Test that wildcard CORS is allowed but warned in development.
            <Function test_cors_validation_specific_origins>
              Test that specific origins are accepted.
        <Module test_postgres_connection_config.py>
          Test to prevent PostgreSQL connection configuration regressions.

          TDD Context:
          - RED (2025-01-17): Integration tests failing with connection errors to port 9432
          - Root cause: Password mismatch between docker-compose.test.yml and conftest.py fixtures
          - GREEN: Ensure all fixtures use consistent PostgreSQL connection parameters
          - REFACTOR: This test prevents configuration drift between docker-compose and test fixtures

          Following TDD: This test written FIRST to catch configuration mismatches.
          <Class TestPostgresConnectionConfig>
            Validate PostgreSQL connection configuration consistency.
            <Function test_docker_compose_and_conftest_use_same_password>
              Test: Docker-compose and conftest fixtures must use same PostgreSQL password.

              RED (Before Fix - 2025-01-17):
              - docker-compose.test.yml: POSTGRES_PASSWORD: postgres
              - conftest.py postgres_connection_clean: password default "test" (MISMATCH)
              - Integration tests fail: [Errno 111] Connect call failed

              GREEN (After Fix):
              - Both use "postgres" password
              - Integration tests connect successfully
            <Function test_docker_compose_and_conftest_use_same_port>
              Test: Docker-compose and conftest fixtures must use same port (9432).
            <Function test_docker_compose_and_conftest_use_same_user>
              Test: Docker-compose and conftest fixtures must use same user (postgres).
            <Function test_docker_compose_and_conftest_use_same_database>
              Test: Docker-compose creates databases that conftest fixtures connect to.
            <Function test_integration_test_workflow_sets_postgres_env_vars>
              Test: CI workflow must set all required PostgreSQL environment variables.
      <Dir core>
        <Module test_cache_isolation.py>
          🔴 RED → 🟢 GREEN PHASE - Tests for cache database isolation

          SECURITY FINDING #6: Cache invalidation scope issue.

          Both L2 cache and API key cache use Redis DB 2. When cache.clear() is called
          without a pattern, it uses flushdb() which clears the ENTIRE database,
          affecting BOTH caches (unintended data loss).

          Expected to FAIL until:
          1. API key cache moved to separate Redis DB (config.py api_key_cache_db: 2 → 3)
          2. flushdb() replaced with pattern-based deletion (cache.py)
          <Function test_api_key_cache_uses_separate_database>
            🟢 GREEN: Test that API key cache uses different Redis DB than L2 cache.

            PROBLEM: Both use DB 2, causing cache.clear() to affect API keys.

            This test should PASS after changing api_key_cache_db to 3.

            **pytest-xdist Isolation:**
            - Uses monkeypatch.setenv() for automatic environment cleanup

            References:
            - tests/regression/test_pytest_xdist_environment_pollution.py
            - OpenAI Codex Finding: test_cache_isolation.py:27
          <Function test_cache_clear_without_pattern_does_not_use_flushdb>
            🟢 GREEN: Test that cache.clear() without pattern uses pattern-based deletion, not flushdb().

            PROBLEM: cache.clear() without pattern calls flushdb() which clears entire Redis DB.
            This affects all data in that DB, including API key cache if sharing DB 2.

            This test should PASS after replacing flushdb() with pattern-based deletion.
          <Function test_cache_service_clear_with_pattern_works>
            Verify that cache.clear(pattern="foo:*") works correctly.

            This should work both before and after the fix.
        <Module test_config_network_security.py>
          🔴 RED PHASE - Test for network security configuration defaults

          SECURITY FINDING #3: Default network mode must be secure.

          This test validates that the default network mode for code execution is "none" (maximum isolation)
          rather than "allowlist" or "unrestricted". Users must explicitly opt-in to network access.

          Expected to FAIL until core/config.py:216 is fixed to default to "none".
          <Function test_default_network_mode_is_secure_none>
            🔴 RED: Test that Settings defaults code_execution_network_mode to "none".

            SECURITY CRITICAL: Code execution should default to maximum network isolation.

            This test will FAIL because core/config.py:216 currently defaults to "allowlist".
            Expected to PASS after fixing the default to "none".
          <Function test_network_mode_can_be_explicitly_set_to_allowlist>
            Verify that network mode CAN be explicitly set to allowlist if desired.

            This ensures the fix doesn't prevent legitimate use cases.
          <Function test_network_mode_can_be_explicitly_set_to_unrestricted>
            Verify that network mode CAN be explicitly set to unrestricted if desired.

            This ensures the fix doesn't prevent legitimate use cases.
        <Module test_context_manager.py>
          Tests for Context Manager (Conversation Compaction)

          Tests both unit functionality and integration behavior.
          Uses mocking to avoid actual LLM calls for fast execution.
          <Class TestContextManager>
            Unit tests for ContextManager.
            <Function test_initialization>
              Test ContextManager initializes with correct config.
            <Function test_needs_compaction_short_conversation>
              Test that short conversations don't trigger compaction.
            <Function test_needs_compaction_long_conversation>
              Test that long conversations trigger compaction.
            <Coroutine test_compact_conversation_structure>
              Test that compaction maintains proper message structure.
            <Coroutine test_compact_conversation_preserves_recent>
              Test that compaction preserves recent messages.
            <Coroutine test_compact_conversation_with_system_messages>
              Test that system messages are preserved during compaction.
            <Coroutine test_compact_conversation_no_older_messages>
              Test compaction with no older messages to summarize.
            <Coroutine test_summarization_calls_llm>
              Test that summarization actually calls the LLM.
            <Coroutine test_summarization_fallback_on_error>
              Test that summarization has fallback when LLM fails.
            <Function test_message_to_text>
              Test conversion of messages to text for token counting.
            <Function test_get_role_label>
              Test role label extraction from messages.
            <Function test_extract_key_information>
              Test extraction of key information from conversation.
            <Function test_extract_key_information_facts>
              Test extraction of facts from conversation.
            <Function test_extract_key_information_action_items>
              Test extraction of action items from conversation.
            <Function test_extract_key_information_preferences>
              Test extraction of user preferences from conversation.
            <Function test_extract_key_information_all_categories>
              Test that all 6 categories are populated when relevant content exists.
          <Class TestConvenienceFunctions>
            Test convenience functions.
            <Coroutine test_compact_if_needed_no_compaction>
              Test compact_if_needed doesn't compact short conversations.
            <Coroutine test_compact_if_needed_with_compaction>
              Test compact_if_needed compacts long conversations.
          <Class TestTokenCounting>
            Test token counting functionality.
            <Function test_token_counting_accuracy>
              Test that token counting is reasonably accurate.
          <Class TestCompactionResult>
            Test CompactionResult model.
            <Function test_compaction_result_creation>
              Test creating CompactionResult.
            <Function test_compaction_result_validation>
              Test CompactionResult validation.
      <Package documentation>
        <Module test_file_naming_validator.py>
          Test file naming validator for documentation.

          Following TDD principles:
          1. RED: Write failing tests first
          2. GREEN: Implement validator to make tests pass
          3. REFACTOR: Clean up code
          <Class TestFileNamingValidator>
            Test file naming convention validation.
            <Function test_lowercase_kebab_case_passes>
              Valid kebab-case filenames should pass.
            <Function test_uppercase_detected>
              UPPERCASE filenames should be detected.
            <Function test_snake_case_detected>
              snake_case filenames should be detected.
            <Function test_mixed_case_detected>
              mixedCase and PascalCase filenames should be detected.
            <Function test_conventional_files_excluded>
              Conventional files like README.md should be excluded.
            <Function test_suggests_correct_name>
              Error messages should include suggested correct name.
            <Function test_is_kebab_case_function>
              Test the is_kebab_case helper function.
            <Function test_find_invalid_filenames_in_docs>
              Test finding all invalid filenames in docs directory.
            <Function test_numbers_allowed_in_kebab_case>
              Numbers should be allowed in kebab-case filenames.
            <Function test_only_validates_docs_directory>
              Validator should only check files in docs/ directory.
            <Function test_md_files_not_allowed_in_docs>
              .md files should not be allowed in docs/ directory.
              docs/ is exclusively for .mdx files.
          <Class TestValidatorCLI>
            Test the CLI interface of the validator.
            <Function test_validator_exit_code_on_errors>
              Validator should exit with code 1 when errors found.
            <Function test_validator_exit_code_on_success>
              Validator should exit with code 0 when no errors.
          <Class TestEdgeCases>
            Test edge cases and special scenarios.
            <Function test_empty_filename>
              Empty filename should be handled gracefully.
            <Function test_hidden_files_allowed>
              Hidden files like .gitignore should be allowed.
            <Function test_multiple_hyphens_not_allowed>
              Multiple consecutive hyphens should NOT be allowed.
            <Function test_leading_or_trailing_hyphens_invalid>
              Filenames shouldn't start or end with hyphens.
        <Module test_frontmatter_validator.py>
          Unit tests for frontmatter validator.

          Tests validate that:
          1. All MDX files have valid YAML frontmatter
          2. Required fields (title, description) are present
          3. Frontmatter is properly formatted
          4. Optional fields are validated when present
          <Class TestFrontmatterValidator>
            Test suite for FrontmatterValidator.
            <Function test_valid_frontmatter_passes>
              Test that MDX files with valid frontmatter pass.
            <Function test_missing_frontmatter_detected>
              Test that files without frontmatter are detected.
            <Function test_missing_title_detected>
              Test that missing title field is detected.
            <Function test_missing_description_detected>
              Test that missing description field is detected.
            <Function test_invalid_yaml_detected>
              Test that invalid YAML frontmatter is detected.
            <Function test_empty_title_detected>
              Test that empty title is detected as invalid.
            <Function test_empty_description_detected>
              Test that empty description is detected as invalid.
            <Function test_template_files_excluded>
              Test that template files are excluded from validation.
            <Function test_multiple_errors_reported>
              Test that multiple files with errors are all reported.
            <Function test_optional_fields_allowed>
              Test that optional fields are allowed and don't cause errors.
            <Function test_statistics_accurate>
              Test that statistics are accurately collected.
            <Function test_nested_files_validated>
              Test that files in nested directories are validated.
        <Module test_mdx_extension_validator.py>
          Unit tests for MDX extension validator.

          Tests validate that:
          1. All files in docs/ directory use .mdx extension
          2. No .md files exist in docs/ (except specific exclusions)
          3. Template files are excluded from validation
          4. Proper error reporting
          <Class TestMDXExtensionValidator>
            Test suite for MDXExtensionValidator.
            <Function test_valid_mdx_files_pass>
              Test that directory with only .mdx files passes.
            <Function test_md_file_detected>
              Test that .md files in docs/ are detected as errors.
            <Function test_root_md_files_allowed>
              Test that .md files in root are allowed (README.md, etc).
            <Function test_nested_md_files_detected>
              Test that .md files in nested directories are detected.
            <Function test_exclude_patterns_work>
              Test that exclude patterns properly filter .md files.
            <Function test_multiple_md_files_all_reported>
              Test that all .md files are reported, not just the first.
            <Function test_mixed_files_only_md_reported>
              Test that only .md files are reported as errors when mixed with .mdx.
            <Function test_empty_docs_directory>
              Test that empty docs directory is valid.
            <Function test_cli_exit_codes>
              Test that CLI returns proper exit codes.
            <Function test_statistics_accurate>
              Test that statistics are accurately collected.
            <Function test_relative_paths_in_errors>
              Test that errors contain relative paths, not absolute.
        <Module test_todo_audit.py>
          Tests for TODO/FIXME Audit Script

          Following TDD principles:
          1. Test finding TODO/FIXME markers
          2. Test categorization
          3. Test reporting
          4. Test exclusion patterns
          <Class TestTodoAuditor>
            Test suite for TodoAuditor.
            <Function test_empty_directory_has_no_todos>
              Test that empty directory has no TODOs.
            <Function test_finds_todo_marker>
              Test finding TODO markers.
            <Function test_finds_fixme_marker>
              Test finding FIXME markers.
            <Function test_finds_xxx_marker>
              Test finding XXX placeholder markers.
            <Function test_finds_multiple_markers_in_file>
              Test finding multiple markers in a single file.
            <Function test_tracks_line_numbers>
              Test that line numbers are tracked accurately.
            <Function test_handles_nested_directories>
              Test auditing nested directory structure.
            <Function test_excludes_template_files>
              Test that template files are excluded.
            <Function test_processes_multiple_files>
              Test processing multiple files.
            <Function test_statistics_are_accurate>
              Test that statistics are calculated correctly.
            <Function test_nonexistent_directory>
              Test behavior with nonexistent directory.
          <Class TestTodoMarker>
            Test suite for TodoMarker dataclass.
            <Function test_marker_attributes>
              Test TodoMarker attributes.
          <Class TestAuditResult>
            Test suite for AuditResult dataclass.
            <Function test_audit_result_defaults>
              Test AuditResult default values.
            <Function test_audit_result_with_data>
              Test AuditResult with data.
      <Package execution>
        <Module test_code_validator.py>
          Unit tests for code validator

          Tests AST-based validation, import whitelisting, and security controls.
          Following TDD best practices - these tests should FAIL until implementation is complete.
          <Class TestCodeValidator>
            Test suite for CodeValidator
            <Function test_validator_initialization>
              Test validator initializes with allowed imports
            <Function test_simple_valid_code>
              Test validation of simple safe code
            <Function test_valid_code_with_allowed_import>
              Test code with allowed import passes validation
            <Function test_valid_code_with_multiple_imports>
              Test code with multiple allowed imports
            <Function test_reject_dangerous_import_os>
              Test that os module import is blocked
            <Function test_reject_dangerous_import_subprocess>
              Test that subprocess import is blocked
            <Function test_reject_dangerous_import_sys>
              Test that sys import is blocked
            <Function test_reject_eval_usage>
              Test that eval() calls are blocked
            <Function test_reject_exec_usage>
              Test that exec() calls are blocked
            <Function test_reject_compile_usage>
              Test that compile() calls are blocked
            <Function test_reject_dunder_import>
              Test that __import__() is blocked
            <Function test_reject_from_import_dangerous>
              Test that 'from os import system' is blocked
            <Function test_reject_os_system_via_attribute>
              Test detection of dangerous attribute access patterns
            <Function test_allow_safe_builtins>
              Test that safe builtin functions are allowed
            <Function test_syntax_error_detection>
              Test detection of syntax errors
            <Function test_empty_code>
              Test validation of empty code
            <Function test_whitespace_only_code>
              Test validation of whitespace-only code
            <Function test_valid_pandas_usage>
              Test that pandas operations are allowed
            <Function test_valid_numpy_usage>
              Test that numpy operations are allowed
            <Function test_reject_import_not_in_whitelist>
              Test that imports not in whitelist are blocked
            <Function test_reject_open_file_access>
              Test that open() calls are blocked (file access)
            <Function test_reject_input_function>
              Test that input() is blocked (can't interact in sandbox)
            <Function test_reject_globals_access>
              Test that globals() access is blocked
            <Function test_reject_locals_access>
              Test that locals() access is blocked
            <Function test_reject_vars_access>
              Test that vars() access is blocked
            <Function test_validation_result_structure>
              Test ValidationResult structure
            <Function test_multiple_validation_errors>
              Test that multiple errors are collected
            <Function test_nested_dangerous_call>
              Test detection of nested dangerous calls
            <Function test_lambda_with_eval>
              Test detection of eval in lambda
            <Function test_list_comprehension_safe>
              Test that safe list comprehensions work
            <Function test_dict_comprehension_safe>
              Test that safe dict comprehensions work
            <Function test_generator_expression_safe>
              Test that safe generator expressions work
            <Function test_class_definition_safe>
              Test that safe class definitions work
            <Function test_async_code_safe>
              Test that safe async code works
            <Function test_context_manager_safe>
              Test that safe context managers work
          <Class TestSecurityInjectionPatterns>
            Security-focused tests for injection attack patterns
            <Function test_reject_pickle_import>
              Test that pickle module is blocked (deserialization attack)
            <Function test_reject_importlib_usage>
              Test that importlib is blocked (dynamic import)
            <Function test_reject_base64_exec_pattern>
              Test detection of base64 + exec pattern
            <Function test_reject_string_formatting_with_exec>
              Test detection of string format + exec
            <Function test_reject_getattr_attribute_access>
              Test that getattr with dangerous names is blocked
            <Function test_reject_setattr_usage>
              Test that setattr is blocked (can modify objects)
            <Function test_reject_delattr_usage>
              Test that delattr is blocked
          <Class TestCodeValidatorProperties>
            Property-based tests using Hypothesis for fuzzing
            <Function test_validator_never_crashes>
              Property: validator should never crash on any input
            <Function test_random_text_mostly_invalid>
              Property: random text should mostly be invalid or have syntax errors
            <Function test_dangerous_patterns_always_rejected>
              Property: known dangerous patterns always rejected
            <Function test_safe_patterns_always_accepted>
              Property: known safe patterns always accepted
          <Class TestValidationResult>
            Test ValidationResult data class
            <Function test_validation_result_creation>
              Test creating ValidationResult
            <Function test_validation_result_with_errors>
              Test ValidationResult with errors
            <Function test_validation_result_repr>
              Test ValidationResult string representation
          <Class TestCodeValidationError>
            Test CodeValidationError exception
            <Function test_code_validation_error_creation>
              Test creating CodeValidationError
            <Function test_code_validation_error_inheritance>
              Test that CodeValidationError inherits from Exception
        <Module test_docker_sandbox_unit.py>
          Unit tests for Docker sandbox with mocked Docker client.

          Tests focus on security-critical logic without requiring Docker.
          Following TDD - targets low-coverage methods from Codex audit.

          Coverage targets:
          - _get_network_mode() - Network isolation security logic (~2% coverage)
          - Security configurations and error handling
          <Class TestDockerSandboxNetworkMode>
            Test network mode security logic (security-critical)
            <Function test_get_network_mode_none_is_secure_default>
              Test that network mode 'none' returns 'none' (maximum isolation)

              SECURITY: Validates default secure configuration
            <Function test_get_network_mode_unrestricted_returns_bridge>
              Test that network mode 'unrestricted' returns 'bridge'

              SECURITY: Explicit opt-in to network access required
            <Function test_get_network_mode_allowlist_fails_closed_to_none>
              Test that unimplemented 'allowlist' mode fails closed to 'none'

              SECURITY CRITICAL: CWE-1188 mitigation - unimplemented features fail safely
          <Class TestDockerSandboxInitialization>
            Test sandbox initialization and Docker client connection
            <Function test_init_connects_to_docker_successfully>
              Test that initialization connects to Docker daemon
            <Function test_init_handles_docker_unavailable_gracefully>
              Test graceful failure when Docker is not available

              Error handling: Should provide clear error message
            <Function test_init_uses_custom_image_when_specified>
              Test that custom Docker image can be specified
          <Class TestDockerSandboxErrorHandling>
            Test error handling for Docker operations
            <Function test_handles_image_not_found_with_clear_error>
              Test graceful handling when Docker image is not available

              Error handling: Should provide actionable error message during image pull
            <Function test_cleanup_handles_already_removed_container>
              Test cleanup handles containers that are already removed

              Idempotency: Cleanup should be safe to call multiple times
            <Function test_cleanup_resilient_to_stop_errors>
              Test cleanup handles errors when stopping container

              Error handling: Should still attempt to remove even if stop fails
        <Module test_network_mode_logic.py>
          Unit tests for network mode logic in DockerSandbox

          Tests the _get_network_mode method to ensure it fails closed when allowlist is not implemented.
          These tests don't require Docker to be installed.
          <Function test_allowlist_mode_fails_closed_with_domains>
            🟢 GREEN: Test that allowlist mode returns "none" when domains are specified.

            SECURITY: Unimplemented allowlist must fail closed, not fall back to bridge (unrestricted).
          <Function test_allowlist_mode_with_no_domains_returns_none>
            Test that allowlist mode with no domains returns "none".
          <Function test_network_mode_none_returns_none>
            Test that network_mode="none" returns "none".
          <Function test_network_mode_unrestricted_returns_bridge>
            Test that network_mode="unrestricted" returns "bridge".
        <Module test_resource_limits.py>
          Unit tests for resource limits configuration

          Tests timeout, memory, CPU quota validation and enforcement.
          Following TDD best practices - these tests should FAIL until implementation is complete.
          <Class TestResourceLimits>
            Test suite for ResourceLimits configuration
            <Function test_default_resource_limits>
              Test default resource limit values
            <Function test_custom_resource_limits>
              Test creating custom resource limits
            <Function test_timeout_validation_positive>
              Test that timeout must be positive
            <Function test_timeout_maximum_limit>
              Test that timeout has a reasonable maximum
            <Function test_memory_limit_validation_positive>
              Test that memory limit must be positive
            <Function test_memory_limit_minimum>
              Test that memory limit has a reasonable minimum
            <Function test_memory_limit_maximum>
              Test that memory limit has a reasonable maximum
            <Function test_cpu_quota_validation_positive>
              Test that CPU quota must be positive
            <Function test_cpu_quota_maximum>
              Test that CPU quota has a reasonable maximum
            <Function test_cpu_quota_fractional>
              Test that CPU quota accepts fractional values
            <Function test_resource_limits_immutable>
              Test that resource limits are immutable after creation
            <Function test_resource_limits_repr>
              Test string representation of resource limits
            <Function test_resource_limits_dict_conversion>
              Test converting resource limits to dictionary
            <Function test_resource_limits_from_dict>
              Test creating resource limits from dictionary
            <Function test_network_access_control>
              Test network access mode configuration
            <Function test_network_mode_validation>
              Test that network mode must be valid
            <Function test_network_allowlist_with_empty_domains>
              Test that allowlist mode allows empty domains (blocks all network)
            <Function test_disk_quota_configuration>
              Test disk quota configuration
            <Function test_disk_quota_validation>
              Test disk quota validation
            <Function test_max_processes_configuration>
              Test maximum process limit configuration
            <Function test_max_processes_validation>
              Test maximum process validation
          <Class TestResourceLimitsProperties>
            Property-based tests for resource limits
            <Function test_valid_timeout_range_accepted>
              Property: all timeouts in valid range should be accepted
            <Function test_valid_memory_range_accepted>
              Property: all memory limits in valid range should be accepted
            <Function test_valid_cpu_quota_range_accepted>
              Property: all CPU quotas in valid range should be accepted
            <Function test_non_positive_timeout_rejected>
              Property: all non-positive timeouts should be rejected
            <Function test_non_positive_memory_rejected>
              Property: all non-positive memory limits should be rejected
          <Class TestResourceLimitError>
            Test ResourceLimitError exception
            <Function test_resource_limit_error_creation>
              Test creating ResourceLimitError
            <Function test_resource_limit_error_inheritance>
              Test that ResourceLimitError inherits from Exception
          <Class TestPresetResourceProfiles>
            Test preset resource profiles for common use cases
            <Function test_development_profile>
              Test development profile (relaxed limits)
            <Function test_production_profile>
              Test production profile (strict limits)
            <Function test_testing_profile>
              Test testing profile (minimal limits)
            <Function test_data_processing_profile>
              Test data processing profile (high memory, more CPU)
          <Class TestResourceLimitsComparison>
            Test resource limits comparison and validation
            <Function test_limits_equality>
              Test that identical limits are equal
            <Function test_limits_inequality>
              Test that different limits are not equal
            <Function test_is_within_limits>
              Test checking if one limit set is within another
          <Class TestResourceLimitsParameterNameValidation>
            Regression tests for ResourceLimits parameter naming

            CODEX FINDING: OpenAI Codex identified that test_docker_sandbox.py:693
            was using incorrect parameter names (max_memory_mb, max_cpu_percent)
            instead of the correct names (memory_limit_mb, cpu_quota).

            These tests ensure that incorrect parameter names are rejected,
            preventing this issue from recurring.
            <Function test_rejects_max_memory_mb_parameter>
              Test that using old/incorrect 'max_memory_mb' parameter raises TypeError

              REGRESSION: Prevents using deprecated parameter name from Codex finding
            <Function test_rejects_max_cpu_percent_parameter>
              Test that using old/incorrect 'max_cpu_percent' parameter raises TypeError

              REGRESSION: Prevents using deprecated parameter name from Codex finding
            <Function test_rejects_both_incorrect_parameters>
              Test that using both incorrect parameters raises TypeError

              REGRESSION: Ensures the exact error from test_docker_sandbox.py:693 is caught
            <Function test_correct_parameter_names_work>
              Test that correct parameter names work as expected

              This documents the correct usage pattern to replace incorrect names
            <Function test_cpu_quota_is_float_not_percentage>
              Test that cpu_quota uses cores (float), not percentage (int)

              DOCUMENTATION: cpu_quota represents CPU cores (0.5 = half a core),
              not a percentage (50% = 50). This is intentional.
      <Dir health>
        <Module test_database_checks.py>
          Unit tests for database validation module.

          Tests the database architecture validation logic from ADR-0056.

          Test Coverage:
          - Environment detection (dev, test, staging, production)
          - Database existence validation
          - Table existence validation
          - Error handling and connection failures
          - Validation result aggregation
          - Integration with PostgreSQL

          References:
              - ADR-0056: Database Architecture and Naming Convention
              - src/mcp_server_langgraph/health/database_checks.py
          <Class TestEnvironmentDetection>
            Tests for environment detection logic
            <Function test_detect_environment_from_ENVIRONMENT_variable>
              Should detect environment from ENVIRONMENT variable
            <Function test_detect_environment_from_TESTING_variable>
              Should detect test environment from TESTING=true
            <Function test_detect_environment_from_PYTEST_CURRENT_TEST>
              Should detect test environment from PYTEST_CURRENT_TEST
            <Function test_detect_environment_from_POSTGRES_DB_suffix>
              Should detect test environment from POSTGRES_DB ending with _test
            <Function test_detect_environment_from_POSTGRES_DB_openfga_test>
              Should detect test environment from POSTGRES_DB=openfga_test (backward compat)
            <Function test_detect_environment_defaults_to_dev>
              Should default to development environment
            <Function test_explicit_environment_override>
              Should use explicitly provided environment
          <Class TestExpectedDatabasesConfiguration>
            Tests for expected database configuration
            <Function test_get_expected_databases_dev_environment>
              Should return databases without _test suffix in dev environment
            <Function test_get_expected_databases_test_environment>
              Should return databases with _test suffix in test environment
            <Function test_gdpr_database_info_structure>
              Should configure GDPR database with correct metadata
            <Function test_openfga_database_info_structure>
              Should configure OpenFGA database with correct metadata
            <Function test_keycloak_database_info_structure>
              Should configure Keycloak database with correct metadata
          <Class TestDatabaseValidation>
            Tests for database validation logic
            <Coroutine test_validate_database_when_database_exists_and_all_tables_present>
              Should return valid result when database exists with all required tables
            <Coroutine test_validate_database_when_database_does_not_exist>
              Should return error when database does not exist
            <Coroutine test_validate_database_when_migration_managed_tables_missing>
              Should return error when migration-managed tables are missing
            <Coroutine test_validate_database_when_service_managed_tables_missing>
              Should return warning (not error) when service-managed tables are missing
            <Coroutine test_validate_database_when_connection_fails>
              Should return error when PostgreSQL connection fails
          <Class TestOverallValidation>
            Tests for overall validation of all databases
            <Coroutine test_validate_all_databases_when_all_valid>
              Should return overall valid result when all databases are valid
            <Coroutine test_validate_aggregates_errors_from_all_databases>
              Should aggregate all errors from database validations
          <Class TestValidationResultSerialization>
            Tests for validation result serialization
            <Function test_validation_result_to_dict>
              Should serialize validation result to dictionary
          <Class TestConvenienceFunction>
            Tests for the convenience function
            <Coroutine test_validate_database_architecture_convenience_function>
              Should provide convenient validation with default parameters
            <Coroutine test_validate_database_architecture_with_custom_parameters>
              Should accept custom connection parameters
          <Class TestDatabaseValidationResultProperty>
            Tests for DatabaseValidationResult.is_valid property
            <Function test_is_valid_when_exists_and_no_errors>
              Should be valid when database exists and has no errors
            <Function test_is_valid_false_when_database_does_not_exist>
              Should be invalid when database doesn't exist
            <Function test_is_valid_false_when_errors_present>
              Should be invalid when errors are present
      <Dir llm>
        <Module test_llm_factory_contract.py>
          Contract tests for LLMFactory._format_messages

          These tests validate the message formatting contract without using AsyncMock,
          ensuring the method correctly handles various input types including edge cases
          that caused the string-to-character-list bug.
          <Class TestFormatMessagesContract>
            Test _format_messages method with various input types.
            <Function test_format_single_human_message>
              Test formatting a single HumanMessage.
            <Function test_format_single_ai_message>
              Test formatting a single AIMessage.
            <Function test_format_single_system_message>
              Test formatting a single SystemMessage.
            <Function test_format_mixed_message_types>
              Test formatting a conversation with mixed message types.
            <Function test_format_dict_message_with_role_and_content>
              Test formatting a dict message that's already in correct format.
            <Function test_format_dict_message_with_content_only>
              Test formatting a dict with content but no role.
            <Function test_format_malformed_dict_message>
              Test formatting a malformed dict message.
            <Function test_format_empty_list>
              Test formatting an empty message list.
            <Function test_format_preserves_long_content>
              Test that formatting preserves long content without truncation.
            <Function test_format_multiline_content>
              Test formatting messages with multiline content.
            <Function test_format_special_characters>
              Test formatting messages with special characters.
            <Function test_format_unicode_content>
              Test formatting messages with Unicode characters.
            <Function test_format_xml_structured_prompt>
              Test formatting XML-structured prompts (Anthropic best practice).
          <Class TestFormatMessagesEdgeCases>
            Test edge cases that previously caused bugs.
            <Function test_format_does_not_accept_raw_strings>
              Test that passing a raw string is handled (but produces incorrect output).

              REGRESSION TEST: Previously, passing a string would iterate character-by-character.
              This test documents the incorrect behavior to prevent regression.

              NOTE: The _format_messages method doesn't validate input types - it just handles
              whatever is passed. The CORRECT usage is shown in test_correct_usage_wrap_string_in_message.
            <Function test_correct_usage_wrap_string_in_message>
              Test the CORRECT way to use _format_messages with a string prompt.

              Strings must be wrapped in a HumanMessage object.
            <Function test_format_with_custom_message_class>
              Test formatting with a custom BaseMessage subclass.
      <Dir observability>
        <Module test_json_logger.py>
          Unit tests for JSON logger with OpenTelemetry trace injection

          Tests the CustomJSONFormatter class and structured logging functionality.
          <Class TestCustomJSONFormatter>
            Tests for CustomJSONFormatter class
            <Function test_basic_formatting>
              Test basic JSON formatting without trace context
            <Function test_timestamp_format>
              Test ISO 8601 timestamp with milliseconds
            <Function test_trace_context_injection>
              Test OpenTelemetry trace context injection
            <Function test_process_and_thread_info>
              Test process and thread information
            <Function test_location_info>
              Test file location information
            <Function test_custom_extra_fields>
              Test custom extra fields via logging.extra
            <Function test_exception_handling>
              Test exception stack trace capture
            <Function test_pretty_print_formatting>
              Test pretty-print JSON formatting
            <Function test_compact_formatting>
              Test compact JSON formatting (no indentation)
            <Function test_hostname_inclusion>
              Test hostname field inclusion
            <Function test_no_trace_context>
              Test logging without active trace context
          <Class TestSetupJSONLogging>
            Tests for setup_json_logging helper function
            <Function test_setup_json_logging>
              Test logger setup with JSON formatting
            <Function test_logger_output>
              Test actual logger output is valid JSON
          <Class TestLogWithContext>
            Tests for log_with_context helper function
            <Function test_log_with_context>
              Test logging with custom context fields
          <Class TestEdgeCases>
            Tests for edge cases and error handling
            <Function test_empty_message>
              Test handling of empty log message
            <Function test_unicode_message>
              Test handling of Unicode characters
            <Function test_large_extra_data>
              Test handling of large custom data
            <Function test_special_characters_in_fields>
              Test handling of special characters in field values
          <Class TestPerformance>
            Performance tests for JSON logging
            <Function test_formatting_performance>
              Benchmark JSON formatting performance
            <Function test_formatting_with_trace_performance>
              Benchmark JSON formatting with active trace
        <Module test_json_logger_additional.py>
          Additional tests for JSON logger to increase coverage.

          These tests target uncovered code paths in json_logger.py:
          - ImportError fallback for pythonjsonlogger
          - Hostname socket error handling
          - Invalid span context handling
          - Edge cases in add_fields method
          <Class TestImportFallback>
            Tests for pythonjsonlogger import fallback
            <Function test_fallback_to_standard_formatter>
              Test that formatter falls back to standard logging.Formatter when pythonjsonlogger unavailable
          <Class TestHostnameHandling>
            Tests for hostname field handling
            <Function test_hostname_exception_handling>
              Test hostname error handling when socket.gethostname() fails
            <Function test_hostname_disabled>
              Test that hostname is not included when include_hostname=False
          <Class TestTraceContextEdgeCases>
            Tests for OpenTelemetry trace context edge cases
            <Function test_invalid_span_context>
              Test handling of invalid span context
            <Function test_no_span_context>
              Test handling when span.get_span_context() returns None
            <Function test_no_active_span>
              Test handling when there's no active span
          <Class TestExceptionHandlingEdgeCases>
            Tests for exception info handling edge cases
            <Function test_exception_with_none_values>
              Test handling of exc_info tuple with None values
            <Function test_exception_info_as_true>
              Test handling of exc_info=True (rare case)
          <Class TestAddFieldsCustomization>
            Tests for add_fields method customization
            <Function test_timestamp_override>
              Test that existing timestamp is not overridden
            <Function test_extra_fields_with_underscores>
              Test that fields starting with underscore are excluded
            <Function test_standard_fields_excluded_from_extra>
              Test that standard logging fields are not duplicated in output
          <Class TestFormatterInitialization>
            Tests for formatter initialization options
            <Function test_custom_datefmt>
              Test formatter with custom date format
            <Function test_custom_fmt_string>
              Test formatter with custom format string
            <Function test_different_style_formats>
              Test formatter with different format styles
          <Class TestSetupJSONLoggingEdgeCases>
            Tests for setup_json_logging edge cases
            <Function test_setup_replaces_existing_handlers>
              Test that setup removes existing handlers
            <Function test_setup_with_all_options>
              Test setup with all optional parameters
          <Class TestMessageFormatting>
            Tests for getMessage() and format string handling
            <Function test_message_with_args>
              Test message formatting with arguments
            <Function test_message_with_dict_args>
              Test message formatting with dictionary arguments
          <Class TestJSONSerializationEdgeCases>
            Tests for JSON serialization edge cases
            <Function test_non_serializable_extra_field>
              Test handling of non-JSON-serializable extra fields
            <Function test_datetime_in_extra_field>
              Test datetime objects in extra fields
      <Package resilience>
        <Module test_rate_limiter.py>
          Unit tests for rate_limiter.py - Rate Limiting Middleware
          <Class TestRateLimiterUserExtraction>
            Test extracting user info from JWT tokens for rate limiting
            <Function test_get_user_id_from_jwt_with_valid_token>
              Test extracting user ID from request.state.user set by AuthMiddleware
            <Function test_get_user_id_from_jwt_with_user_id_claim>
              Test extracting user_id claim from request.state.user
            <Function test_get_user_id_from_jwt_with_expired_token>
              Test that user ID is extracted even with expired token data in state
            <Function test_get_user_id_from_jwt_with_invalid_token>
              Test handling when AuthMiddleware didn't set user (invalid token)
            <Function test_get_user_id_from_jwt_without_bearer_token>
              Test handling request without user in state (no auth)
            <Function test_get_user_id_from_jwt_with_malformed_header>
              Test handling when state.user is not set (malformed header)
          <Class TestRateLimiterTierExtraction>
            Test tier extraction for tiered rate limiting
            <Function test_get_user_tier_for_premium_user>
              Test extracting premium tier from request.state.user
            <Function test_get_user_tier_for_enterprise_user>
              Test extracting enterprise tier from request.state.user
            <Function test_get_user_tier_defaults_to_free>
              Test default tier when user has no roles or tier field
            <Function test_get_user_tier_for_anonymous_user>
              Test anonymous tier when no user in state
            <Function test_get_user_tier_with_plan_claim>
              Test extracting tier from 'plan' field (fallback from roles)
            <Function test_get_user_tier_rejects_invalid_tiers>
              Test that invalid tier names default to free
          <Class TestRateLimiterKeyGeneration>
            Test rate limit key generation (user > IP > global)
            <Function test_get_rate_limit_key_for_authenticated_user>
              Test rate limit key uses user ID for authenticated users
            <Function test_get_rate_limit_key_for_anonymous_by_ip>
              Test rate limit key uses IP address for anonymous users
            <Function test_get_rate_limit_key_fallback_to_global>
              Test rate limit key falls back to global when no user ID or IP
          <Class TestRateLimiterTierLimits>
            Test tiered rate limit values
            <Function test_rate_limit_for_tier_anonymous>
              Test anonymous users get lowest limit
            <Function test_rate_limit_for_tier_free>
              Test free tier users get basic limit
            <Function test_rate_limit_for_tier_standard>
              Test standard tier users get higher limit
            <Function test_rate_limit_for_tier_premium>
              Test premium tier users get generous limit
            <Function test_rate_limit_for_tier_enterprise>
              Test enterprise tier users get unlimited access
            <Function test_rate_limit_for_invalid_tier_defaults_to_free>
              Test invalid tier defaults to free tier limit
          <Class TestDynamicRateLimiting>
            Test dynamic rate limit determination
            <Function test_get_dynamic_limit_for_premium_user>
              Test dynamic rate limiting applies premium limits
            <Function test_get_dynamic_limit_for_anonymous_user>
              Test dynamic rate limiting applies anonymous limits
            <Function test_get_dynamic_limit_for_enterprise_user>
              Test dynamic rate limiting applies enterprise limits
          <Class TestRateLimiterSecurityProperties>
            Test security properties of rate limiter
            <Function test_rate_limiter_does_not_expose_secret_key>
              SECURITY: Ensure rate limiter doesn't expose JWT secret in errors.

              Rate limiting should gracefully handle errors without leaking sensitive info.
            <Function test_rate_limiter_prevents_tier_escalation>
              SECURITY: Ensure users cannot escalate their tier via crafted JWTs.

              Invalid tiers should be rejected and downgraded to free tier.
            <Function test_rate_limiter_handles_jwt_signature_verification_failure>
              SECURITY: Ensure rate limiter handles JWT signature verification failures.

              Tokens signed with wrong key should be rejected by AuthMiddleware (treated as anonymous).
        <Module test_resilience_metrics.py>
          Unit tests for resilience metrics module.

          Tests OpenTelemetry metrics recording for all resilience patterns:
          - Circuit breaker metrics
          - Retry metrics
          - Timeout metrics
          - Bulkhead metrics
          - Fallback metrics
          <Class TestCircuitBreakerMetrics>
            Tests for circuit breaker metric recording
            <Function test_record_success_event>
              Test recording circuit breaker success event
            <Function test_record_failure_event>
              Test recording circuit breaker failure event
            <Function test_record_failure_without_exception_type>
              Test recording failure without exception type
            <Function test_record_state_change_event>
              Test recording circuit breaker state change
          <Class TestRetryMetrics>
            Tests for retry metric recording
            <Function test_record_retry_attempt>
              Test recording retry attempt
            <Function test_record_retry_exhausted>
              Test recording retry exhaustion
            <Function test_record_success_after_retry>
              Test recording successful retry
            <Function test_record_retry_without_optional_fields>
              Test recording retry event without optional fields
          <Class TestTimeoutMetrics>
            Tests for timeout metric recording
            <Function test_record_timeout_violation>
              Test recording timeout exceeded event
            <Function test_record_timeout_for_different_operations>
              Test recording timeouts for different operation types
          <Class TestBulkheadMetrics>
            Tests for bulkhead metric recording
            <Function test_record_bulkhead_rejection>
              Test recording bulkhead rejection
            <Function test_record_active_operations>
              Test recording active operations count
            <Function test_record_queue_depth>
              Test recording queue depth
            <Function test_record_active_without_count>
              Test recording active event without count (should not call set)
            <Function test_record_queued_without_depth>
              Test recording queued event without depth (should not call set)
          <Class TestFallbackMetrics>
            Tests for fallback metric recording
            <Function test_record_default_fallback>
              Test recording default fallback usage
            <Function test_record_function_fallback>
              Test recording function fallback usage
            <Function test_record_cache_fallback>
              Test recording cache fallback with cache hit counter
            <Function test_record_non_cache_fallback_no_cache_hit>
              Test non-cache fallback doesn't increment cache hits
          <Class TestMetricsSummary>
            Tests for metrics summary and export functions
            <Function test_get_resilience_metrics_summary>
              Test getting metrics summary
            <Function test_export_prometheus_format>
              Test Prometheus format export
          <Class TestMetricsIntegration>
            Integration tests for metrics recording
            <Function test_multiple_circuit_breaker_events>
              Test recording multiple circuit breaker events
            <Function test_retry_workflow>
              Test complete retry workflow metrics
            <Function test_bulkhead_lifecycle>
              Test bulkhead full lifecycle metrics
          <Class TestMetricsOpenTelemetryIntegration>
            Tests for OpenTelemetry metrics integration
            <Function test_metrics_have_correct_names>
              Verify all metrics have correct OpenTelemetry names
            <Function test_meter_is_initialized>
              Test that OpenTelemetry meter is initialized
          <Class TestEdgeCases>
            Tests for edge cases and error handling
            <Function test_record_event_with_empty_strings>
              Test recording events with empty string values
            <Function test_record_timeout_with_zero_timeout>
              Test recording timeout with zero timeout value
            <Function test_record_bulkhead_with_negative_counts>
              Test recording bulkhead with negative values (edge case)
        <Module test_response_optimizer.py>
          Tests for ResponseOptimizer utility

          Tests token counting, response truncation, format control,
          and high-signal information extraction.
          <Class TestResponseOptimizer>
            Test ResponseOptimizer class.
            <Function test_initialization>
              Test ResponseOptimizer initialization.
            <Function test_initialization_with_model>
              Test ResponseOptimizer with specific model.
            <Function test_count_tokens_simple>
              Test token counting with simple text.
            <Function test_count_tokens_empty>
              Test token counting with empty string.
            <Function test_count_tokens_long_text>
              Test token counting with long text.
            <Function test_truncate_response_no_truncation_needed>
              Test truncation when text is under limit.
            <Function test_truncate_response_with_truncation>
              Test truncation when text exceeds limit.
            <Function test_truncate_response_custom_message>
              Test truncation with custom truncation message.
            <Function test_format_response_concise>
              Test format_response with concise format.
            <Function test_format_response_detailed>
              Test format_response with detailed format.
            <Function test_format_response_short_text>
              Test format_response with text shorter than limits.
            <Function test_format_response_custom_max_tokens>
              Test format_response with custom max_tokens.
            <Function test_extract_high_signal_basic>
              Test high-signal extraction with basic data.
            <Function test_extract_high_signal_custom_exclude>
              Test high-signal extraction with custom exclude list.
            <Function test_extract_high_signal_empty_data>
              Test high-signal extraction with empty data.
          <Class TestGlobalFunctions>
            Test global convenience functions.
            <Function test_count_tokens_global>
              Test global count_tokens function.
            <Function test_truncate_response_global>
              Test global truncate_response function.
            <Function test_format_response_global>
              Test global format_response function.
            <Function test_extract_high_signal_global>
              Test global extract_high_signal function.
          <Class TestEdgeCases>
            Test edge cases and error conditions.
            <Function test_very_long_text>
              Test with text much longer than max tokens.
            <Function test_unicode_text>
              Test with Unicode characters.
            <Function test_special_characters>
              Test with special characters and markdown.
            <Function test_truncation_preserves_text_beginning>
              Test that truncation keeps the beginning of text.
          <Class TestPerformance>
            Test performance characteristics.
            <Function test_token_counting_performance>
              Test that token counting is reasonably fast.
            <Function test_truncation_performance>
              Test that truncation is reasonably fast.
          <Function test_format_response_parametrized[Short text-concise-False]>
            Parametrized test for format_response.
          <Function test_format_response_parametrized[Short text-detailed-False]>
            Parametrized test for format_response.
          <Function test_format_response_parametrized[Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word -concise-True]>
            Parametrized test for format_response.
          <Function test_format_response_parametrized[Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word Word -detailed-True]>
            Parametrized test for format_response.
          <Function test_extract_high_signal_parametrized[data0-should_contain0-should_not_contain0]>
            Parametrized test for extract_high_signal.
          <Function test_extract_high_signal_parametrized[data1-should_contain1-should_not_contain1]>
            Parametrized test for extract_high_signal.
          <Function test_extract_high_signal_parametrized[data2-should_contain2-should_not_contain2]>
            Parametrized test for extract_high_signal.
          <Class TestLiteLLMTokenCounting>
            Test LiteLLM-based token counting for model-aware accuracy.

            TDD RED phase: Tests written FIRST to define expected behavior for LiteLLM integration.
            Will fail until tiktoken is replaced with litellm.token_counter().
            <Function test_count_tokens_openai_model>
              Test token counting for OpenAI models (GPT-4)
            <Function test_count_tokens_gemini_model>
              Test token counting for Gemini models.

              RED: Will be more accurate after switching to LiteLLM.
              Currently uses tiktoken which approximates Gemini tokens.
            <Function test_count_tokens_claude_model>
              Test token counting for Claude models.

              RED: Will be more accurate after switching to LiteLLM.
            <Function test_count_tokens_unknown_model_fallback>
              Test fallback behavior for unknown models
            <Function test_model_specific_token_accuracy>
              Test that different models may have different token counts.

              After LiteLLM integration, models should use their native tokenizers.
      <Package session>
        <Module test_cleanup_scheduler.py>
          Tests for data cleanup scheduler

          Covers scheduled background jobs for data retention policy enforcement.
          <Class TestCleanupSchedulerInit>
            Test CleanupScheduler initialization
            <Function test_initialization_default>
              Test default initialization
            <Function test_initialization_with_params>
              Test initialization with parameters
          <Class TestCleanupSchedulerStart>
            Test scheduler startup
            <Coroutine test_start_creates_retention_service>
              Test that start creates retention service
            <Coroutine test_start_schedules_job>
              Test that start schedules cleanup job
            <Coroutine test_start_with_custom_schedule>
              Test scheduling with custom cron expression
            <Coroutine test_start_with_invalid_schedule>
              Test handling of invalid cron expression
          <Class TestCleanupSchedulerStop>
            Test scheduler shutdown
            <Coroutine test_stop_shuts_down_scheduler>
              Test that stop shuts down scheduler
          <Class TestCleanupExecution>
            Test cleanup execution
            <Coroutine test_run_cleanup_success>
              Test successful cleanup execution
            <Coroutine test_run_cleanup_with_errors>
              Test cleanup execution with errors
            <Coroutine test_run_cleanup_without_service>
              Test cleanup execution when service not initialized
            <Coroutine test_run_cleanup_exception_handling>
              Test that exceptions in cleanup are caught
          <Class TestNotifications>
            Test cleanup notifications
            <Coroutine test_send_cleanup_notification>
              Test sending cleanup notification
            <Coroutine test_notifications_disabled>
              Test that notifications can be disabled
          <Class TestManualExecution>
            Test manual cleanup triggering
            <Coroutine test_run_now>
              Test manual cleanup trigger
          <Class TestHelperFunctions>
            Test helper functions
            <Coroutine test_get_next_run_time>
              Test getting next scheduled run time
            <Function test_get_next_run_time_not_scheduled>
              Test getting next run time when not scheduled
          <Class TestGlobalScheduler>
            Test global scheduler functions
            <Coroutine test_start_cleanup_scheduler>
              Test starting global cleanup scheduler
            <Coroutine test_start_cleanup_scheduler_already_running>
              Test starting scheduler when already running
            <Coroutine test_stop_cleanup_scheduler>
              Test stopping global cleanup scheduler
            <Coroutine test_stop_cleanup_scheduler_not_running>
              Test stopping scheduler when not running
            <Function test_get_cleanup_scheduler_none>
              Test getting scheduler when none exists
          <Class TestDryRunMode>
            Test dry-run mode
            <Coroutine test_dry_run_mode>
              Test that dry-run mode is passed through
        <Module test_session.py>
          Comprehensive tests for Session Management

          Tests cover:
          - InMemorySessionStore implementation
          - RedisSessionStore implementation
          - Session lifecycle operations
          - Expiration and TTL handling
          - Sliding window behavior
          - Concurrent session limits
          - Error handling
          - Factory function
          <Class TestInMemorySessionStore>
            Tests for InMemorySessionStore
            <Coroutine test_create_session>
              Test creating a session
            <Coroutine test_get_session>
              Test retrieving a session
            <Coroutine test_get_nonexistent_session>
              Test retrieving a non-existent session
            <Coroutine test_update_session>
              Test updating session metadata
            <Coroutine test_update_nonexistent_session>
              Test updating a non-existent session
            <Coroutine test_refresh_session>
              Test refreshing session expiration
            <Coroutine test_refresh_with_custom_ttl>
              Test refreshing with custom TTL
            <Coroutine test_delete_session>
              Test deleting a session
            <Coroutine test_delete_nonexistent_session>
              Test deleting a non-existent session
            <Coroutine test_session_expiration>
              Test that expired sessions are not returned
            <Coroutine test_list_user_sessions>
              Test listing all sessions for a user
            <Coroutine test_list_user_sessions_empty>
              Test listing sessions for user with no sessions
            <Coroutine test_delete_user_sessions>
              Test deleting all sessions for a user
            <Coroutine test_concurrent_session_limit>
              Test concurrent session limit enforcement
            <Coroutine test_sliding_window_enabled>
              Test sliding window expiration (refreshes on access)
            <Coroutine test_sliding_window_disabled>
              Test that sliding window can be disabled
            <Coroutine test_custom_ttl_per_session>
              Test creating sessions with custom TTL
          <Class TestRedisSessionStore>
            Tests for RedisSessionStore
            <Coroutine test_create_session>
              Test creating a session in Redis
            <Coroutine test_get_session>
              Test retrieving a session from Redis
            <Coroutine test_get_nonexistent_session>
              Test retrieving non-existent session
            <Coroutine test_update_session>
              Test updating session metadata
            <Coroutine test_refresh_session>
              Test refreshing session expiration
            <Coroutine test_delete_session>
              Test deleting a session
            <Coroutine test_list_user_sessions>
              Test listing all sessions for a user
            <Coroutine test_delete_user_sessions>
              Test deleting all sessions for a user
            <Coroutine test_concurrent_session_limit>
              Test concurrent session limit with Redis
            <Coroutine test_connection_error_handling>
              Test handling of Redis connection errors
          <Class TestSessionStoreFactory>
            Tests for create_session_store factory function
            <Function test_create_memory_store>
              Test creating in-memory store
            <Function test_create_redis_store>
              Test creating Redis store
            <Function test_create_default_store>
              Test creating store with defaults
            <Function test_create_with_custom_settings>
              Test creating store with custom settings
            <Function test_invalid_backend>
              Test error handling for invalid backend
          <Class TestSessionIntegration>
            Integration tests for session management
            <Coroutine test_session_lifecycle>
              Test complete session lifecycle
            <Coroutine test_multi_user_sessions>
              Test managing sessions for multiple users
          <Class TestSessionEdgeCases>
            P2: Test session management edge cases and validation
            <Coroutine test_session_data_with_zulu_time_normalization>
              Test that SessionData handles Zulu time (Z suffix) correctly
            <Coroutine test_get_inactive_sessions_with_large_session_store>
              Test get_inactive_sessions with many sessions (performance check)
            <Coroutine test_session_data_with_invalid_iso_format_raises_error>
              Test that SessionData validation rejects invalid ISO format timestamps
        <Module test_session_timeout.py>
          Tests for session timeout middleware (HIPAA 164.312(a)(2)(iii))

          Tests automatic logoff after period of inactivity.
          <Class TestSessionTimeoutMiddleware>
            Test SessionTimeoutMiddleware functionality
            <Coroutine test_middleware_initialization>
              Test middleware initialization
            <Coroutine test_middleware_with_default_timeout>
              Test middleware with default 15-minute timeout
            <Coroutine test_public_endpoint_bypasses_timeout>
              Test that public endpoints bypass timeout check
            <Coroutine test_request_without_session_continues>
              Test that requests without session continue normally
            <Coroutine test_active_session_continues>
              Test that active session (within timeout) continues
            <Coroutine test_inactive_session_times_out>
              Test that inactive session (beyond timeout) is terminated
            <Coroutine test_sliding_window_updates_last_accessed>
              Test that session activity updates last_accessed (sliding window)
            <Coroutine test_timeout_response_format>
              Test timeout response has proper format
          <Class TestSessionTimeoutHelpers>
            Test helper methods
            <Function test_get_session_id_from_cookie>
              Test extracting session ID from cookie
            <Function test_get_session_id_from_state>
              Test extracting session ID from request state
            <Function test_get_session_id_none>
              Test when no session ID is found
            <Function test_get_session_id_from_jwt_bearer_token>
              Test extracting session ID from JWT Bearer token (NEW)
            <Function test_get_session_id_from_jwt_sid_claim>
              Test extracting session ID from JWT 'sid' claim (NEW)
            <Function test_get_session_id_from_jwt_jti_claim>
              Test extracting session ID from JWT 'jti' claim (NEW)
            <Function test_get_session_id_from_jwt_invalid_token>
              Test handling invalid JWT token (NEW)
            <Function test_get_session_id_jwt_priority_over_cookie>
              Test JWT session ID takes priority over cookie (NEW)
            <Function test_get_session_id_from_rs256_jwt_token>
              Test extracting session ID from RS256 JWT token (asymmetric algorithm)
            <Function test_is_public_endpoint_health>
              Test /health is recognized as public
            <Function test_is_public_endpoint_metrics>
              Test /metrics is recognized as public
            <Function test_is_public_endpoint_login>
              Test /api/v1/auth/login is recognized as public
            <Function test_is_public_endpoint_docs>
              Test /docs is recognized as public
            <Function test_is_public_endpoint_private>
              Test private endpoints are not public
          <Class TestCreateSessionTimeoutMiddleware>
            Test middleware factory function
            <Function test_create_middleware_with_minutes>
              Test creating middleware with timeout in minutes
            <Function test_create_middleware_default_timeout>
              Test creating middleware with default timeout
          <Class TestSessionTimeoutEdgeCases>
            Test edge cases and error handling
            <Coroutine test_session_not_found_continues>
              Test that missing session (deleted/expired) continues normally
            <Coroutine test_session_store_error_continues>
              Test that session store errors don't break requests
            <Coroutine test_custom_timeout_values>
              Test various timeout values
            <Coroutine test_request_without_client_info>
              Test handling request without client information
          <Class TestHIPAACompliance>
            Test HIPAA compliance aspects
            <Coroutine test_logs_timeout_events>
              Test that timeout events are logged (audit requirement)
            <Function test_default_timeout_meets_hipaa>
              Test that default timeout meets HIPAA recommendation (15 minutes)
      <Package storage>
        <Module test_conversation_store_async.py>
          Tests for async conversation store - OpenAI Codex Finding #2

          FINDING #2 (MEDIUM): Conversation store blocking event loop.

          The conversation store uses synchronous Redis client (redis.from_url) in async methods:
          - record_conversation (async) calls self._redis_client.setex() (sync) - line 130
          - get_conversation (async) calls self._redis_client.get() (sync) - line 146
          - list_user_conversations (async) calls self._redis_client.scan_iter() (sync) - line 170

          This blocks the async event loop under Redis backend usage.

          ## MITIGATION STATUS: Acceptable Risk (Documented Limitation)

          **Context:**
          - This is a FALLBACK feature for development when OpenFGA isn't available (line 4-5 of module)
          - Default backend is "memory" (non-blocking, used in all tests)
          - Production deployments should use OpenFGA for conversation management
          - Redis backend is optional and only for development/staging environments

          **Risk Assessment:**
          - PRODUCTION: Not applicable (uses OpenFGA)
          - DEVELOPMENT: Low risk (local Redis, low concurrency)
          - STAGING: Medium risk (could block under load, but not customer-facing)

          **Future Enhancement:**
          - Migrate to redis.asyncio when/if Redis backend becomes production-critical
          - For now, documented as known limitation for Redis backend
          - Memory backend (default) is non-blocking and works correctly

          These tests validate the ASYNC INTERFACE works correctly with memory backend.
          Full async Redis migration deferred to future PR (lower priority fallback feature).
          <Function test_conversation_store_uses_async_redis_client>
            🟢 GREEN: Test that ConversationStore uses async Redis client.

            PROBLEM: Uses sync redis.from_url() which blocks event loop in async methods.

            This test will PASS after migrating to redis.asyncio.
          <Coroutine test_record_conversation_does_not_block_event_loop>
            🟢 GREEN: Test that record_conversation doesn't block the event loop.

            PROBLEM: Calls sync self._redis_client.setex() in async method.

            This test will PASS after migrating to await self._redis_client.setex().
          <Coroutine test_get_conversation_does_not_block_event_loop>
            🟢 GREEN: Test that get_conversation doesn't block the event loop.

            PROBLEM: Calls sync self._redis_client.get() in async method.

            This test will PASS after migrating to await self._redis_client.get().
          <Coroutine test_list_user_conversations_does_not_block_event_loop>
            🟢 GREEN: Test that list_user_conversations doesn't block the event loop.

            PROBLEM: Calls sync self._redis_client.scan_iter() in async method.

            This test will PASS after migrating to async iteration over Redis.
          <Function test_redis_backend_requires_async_operations>
            Test that Redis backend uses async operations, not sync.

            This is more of a documentation test - it describes what SHOULD be true.
        <Module test_storage.py>
          Tests for Storage Backend Interfaces for Compliance Data

          Covers abstract interfaces and data models for compliance data storage.
          <Class TestUserProfile>
            Test UserProfile data model
            <Function test_user_profile_creation>
              Test creating user profile
            <Function test_user_profile_minimal>
              Test user profile with minimal required fields
            <Function test_user_profile_validation>
              Test user profile validation
          <Class TestConversation>
            Test Conversation data model
            <Function test_conversation_creation>
              Test creating conversation
            <Function test_conversation_defaults>
              Test conversation default values
            <Function test_conversation_archived>
              Test archived conversation
          <Class TestUserPreferences>
            Test UserPreferences data model
            <Function test_user_preferences_creation>
              Test creating user preferences
            <Function test_user_preferences_defaults>
              Test user preferences with defaults
          <Class TestAuditLogEntry>
            Test AuditLogEntry data model
            <Function test_audit_log_creation>
              Test creating audit log
            <Function test_audit_log_minimal>
              Test audit log with minimal fields
          <Class TestConsentRecord>
            Test ConsentRecord data model
            <Function test_consent_record_creation>
              Test creating consent record
            <Function test_consent_record_revoked>
              Test revoked consent
          <Class TestInMemoryUserProfileStore>
            Test in-memory user profile storage
            <Coroutine test_create_and_retrieve_user_profile>
              Test creating and retrieving user profile
            <Coroutine test_get_nonexistent_user_profile>
              Test retrieving non-existent user profile
            <Coroutine test_update_user_profile>
              Test updating user profile
            <Coroutine test_delete_user_profile>
              Test deleting user profile
            <Coroutine test_delete_nonexistent_user_profile>
              Test deleting non-existent user profile
            <Coroutine test_create_duplicate_profile>
              Test that creating duplicate profile fails
          <Class TestInMemoryConversationStore>
            Test in-memory conversation storage
            <Coroutine test_create_and_retrieve_conversation>
              Test creating and retrieving conversation
            <Coroutine test_list_user_conversations>
              Test listing all conversations for a user
            <Coroutine test_list_archived_conversations>
              Test listing archived conversations
            <Coroutine test_delete_conversation>
              Test deleting conversation
            <Coroutine test_delete_user_conversations>
              Test deleting all conversations for a user
          <Class TestInMemoryPreferencesStore>
            Test in-memory preferences storage
            <Coroutine test_set_and_retrieve_preferences>
              Test setting and retrieving user preferences
            <Coroutine test_update_preferences>
              Test updating existing preferences
            <Coroutine test_update_nonexistent_preferences>
              Test updating preferences for user without existing preferences
            <Coroutine test_delete_preferences>
              Test deleting user preferences
          <Class TestInMemoryAuditLogStore>
            Test in-memory audit log storage
            <Coroutine test_log_and_retrieve_audit_entry>
              Test logging and retrieving audit entry
            <Coroutine test_list_user_logs>
              Test listing audit logs for a user
            <Coroutine test_list_user_logs_with_date_filter>
              Test filtering logs by date range
            <Coroutine test_anonymize_user_logs>
              Test anonymizing user logs (GDPR compliance)
          <Class TestInMemoryConsentStore>
            Test in-memory consent storage
            <Coroutine test_create_and_retrieve_consent>
              Test creating and retrieving consent record
            <Coroutine test_get_latest_consent>
              Test getting latest consent for a type
            <Coroutine test_delete_user_consents>
              Test deleting all consents for a user
      <Module test_app_factory.py>
        Tests for FastAPI app factory pattern with settings override.

        Verifies that create_app() accepts settings_override parameter,
        allowing tests to customize configuration without affecting global state.
        <Class TestAppFactoryPattern>
          Test app factory pattern for test configurability
          <Function test_create_app_with_default_settings>
            Test that create_app() works with default settings.

            This is the current behavior - should continue to work.
            Uses skip_startup_validation=True to avoid DB dependency in unit tests.
          <Function test_create_app_with_settings_override>
            Test that create_app() accepts settings_override parameter.

            This is the new feature - tests can provide custom settings.
          <Function test_multiple_app_instances_with_different_settings>
            Test that multiple app instances can be created with different settings.

            This ensures no global state pollution between instances.
          <Function test_create_app_without_override_uses_global_settings>
            Test that create_app() without override uses global settings.

            Backward compatibility - existing usage should work unchanged.
        <Class TestAppFactoryBackwardCompatibility>
          Test backward compatibility with existing deployment patterns
          <Function test_module_level_app_variable_exists>
            Test that module-level 'app' variable exists for uvicorn.

            Deployment scripts use: uvicorn mcp_server_langgraph.app:app
            This must continue to work.
        <Class TestAppFactoryRouterMounting>
          P1: Test router mounting order and registration
          <Function test_health_router_mounted>
            Test that health router is mounted and accessible
          <Function test_uvicorn_can_import_app>
            Test that uvicorn can import the app variable.

            Simulates: uvicorn mcp_server_langgraph.app:app
      <Module test_cache_redis_config.py>
        Unit tests for Redis cache configuration

        Tests CRITICAL to prevent: ConnectionError, TimeoutError in production

        Following TDD best practices (RED-GREEN-REFACTOR):
        1. RED: These tests define expected behavior
        2. GREEN: Implementation must make these pass
        3. REFACTOR: Improve quality while keeping tests green

        OpenAI Codex Finding (2025-11-17):
        ====================================
        This test file was missing, causing test_all_critical_tests_exist to fail.
        Created to ensure Redis cache configuration is validated before production deployments.
        <Class TestRedisURLParsing>
          Validate Redis URL parsing and configuration
          <Function test_redis_checkpoint_url_defaults>
            Redis checkpoint URL should have sensible defaults
          <Function test_redis_session_url_defaults>
            Redis session URL should have sensible defaults
          <Function test_redis_urls_use_different_databases>
            Checkpoint and session Redis should use different databases for isolation
          <Function test_custom_redis_urls_accepted>
            Settings should accept custom Redis URLs
        <Class TestRedisConnectionPool>
          Validate Redis connection pool configuration
          <Coroutine test_redis_connection_pool_has_limits>
            Redis connection pools should have max_connections limit
          <Coroutine test_redis_connection_timeout_configured>
            Redis connections should have timeout configuration
        <Class TestRedisFailover>
          Validate graceful degradation when Redis is unavailable
          <Coroutine test_application_continues_without_redis>
            Application should continue working when Redis is unavailable
          <Function test_checkpointing_can_be_disabled>
            Checkpointing should be disable-able for environments without Redis
          <Function test_agent_works_without_checkpointing>
            Agent should work when checkpointing is disabled
        <Class TestRedisDatabaseIsolation>
          Validate Redis database isolation between services
          <Function test_checkpoint_and_session_use_different_dbs>
            Checkpoints and sessions should use different Redis databases
          <Function test_test_mode_uses_isolated_databases>
            Test mode should use isolated Redis databases
        <Class TestRedisResilience>
          Validate Redis retry and resilience patterns
          <Coroutine test_redis_operations_have_retry_logic>
            Redis operations should retry on transient failures
          <Function test_redis_connection_pool_supports_reconnection>
            Redis connection pool should support automatic reconnection
        <Class TestRedisProductionReadiness>
          Validate Redis configuration for production deployments
          <Function test_redis_urls_support_authentication>
            Redis URLs should support password authentication
          <Function test_redis_urls_support_tls>
            Redis URLs should support TLS connections
          <Function test_redis_sentinel_urls_supported>
            Redis Sentinel URLs should be supported for high availability
        <Class TestRedisIntegration>
          Integration tests for Redis with application components
          <Coroutine test_checkpointer_uses_redis_when_enabled>
            Agent checkpointer should use Redis when enabled
      <Module test_calculator_tools.py>
        Unit tests for calculator tools

        Tests arithmetic operations and expression evaluation.
        <Class TestCalculatorTool>
          Test suite for calculator tool
          <Function test_calculator_simple_addition>
            Test simple addition expression
          <Function test_calculator_complex_expression>
            Test complex mathematical expression
          <Function test_calculator_division>
            Test division expression
          <Function test_calculator_power>
            Test exponentiation
          <Function test_calculator_modulo>
            Test modulo operation
          <Function test_calculator_negative_numbers>
            Test negative number handling
          <Function test_calculator_parentheses>
            Test parentheses precedence
          <Function test_calculator_invalid_expression>
            Test error handling for invalid expression
          <Function test_calculator_unsafe_operation>
            Test security - should reject unsafe operations
          <Function test_calculator_division_by_zero>
            Test division by zero handling
        <Class TestAddTool>
          Test suite for add tool
          <Function test_add_positive_numbers>
            Test adding two positive numbers
          <Function test_add_negative_numbers>
            Test adding negative numbers
          <Function test_add_mixed_signs>
            Test adding numbers with different signs
          <Function test_add_zero>
            Test adding zero
          <Function test_add_floats>
            Test adding floating point numbers
        <Class TestSubtractTool>
          Test suite for subtract tool
          <Function test_subtract_positive_numbers>
            Test subtracting positive numbers
          <Function test_subtract_negative_result>
            Test subtraction resulting in negative
          <Function test_subtract_zero>
            Test subtracting zero
        <Class TestMultiplyTool>
          Test suite for multiply tool
          <Function test_multiply_positive_numbers>
            Test multiplying positive numbers
          <Function test_multiply_by_zero>
            Test multiplying by zero
          <Function test_multiply_negative_numbers>
            Test multiplying negative numbers
          <Function test_multiply_mixed_signs>
            Test multiplying numbers with different signs
        <Class TestDivideTool>
          Test suite for divide tool
          <Function test_divide_positive_numbers>
            Test dividing positive numbers
          <Function test_divide_with_remainder>
            Test division with remainder
          <Function test_divide_by_zero>
            Test division by zero error handling
          <Function test_divide_zero_by_number>
            Test dividing zero
          <Function test_divide_negative_numbers>
            Test dividing negative numbers
        <Class TestToolSchemas>
          Test tool schema generation
          <Function test_calculator_has_schema>
            Test that calculator tool has proper schema
          <Function test_add_has_schema>
            Test that add tool has proper schema
          <Function test_all_tools_have_names>
            Test that all tools have valid names
      <Module test_checkpoint_config_validation.py>
        Tests for checkpoint configuration startup validation.

        This test module ensures that Redis URL configuration errors are detected at
        startup with clear error messages, preventing the production incident scenario
        where pods crashed during runtime due to malformed URLs.

        Implements "fail fast" principle: configuration errors should be caught during
        application startup, not during first Redis connection attempt.
        <Class TestCheckpointConfigValidator>
          Test startup validation for checkpoint configuration.
          <Function test_validate_redis_url_with_unencoded_password_raises_error>
            Test that unencoded special characters in Redis URL raise validation error.

            This prevents the production incident where unencoded / in password
            caused ValueError during runtime.
          <Function test_validate_redis_url_with_encoded_password_passes>
            Test that properly encoded Redis URL passes validation.
          <Function test_validate_redis_url_without_password_passes>
            Test that Redis URL without password passes validation.
          <Function test_validate_redis_url_with_simple_password_passes>
            Test that Redis URL with simple password (no special chars) passes.
          <Function test_validate_redis_url_detects_multiple_special_chars>
            Test that validator detects all problematic special characters.
          <Function test_validate_invalid_redis_url_format_raises_error>
            Test that completely invalid URL formats are caught.
          <Function test_validator_provides_fix_suggestion>
            Test that validation error provides actionable fix suggestion.
        <Class TestCheckpointValidationIntegration>
          Integration tests for checkpoint validation with settings.
          <Function test_validate_checkpoint_config_with_memory_backend_passes>
            Test that memory backend doesn't require Redis URL validation.
          <Function test_validate_checkpoint_config_with_redis_backend_validates_url>
            Test that Redis backend triggers URL validation.
          <Function test_validate_checkpoint_config_with_valid_redis_url_passes>
            Test that valid Redis configuration passes all checks.
        <Class TestStartupValidationErrorMessages>
          Test that validation errors have helpful, actionable messages.
          <Function test_error_message_includes_problematic_url>
            Test that error message includes the problematic URL for debugging.
          <Function test_error_message_references_production_incident>
            Test that error message references the production incident for context.
          <Function test_error_message_provides_encoded_example>
            Test that error shows correct encoded format.
        <Class TestCheckpointValidatorWithMocking>
          Test validator behavior with Redis connection mocking.
          <Function test_validator_can_optionally_test_redis_connectivity>
            Test that validator can optionally test actual Redis connection.
          <Function test_validator_fails_fast_on_connection_error_if_requested>
            Test fail-fast behavior when connection test is enabled.
        <Class TestRegressionPrevention>
          Regression tests to ensure validation prevents known issues.
          <Function test_production_incident_url_is_caught_at_startup>
            Test that exact production incident URL would be caught at startup.

            This is the ultimate regression test: if we had this validation in
            production, the incident would never have occurred.
          <Function test_all_base64_like_passwords_trigger_validation>
            Test that typical Base64 password patterns are validated.

            Base64 commonly contains +, /, = which need encoding.
      <Module test_conversation_retrieval.py>
        Test conversation retrieval from checkpointer.

        Ensures that _handle_get_conversation properly retrieves conversation
        history from the LangGraph checkpointer.
        <Coroutine test_conversation_retrieval_success>
          Test successful conversation retrieval.
        <Coroutine test_conversation_retrieval_no_checkpointer>
          Test conversation retrieval when checkpointing is disabled.
        <Coroutine test_conversation_retrieval_not_found>
          Test conversation retrieval when thread doesn't exist.
        <Coroutine test_conversation_retrieval_empty_messages>
          Test conversation retrieval when thread exists but has no messages.
        <Coroutine test_conversation_retrieval_authorization_failure>
          Test conversation retrieval with authorization failure.
        <Coroutine test_conversation_retrieval_error_handling>
          Test conversation retrieval error handling.
      <Module test_conversation_store.py>
        Unit tests for conversation metadata store

        Tests in-memory backend for conversation tracking.
        <Class TestConversationStore>
          Test suite for ConversationStore
          <Coroutine test_record_conversation>
            Test recording conversation metadata
          <Coroutine test_update_conversation>
            Test updating existing conversation
          <Coroutine test_get_nonexistent_conversation>
            Test getting nonexistent conversation returns None
          <Coroutine test_list_user_conversations>
            Test listing all conversations for a user
          <Coroutine test_list_conversations_sorted_by_activity>
            Test conversations are sorted by last activity
          <Coroutine test_search_conversations_by_query>
            Test searching conversations by query
          <Coroutine test_search_conversations_no_query>
            Test search with no query returns most recent
          <Coroutine test_search_conversations_limit>
            Test search respects limit
          <Coroutine test_delete_conversation>
            Test deleting conversation
          <Coroutine test_delete_nonexistent_conversation>
            Test deleting nonexistent conversation
          <Coroutine test_get_stats>
            Test getting store statistics
          <Coroutine test_search_by_tags>
            Test searching conversations by tags
        <Class TestConversationMetadata>
          Test ConversationMetadata dataclass
          <Function test_metadata_creation>
            Test creating metadata
          <Function test_metadata_with_tags>
            Test metadata with tags
      <Module test_dependencies_wiring.py>
        Unit tests for dependency injection wiring

        Tests CRITICAL to prevent: ImportError, CircularImportError in production

        Following TDD best practices (RED-GREEN-REFACTOR):
        1. RED: These tests define expected behavior
        2. GREEN: Implementation must make these pass
        3. REFACTOR: Improve quality while keeping tests green

        OpenAI Codex Finding (2025-11-17):
        ====================================
        This test file was missing, causing test_all_critical_tests_exist to fail.
        Created to ensure dependency wiring is validated before production deployments.
        <Class TestModuleImports>
          Validate all critical modules can be imported without errors
          <Function test_all_core_modules_importable>
            All core modules should import without errors
          <Function test_no_circular_imports_in_core>
            Core modules should not have circular import dependencies
        <Class TestSettingsInjection>
          Validate settings can be injected into all components
          <Function test_settings_can_be_created_with_minimal_config>
            Settings should work with minimal required configuration
          <Function test_agent_accepts_settings_injection>
            Agent graph creation should accept settings parameter
          <Function test_container_can_be_created_with_settings>
            ApplicationContainer should accept custom settings
        <Class TestInitializationOrder>
          Validate components initialize in correct order
          <Function test_observability_can_initialize_before_agent>
            Observability must be initialized before agent creation
          <Function test_agent_works_after_observability_init>
            Agent should work correctly when observability is initialized first
        <Class TestProductionReadiness>
          Validate production deployment readiness
          <Function test_all_required_dependencies_present>
            All required dependencies should be importable
          <Function test_settings_validation_prevents_invalid_config>
            Settings should validate field types
        <Class TestIntegrationPoints>
          Validate integration between major components
          <Function test_fastapi_app_can_be_created>
            FastAPI app should be creatable without errors
          <Function test_mcp_server_can_be_retrieved>
            MCP server singleton should be retrievable
      <Module test_filesystem_tools.py>
        Unit tests for filesystem tools

        Tests read-only file operations with security controls.
        <Class TestReadFile>
          Test suite for read_file tool
          <Function test_read_existing_file>
            Test reading an existing text file
          <Function test_read_file_with_size_limit>
            Test reading file with size limit
          <Function test_read_nonexistent_file>
            Test error handling for nonexistent file
          <Function test_read_directory_instead_of_file>
            Test error when trying to read a directory
          <Function test_read_unsafe_path>
            Test security - block access to system directories
          <Function test_read_supported_file_types>
            Test reading different supported file types
          <Function test_read_unsupported_file_type>
            Test rejection of unsupported file types
        <Class TestListDirectory>
          Test suite for list_directory tool
          <Function test_list_directory_contents>
            Test listing directory contents
          <Function test_list_directory_hidden_files>
            Test listing with hidden files
          <Function test_list_empty_directory>
            Test listing empty directory
          <Function test_list_nonexistent_directory>
            Test error handling for nonexistent directory
          <Function test_list_file_instead_of_directory>
            Test error when trying to list a file
          <Function test_list_unsafe_directory>
            Test security - block access to system directories
        <Class TestSearchFiles>
          Test suite for search_files tool
          <Function test_search_files_by_pattern>
            Test searching files by pattern
          <Function test_search_files_recursive>
            Test recursive file search
          <Function test_search_files_max_results>
            Test max results limit
          <Function test_search_files_no_matches>
            Test search with no matches
          <Function test_search_files_specific_name>
            Test searching for specific filename
        <Class TestFilesystemToolSchemas>
          Test filesystem tool schemas
          <Function test_read_file_schema>
            Test read_file has proper schema
          <Function test_list_directory_schema>
            Test list_directory has proper schema
          <Function test_search_files_schema>
            Test search_files has proper schema
      <Module test_gke_autopilot_validator.py>
        Unit tests for GKE Autopilot compliance validator.

        TDD Context:
        - RED (2025-11-12): 7 CI/CD workflows failing due to GKE Autopilot violations
        - GREEN: Created validator and fixed all violations
        - REFACTOR: These tests prevent regression of validation logic

        Following TDD: Tests ensure validator correctly identifies non-compliant resources.
        <Class TestCPUParsing>
          Test CPU string parsing to millicores
          <Function test_parse_cpu_millicores>
            Test parsing CPU values in millicores format
          <Function test_parse_cpu_cores>
            Test parsing CPU values in cores format
          <Function test_parse_cpu_empty>
            Test parsing empty/None CPU values
        <Class TestMemoryParsing>
          Test memory string parsing to MiB
          <Function test_parse_memory_mebibytes>
            Test parsing memory in MiB format
          <Function test_parse_memory_gibibytes>
            Test parsing memory in GiB format
          <Function test_parse_memory_kibibytes>
            Test parsing memory in KiB format
          <Function test_parse_memory_empty>
            Test parsing empty/None memory values
        <Class TestCPURatioValidation>
          Test CPU limit/request ratio validation
          <Function test_cpu_ratio_compliant>
            Test that compliant CPU ratios pass validation
          <Function test_cpu_ratio_violation_5x>
            Test that 5.0x CPU ratio is flagged as violation
          <Function test_cpu_ratio_violation_10x>
            Test that 10.0x CPU ratio is flagged as violation
          <Function test_cpu_ratio_violation_8x>
            Test that 8.0x CPU ratio is flagged as violation
          <Function test_cpu_ratio_fixed_values>
            Test that fixed CPU ratios (4.0x) pass validation
        <Class TestGKEAutopilotConstants>
          Test that GKE Autopilot constants are correctly defined
          <Function test_max_cpu_ratio>
            Ensure MAX_CPU_RATIO is set to 4.0
          <Function test_max_memory_ratio>
            Ensure MAX_MEMORY_RATIO is set to 4.0
          <Function test_cpu_limits>
            Ensure CPU limit constants are correctly defined
          <Function test_memory_limits>
            Ensure memory limit constants are correctly defined
        <Class TestRegressionPrevention>
          Regression tests to prevent the specific failures we encountered.

          These tests encode the exact violations that occurred in the CI/CD runs.
          <Function test_prevent_otel_collector_regression>
            Prevent regression of otel-collector CPU ratio (was 5.0, now 4.0)
          <Function test_prevent_qdrant_regression>
            Prevent regression of qdrant CPU ratio (was 10.0, now 4.0)
          <Function test_prevent_postgres_regression>
            Prevent regression of postgres CPU ratio (was 8.0, now 4.0)
          <Function test_prevent_redis_session_regression>
            Prevent regression of redis-session CPU ratio (was 5.0, now 4.0)
      <Module test_health_endpoints.py>
        Unit tests for health check endpoints.

        Tests verify that health endpoints follow FastAPI sub-app best practices
        and are accessible at the correct paths when mounted.
        <Function test_health_app_routes_are_root_level>
          Test that health app defines routes at root level (/, /ready, /startup).

          When a FastAPI sub-app is mounted at /health, routes should be defined
          at root level to avoid double-path issues (e.g., /health/health/startup).

          CRITICAL: This prevents production health probe failures.
        <Function test_health_endpoints_respond_correctly>
          Test that health endpoints return expected responses.
        <Function test_mounted_health_app_accessible>
          Test that when health app is mounted at /health on main app,
          endpoints are accessible at /health/* paths.
        <Function test_kubernetes_probe_paths>
          Test that Kubernetes health probe paths work correctly.

          Deployment spec configures probes at:
          - Startup: /health/startup
          - Readiness: /health/ready
          - Liveness: /health/live
      <Module test_import_validation.py>
        Test module import validation and type resolution.

        This test suite validates that all modules can be imported correctly
        and that type annotations are properly resolved. These tests follow TDD
        principles and should FAIL initially, then PASS after fixes are applied.

        RED Phase: These tests will fail due to:
        - Missing ResourceLimits import in sandbox.py
        - Undefined 'tools' variable in server_stdio.py
        - Missing CostMetricsCollector import in budget_monitor.py

        GREEN Phase: Tests should pass after imports are added.
        <Function test_sandbox_module_imports_successfully>
          Test that sandbox module can be imported without NameError.

          FAILS when: ResourceLimits is not imported in sandbox.py
          PASSES when: Import is added at top of sandbox.py
        <Function test_sandbox_init_signature_resolves_resource_limits>
          Test that Sandbox.__init__ type hints are properly resolved.

          FAILS when: ResourceLimits is not imported (NameError during inspection)
          PASSES when: Import is added
        <Coroutine test_server_stdio_list_tools_public_does_not_crash>
          Test that list_tools_public() can be called without NameError.

          FAILS when: 'tools' variable is not initialized in server_stdio.py
          PASSES when: tools list is properly initialized before use
        <Coroutine test_server_stdio_list_tools_includes_required_tools>
          Test that list_tools_public() returns all expected tools.

          FAILS when: 'tools' variable is undefined, preventing tool additions
          PASSES when: tools list is initialized and all tools are added
        <Function test_budget_monitor_module_imports_successfully>
          Test that budget_monitor module can be imported without errors.

          FAILS when: CostMetricsCollector type is not imported
          PASSES when: Import is added (top-level or TYPE_CHECKING)
        <Function test_budget_monitor_init_signature_resolves_cost_collector>
          Test that BudgetMonitor.__init__ type hints are properly resolved.

          FAILS when: CostMetricsCollector is not imported for type checking
          PASSES when: Import is added (top-level or TYPE_CHECKING)
        <Function test_budget_monitor_accepts_none_cost_collector>
          Test that BudgetMonitor can be instantiated without cost_collector.

          FAILS when: Type annotations cause import errors
          PASSES when: Imports are correct and Optional works properly
        <Function test_all_critical_modules_import_without_errors>
          Integration test: Verify all critical modules with import issues can be imported.

          FAILS when: Any of the three import errors exist
          PASSES when: All imports are fixed
        <Function test_sandbox_accepts_resource_limits_instance>
          Test that Sandbox can be instantiated with ResourceLimits.

          FAILS when: ResourceLimits import is missing
          PASSES when: Import is added and class is available
      <Module test_input_validation.py>
        Unit tests for input validation to prevent CWE-20 vulnerabilities.

        This module tests validation of user-controlled identifiers that flow into
        critical systems like OpenFGA, Redis, and application logs.
        <Class TestThreadIdValidation>
          Test suite for thread_id validation to prevent CWE-20
          <Function test_thread_id_accepts_valid_alphanumeric>
            Test that valid alphanumeric thread_id is accepted
          <Function test_thread_id_accepts_underscores>
            Test that underscores are allowed in thread_id
          <Function test_thread_id_rejects_newline_characters>
            Test that newline characters are rejected
          <Function test_thread_id_rejects_carriage_return>
            Test that carriage return characters are rejected
          <Function test_thread_id_rejects_null_bytes>
            Test that null bytes are rejected
          <Function test_thread_id_rejects_colon_for_openfga_safety>
            Test that colon is rejected to prevent OpenFGA resource confusion
          <Function test_thread_id_rejects_path_traversal>
            Test that path traversal sequences are rejected
          <Function test_thread_id_rejects_excessive_length>
            Test that excessively long thread_id is rejected (DoS prevention)
          <Function test_thread_id_accepts_none>
            Test that None thread_id is accepted (uses default)
          <Function test_thread_id_rejects_empty_string>
            Test that empty string is rejected (forces None or valid ID)
          <Function test_thread_id_rejects_spaces>
            Test that spaces are rejected
          <Function test_thread_id_with_search_conversations_input>
            Test that thread_id validation applies to SearchConversationsInput
        <Class TestConversationIdUsageInCode>
          Integration tests verifying thread_id doesn't corrupt downstream systems
          <Function test_thread_id_in_openfga_resource>
            Test that valid thread_id creates safe OpenFGA resource string
          <Function test_malicious_thread_id_would_corrupt_openfga>
            Test that validation prevents OpenFGA resource corruption
          <Function test_thread_id_in_span_attribute>
            Test that valid thread_id is safe for OpenTelemetry span attributes
      <Module test_lazy_imports.py>
        Tests for lazy import functionality in __init__.py.

        Verifies that heavy dependencies are only loaded when accessed,
        preventing ModuleNotFoundError for minimal installations.
        <Class TestLazyImports>
          Test lazy import behavior for optional dependencies
          <Function test_package_import_does_not_load_heavy_modules>
            Test that importing the package doesn't trigger heavy module imports.

            Heavy modules:
            - auth.middleware (FastAPI, OpenFGA)
            - auth.openfga (openfga_sdk)
            - core.agent (LangGraph, LangChain)
            - llm.factory (langchain_core, sentence_transformers)
          <Function test_settings_import_is_lightweight>
            Test that importing settings doesn't load heavy dependencies
          <Function test_lazy_import_of_auth_middleware>
            Test that AuthMiddleware is imported only when accessed
          <Function test_lazy_import_of_openfga_client>
            Test that OpenFGAClient is imported only when accessed
          <Function test_lazy_import_of_agent_graph>
            Test that agent_graph is imported only when accessed
          <Function test_lazy_import_of_agent_state>
            Test that AgentState is imported only when accessed
          <Function test_lazy_import_of_llm_factory>
            Test that create_llm_from_config is imported only when accessed
          <Function test_lazy_import_of_observability>
            Test that observability modules are imported only when accessed
          <Function test_nonexistent_attribute_raises_attribute_error>
            Test that accessing nonexistent attributes raises AttributeError
          <Function test_all_exports_defined_in_all_list>
            Test that all exported names are in __all__
          <Function test_version_accessible_without_heavy_imports>
            Test that __version__ can be accessed without loading heavy modules
        <Class TestBackwardsCompatibility>
          Test that existing import patterns still work
          <Function test_star_import_lists_all_exports>
            Test that 'from mcp_server_langgraph import *' works
          <Function test_direct_attribute_access>
            Test that package.attribute access works
      <Module test_llm_fallback_kwargs.py>
        Test LiteLLM fallback kwargs forwarding.

        Ensures that provider-specific kwargs (api_base, aws_secret_access_key, etc.)
        are properly forwarded to fallback models.
        <Function test_fallback_forwards_kwargs_sync>
          Test that sync fallback forwards provider-specific kwargs to same-provider fallback.
        <Coroutine test_fallback_forwards_kwargs_async>
          Test that async fallback forwards provider-specific kwargs to same-provider fallback.
        <Function test_fallback_forwards_ollama_kwargs_sync>
          Test that sync fallback forwards Ollama-specific kwargs.
        <Coroutine test_fallback_forwards_ollama_kwargs_async>
          Test that async fallback forwards Ollama-specific kwargs.
      <Module test_mcp_server_behavioral.py>
        Sample Test: Behavioral Mock Pattern for MCP Server

        This test demonstrates the behavioral mock approach that eliminates brittle @patch decorators.

        TDD Phase: RED - Test written first, implementation follows
        Purpose: Prove that behavioral mocks enable testing BEHAVIOR, not implementation

        Benefits over @patch decorators:
        - ✅ Tests survive refactoring (no coupling to import paths)
        - ✅ Self-documenting (behavioral intent is clear)
        - ✅ Composable (mocks can be combined easily)
        - ✅ Fewer lines (no @patch decorator boilerplate)

        This sample will be used to refactor tests/unit/test_mcp_stdio_server.py (50 @patch → <10)
        <Class TestMCPServerBehavioralPattern>
          Sample test class demonstrating behavioral mock pattern.
          <Coroutine test_handle_chat_authorized_user_gets_response>
            Test: Authorized user receives agent response when sending chat message.

            This test validates BEHAVIOR:
            - GIVEN: User is authenticated and authorized
            - WHEN: User sends a chat message
            - THEN: Agent processes message and returns response

            This test does NOT validate IMPLEMENTATION:
            - Which auth library is used (JWT, OAuth, etc.)
            - Which LLM provider is called (OpenAI, Anthropic, etc.)
            - Internal module import paths
            - Specific function call sequences

            Contrast with old approach (50 @patch decorators):
                @patch("mcp_server_langgraph.mcp.server_stdio.settings")
                @patch("mcp_server_langgraph.mcp.server_stdio.create_auth_middleware")
                @patch("mcp_server_langgraph.mcp.server_stdio.get_agent_graph")
                @patch("mcp_server_langgraph.mcp.server_stdio.tracer")
                @patch("mcp_server_langgraph.mcp.server_stdio.format_response")
                async def test_handle_chat(...):
                    # 5 patches! Very brittle!

            New behavioral approach (0 @patch decorators):
                # Just create mocks with desired behavior
                auth = create_behavioral_auth_middleware(authorized=True)
                agent = create_behavioral_agent_graph(response="Hi!")
          <Coroutine test_handle_chat_unauthorized_user_denied>
            Test: Unauthorized user is denied access.

            BEHAVIOR:
            - GIVEN: User has invalid token
            - WHEN: User attempts to send chat message
            - THEN: Access is denied

            Old approach: 5 @patch decorators + complex mock setup
            New approach: One behavioral mock with authorization=False
          <Coroutine test_handle_chat_new_conversation>
            Test: New conversation starts with no previous messages.

            BEHAVIOR:
            - GIVEN: User has no existing conversation
            - WHEN: User sends first message
            - THEN: Agent creates new conversation and responds

            Old approach: Complex aget_state mock setup with nested MagicMock
            New approach: conversation_exists=False parameter
          <Coroutine test_handle_chat_existing_conversation>
            Test: Existing conversation resumes with message history.

            BEHAVIOR:
            - GIVEN: User has existing conversation with previous messages
            - WHEN: User sends new message
            - THEN: Agent uses conversation context to respond

            Old approach: Complex mock chaining to simulate previous messages
            New approach: conversation_exists=True parameter
          <Coroutine test_handle_chat_agent_error_handling>
            Test: Agent errors are propagated correctly.

            BEHAVIOR:
            - GIVEN: LLM provider encounters error
            - WHEN: Agent attempts to invoke
            - THEN: Error is propagated to caller

            Old approach: Complex side_effect mock setup
            New approach: invoke_error parameter
      <Module test_mcp_server_settings_injection.py>
        Unit tests for MCPAgentServer settings injection.

        Tests verify that settings can be injected into MCPAgentServer at construction time,
        allowing runtime configuration of features like code execution without module reloading.
        <Class TestMCPServerSettingsInjection>
          Test MCPAgentServer settings injection functionality
          <Coroutine test_server_accepts_settings_parameter>
            Test that MCPAgentServer constructor accepts optional settings parameter.

            This enables dependency injection for testing and runtime configuration.
          <Coroutine test_server_uses_global_settings_by_default>
            Test backward compatibility: server uses global settings when none injected.

            Ensures existing code continues to work without modification.
          <Coroutine test_code_execution_tools_appear_when_enabled>
            Test that execute_python tool appears when code execution is enabled.

            This is the primary use case: enabling code execution via settings injection.
          <Coroutine test_code_execution_tools_hidden_when_disabled>
            Test that execute_python tool is hidden when code execution is disabled.

            Verifies runtime evaluation of settings (not import-time caching).
          <Coroutine test_multiple_servers_with_different_settings>
            Test that multiple server instances with different settings work independently.

            Ensures settings isolation between instances (no global state pollution).
          <Coroutine test_settings_injection_does_not_affect_global_settings>
            Test that injecting settings into a server doesn't modify global settings.

            Ensures no side effects on the global configuration object.
          <Coroutine test_settings_injection_with_other_dependencies>
            Test that settings injection works alongside other dependency injection (auth).

            Ensures compatibility with existing DI pattern.
      <Module test_mcp_stdio_server.py>
        Unit Tests for MCP StdIO Server

        Tests the MCPAgentServer implementation including initialization, authentication,
        authorization, and tool handlers for the stdio-based MCP server.

        Target coverage: 85%+ on src/mcp_server_langgraph/mcp/server_stdio.py
        <Class TestMCPAgentServerInitialization>
          Tests for MCPAgentServer.__init__
          <Function test_init_success_with_openfga>
            Test successful initialization with OpenFGA client
          <Function test_init_fails_without_jwt_secret_inmemory>
            Test initialization fails without JWT secret for in-memory auth provider
          <Function test_init_succeeds_without_jwt_secret_keycloak>
            Test initialization succeeds without JWT secret when using Keycloak (uses RS256)
          <Function test_init_fails_production_without_openfga>
            Test initialization fails in production without OpenFGA (fail-closed)
          <Function test_init_warns_without_openfga_in_dev>
            Test initialization warns but succeeds without OpenFGA in development
        <Class TestListTools>
          Tests for list_tools_public()
          <Coroutine test_list_tools_returns_all_tools>
            Test that list_tools_public returns core tools plus optional tools
        <Class TestToolAuthentication>
          Tests for JWT authentication in call_tool handler
          <Coroutine test_call_tool_missing_token>
            Test tool call fails without authentication token
          <Coroutine test_call_tool_invalid_token>
            Test tool call fails with invalid JWT token
        <Class TestToolAuthorization>
          Tests for OpenFGA authorization in call_tool handler
          <Coroutine test_authorization_check_for_tool>
            Test OpenFGA authorization check for tool execution
          <Coroutine test_authorization_denied>
            Test authorization denial prevents tool execution
        <Class TestHandleChat>
          Tests for _handle_chat() method
          <Coroutine test_handle_chat_new_conversation>
            Test chat handler creates new conversation successfully
          <Coroutine test_handle_chat_existing_conversation_authorized>
            Test chat handler with existing conversation checks editor permission
          <Coroutine test_handle_chat_existing_conversation_unauthorized>
            Test chat handler denies access to unauthorized existing conversation
          <Coroutine test_handle_chat_invalid_input>
            Test chat handler validates input with Pydantic schema
        <Class TestResponseFormatting>
          Tests for response format control (concise vs detailed)
          <Coroutine test_concise_format_applied>
            Test concise response format is applied
        <Class TestErrorHandling>
          Tests for error handling in MCP server
          <Coroutine test_handle_chat_agent_error>
            Test error handling when agent execution fails
      <Module test_observability_cleanup.py>
        Tests for observability cleanup and shutdown functionality.

        Ensures that telemetry exporters are properly flushed and closed on application shutdown.
        <Class TestObservabilityShutdown>
          Test observability shutdown and cleanup
          <Function test_shutdown_flushes_tracer_spans>
            Test that shutdown_observability flushes pending trace spans.

            GREEN: Should call force_flush on tracer provider
          <Function test_shutdown_handles_uninitialized_state>
            Test that shutdown_observability handles being called when not initialized.

            GREEN: Should return gracefully without error
          <Function test_shutdown_idempotent>
            Test that shutdown_observability can be called multiple times safely.

            GREEN: Should be idempotent (safe to call multiple times)
          <Function test_shutdown_sets_config_to_none>
            Test that shutdown_observability clears the global config.

            GREEN: Should set _observability_config to None
          <Function test_providers_stored_for_shutdown>
            Test that tracer_provider and meter_provider are stored on ObservabilityConfig.

            This ensures shutdown_observability can actually flush the providers.
            Regression test for Finding #3 from OpenAI Codex security audit.
          <Function test_shutdown_flushes_providers>
            Test that shutdown actually calls force_flush on stored providers.

            Regression test for Finding #3 - verify providers are flushed.
      <Module test_observability_lazy_init.py>
        Tests for observability lazy initialization and safe logger fallback.

        Verifies that functions can log safely even before init_observability() is called.
        This prevents RuntimeError when calling create_user_provider(), create_session_store(),
        etc. from test fixtures or standalone scripts.
        <Class TestObservabilitySafeFallback>
          Test safe fallback behavior before observability initialization
          <Function test_logger_usable_before_init>
            Test that logger can be used before init_observability().

            This is the core issue: functions like create_user_provider() need to log
            but may be called before observability is initialized (e.g., in test fixtures).
          <Function test_create_user_provider_before_observability_init>
            Test that create_user_provider() works before init_observability().

            This function logs at lines 61, 62, 76 in factory.py.
          <Function test_create_session_store_before_observability_init>
            Test that create_session_store() works before init_observability().

            This function logs at lines 127, 133, 146 in auth/factory.py.
          <Function test_logger_fallback_logs_to_stderr>
            Test that fallback logger actually logs messages (to stderr).

            We want to ensure the fallback isn't a no-op - it should still log.
          <Function test_logger_after_init_uses_configured_logger>
            Test that after init_observability(), logger uses the configured logger.

            This ensures the fallback is only used when needed.
        <Class TestObservabilityIntegration>
          Test observability integration with auth components
          <Function test_multiple_auth_calls_before_init>
            Test multiple auth factory calls before observability init.

            Simulates realistic test setup scenario where multiple components
            are created before the app (and observability) is initialized.
      <Module test_parallel_executor_timeout.py>
        TDD tests for ParallelExecutor timeout functionality.

        These tests verify that the parallel executor can handle timeouts gracefully
        without blocking other tool executions.

        CODEX FINDING #2: Optimized to use short sleeps (0.05-0.3s instead of 5-10s)
        for fast test execution while maintaining timeout behavior validation.
        <Class TestParallelExecutorTimeouts>
          Test suite for parallel executor timeout handling (TDD RED → GREEN)
          <Coroutine test_executor_respects_per_task_timeout>
            Test that parallel executor respects per-task timeout configuration.

            CODEX FINDING #2: Optimized to use short sleeps for fast test execution.
          <Coroutine test_hung_tool_doesnt_block_other_tools>
            Test that a hung tool doesn't block execution of other independent tools.

            CODEX FINDING #2: Optimized to use short sleeps for fast test execution.
          <Coroutine test_timeout_none_means_no_timeout>
            Test that timeout=None disables timeout (backward compatibility).

            CODEX FINDING #2: Optimized to use short sleeps for fast test execution.
          <Coroutine test_timeout_value_in_tool_result>
            Test that timeout errors are properly recorded in ToolResult.

            CODEX FINDING #2: Optimized to use short sleeps for fast test execution.
      <Module test_polling_helper.py>
        Unit tests for polling helper utilities.

        These tests validate the poll_until() helper that enables fast polling-based
        waits instead of fixed sleep() delays, saving ~9.5 seconds in integration tests.

        TDD: These tests are written FIRST (RED phase) before implementing poll_until().
        <Class TestPollUntil>
          Tests for poll_until() polling helper.
          <Function test_poll_until_returns_true_immediately>
            poll_until() should return immediately when condition is True.
          <Function test_poll_until_waits_for_condition>
            poll_until() should poll until condition becomes True.
          <Function test_poll_until_times_out>
            poll_until() should return False on timeout.
          <Function test_poll_until_with_zero_timeout>
            poll_until() should check condition once with timeout=0.
          <Function test_poll_until_with_negative_timeout_raises>
            poll_until() should raise ValueError for negative timeout.
          <Function test_poll_until_with_negative_interval_raises>
            poll_until() should raise ValueError for negative interval.
          <Function test_poll_until_with_zero_interval_raises>
            poll_until() should raise ValueError for zero interval.
          <Function test_poll_until_handles_exceptions>
            poll_until() should treat exceptions as False and continue polling.
          <Function test_poll_until_real_world_example>
            poll_until() example: waiting for job completion.
        <Class TestAsyncPollUntil>
          Tests for async_poll_until() async polling helper.
          <Coroutine test_async_poll_until_returns_true_immediately>
            async_poll_until() should return immediately when condition is True.
          <Coroutine test_async_poll_until_waits_for_condition>
            async_poll_until() should poll until condition becomes True.
          <Coroutine test_async_poll_until_times_out>
            async_poll_until() should return False on timeout.
      <Module test_provider_credentials.py>
        TDD tests for multi-credential provider setup (Azure, Bedrock).

        Tests verify that providers requiring multiple environment variables
        (Azure: API_KEY + BASE + VERSION + DEPLOYMENT, Bedrock: ACCESS_KEY + SECRET + REGION)
        are properly configured.
        <Class TestProviderCredentialSetup>
          Test multi-credential provider configuration (TDD RED → GREEN)
          <Function test_azure_provider_sets_all_required_env_vars>
            Test that Azure provider sets API_KEY, BASE, VERSION, and DEPLOYMENT_NAME.

            GREEN: Should pass with multi-credential provider_config_map
          <Function test_bedrock_provider_sets_all_aws_credentials>
            Test that Bedrock provider sets ACCESS_KEY_ID, SECRET_ACCESS_KEY, and REGION.

            GREEN: Should pass with multi-credential provider_config_map
          <Function test_azure_fallback_provider_configures_all_credentials>
            Test that Azure as a fallback provider gets all required credentials.

            GREEN: Should pass with multi-credential mapping
          <Function test_single_credential_providers_still_work>
            Test backward compatibility: single-credential providers (OpenAI, Anthropic, Google).

            GREEN: Should pass (no regression)
          <Function test_missing_azure_endpoint_does_not_crash>
            Test that missing Azure endpoint doesn't crash, just logs warning.

            GREEN: Should handle None values gracefully
      <Module test_pytest_marker_validator.py>
        Unit tests for pytest marker validator.

        TDD Context:
        - RED (2025-11-12): E2E tests failed - missing 'staging' marker
        - GREEN: Added marker, created validator
        - REFACTOR: These tests ensure validator catches unregistered markers

        Following TDD: Tests ensure validator prevents marker registration issues.
        <Class TestMarkerRegistration>
          Test that marker registration detection works correctly
          <Function test_all_used_markers_are_registered>
            Verify all currently used markers in the codebase are registered.

            This is a regression test that will fail if someone adds an unregistered marker.
          <Function test_critical_markers_are_registered>
            Ensure all critical markers from the original failure are registered
          <Function test_service_specific_markers_registered>
            Ensure service-specific markers are registered
          <Function test_compliance_markers_registered>
            Ensure compliance and regulatory markers are registered
          <Function test_infrastructure_markers_registered>
            Ensure infrastructure markers are registered
        <Class TestRegressionPrevention>
          Regression tests for the specific failure we encountered.

          Ensures the 'staging' marker incident cannot recur.
          <Function test_prevent_staging_marker_regression>
            Prevent regression of missing 'staging' marker.

            Original failure (Run #19310965242):
            ERROR collecting tests/deployment/test_staging_deployment_requirements.py
            'staging' not found in `markers` configuration option
          <Function test_deployment_marker_exists>
            Ensure deployment marker exists (used as alternative to staging)
        <Class TestMarkerValidatorBehavior>
          Test the behavior of the marker validator script
          <Function test_registered_markers_not_empty>
            Validator should find registered markers in pyproject.toml
          <Function test_used_markers_not_empty>
            Validator should find markers used in test files
          <Function test_built_in_markers_excluded>
            Built-in pytest markers should not be counted as 'used'
        <Class TestSpecificMarkerCategories>
          Test that specific categories of markers are properly registered
          <Function test_test_type_markers>
            Test type classification markers
          <Function test_performance_markers>
            Performance-related markers
          <Function test_environment_markers>
            Environment-specific markers
        <Class TestMarkerCount>
          Test that we have the expected number of markers
          <Function test_minimum_marker_count>
            Ensure we have at least the expected number of registered markers
          <Function test_specific_count_regression>
            Regression test: Ensure we have exactly the expected markers.

            After adding 'staging' marker, we should have 39+ markers.
      <Module test_redis_url_encoding.py>
        Tests for Redis URL encoding with special characters in passwords.

        This test module validates proper URL encoding of Redis connection strings,
        specifically addressing the production incident where unencoded special characters
        in passwords caused ValueError during redis.connection.parse_url().

        Related Issue: Staging deployment revision 758b8f744 crash
        Root Cause: Password with / and + characters not percent-encoded in Redis URL
        <Class TestRedisURLEncoding>
          Test URL encoding for Redis connection strings per RFC 3986.
          <Function test_password_with_forward_slash_encoded>
            Test password containing '/' is percent-encoded to %2F.

            This is the primary failure case from production where unencoded /
            caused the URL parser to treat the password fragment as a path component.
          <Function test_password_with_plus_sign_encoded>
            Test password containing '+' is percent-encoded to %2B.
          <Function test_password_with_equals_sign_encoded>
            Test password containing '=' is percent-encoded to %3D.
          <Function test_base64_like_password_with_multiple_special_chars_encoded>
            Test realistic Base64-like password with multiple special characters.

            This simulates the actual failing password from production:
            Du0PmDvmqDWqDTgfGnmi6/SKyuQydi3z7cPTgEQoE+s=

            The unencoded / caused ValueError: Port could not be cast to integer value
          <Function test_password_without_special_chars_unchanged>
            Test password without special chars passes through unchanged.
          <Function test_url_without_password_unchanged>
            Test URL without password passes through unchanged.
          <Function test_url_with_username_and_password_encoded>
            Test URL with both username and password encodes only password.
          <Function test_empty_password_unchanged>
            Test URL with empty password (just colon) unchanged.
          <Function test_at_sign_in_password_encoded>
            Test password containing @ is percent-encoded to %40.

            The @ symbol is used as delimiter in URL, so it must be encoded
            if it appears in the password.
          <Function test_multiple_database_numbers_preserved>
            Test that database number in path is preserved.
          <Function test_custom_port_preserved>
            Test that custom Redis port is preserved.
          <Function test_idempotent_encoding>
            Test that encoding same URL twice produces same result.

            This ensures already-encoded passwords are not double-encoded.
          <Function test_all_reserved_characters_encoded>
            Test that all RFC 3986 reserved characters are encoded in password.

            Reserved characters that need encoding: : / ? # [ ] @ ! $ & ' ( ) * + , ; =
      <Module test_scim_filters.py>
        Unit tests for SCIM filter parsing

        TDD Regression Tests: These tests validate the SCIM filter parsing logic
        to prevent regression of the missing 'startsWith' operator implementation.

        Tests cover:
        - userName eq (equals) - exact match
        - userName sw (startsWith) - prefix match
        - email eq (equals) - exact match
        - email sw (startsWith) - prefix match
        - Malformed filters (error handling)

        References:
        - RFC 7644: SCIM Protocol
        - OpenAI Codex finding: Missing startsWith operator support
        <Class TestSCIMFilterParsing>
          Unit tests for SCIM filter parsing logic
          <Coroutine test_username_equals_filter_uses_exact_match>
            TDD: Test that 'userName eq' filter uses exact match in Keycloak query

            GIVEN: SCIM filter with 'userName eq "alice"'
            WHEN: list_users endpoint is called
            THEN: Keycloak search_users is called with exact=true flag
          <Coroutine test_username_startswith_filter_uses_prefix_match>
            TDD REGRESSION TEST: Test that 'userName sw' filter uses prefix match

            This test validates the fix for the missing startsWith operator.

            GIVEN: SCIM filter with 'userName sw "ali"'
            WHEN: list_users endpoint is called
            THEN: Keycloak search_users is called WITHOUT exact flag (prefix search)
          <Coroutine test_email_equals_filter_uses_exact_match>
            TDD: Test that 'email eq' filter uses exact match

            GIVEN: SCIM filter with 'email eq "alice@example.com"'
            WHEN: list_users endpoint is called
            THEN: Keycloak search_users is called with exact=true flag
          <Coroutine test_email_startswith_filter_uses_prefix_match>
            TDD REGRESSION TEST: Test that 'email sw' filter uses prefix match

            GIVEN: SCIM filter with 'email sw "alice@"'
            WHEN: list_users endpoint is called
            THEN: Keycloak search_users is called WITHOUT exact flag (prefix search)
          <Coroutine test_malformed_filter_returns_all_users>
            TDD: Test that malformed filters fail-safe by returning all users

            GIVEN: Malformed SCIM filter
            WHEN: list_users endpoint is called
            THEN: All users are returned (fail-safe behavior)
          <Coroutine test_no_filter_returns_all_users>
            TDD: Test that missing filter returns all users

            GIVEN: No SCIM filter provided
            WHEN: list_users endpoint is called
            THEN: All users are returned
        <Class TestSCIMFilterEdgeCases>
          Edge case tests for SCIM filter parsing
          <Coroutine test_filter_with_spaces_in_value>
            Test filter with spaces in the search value
          <Coroutine test_filter_with_special_characters>
            Test filter with special characters in email
          <Coroutine test_pagination_parameters_passed_correctly>
            Test that pagination parameters are passed to Keycloak correctly
      <Module test_search_tools.py>
        Unit tests for search tools

        Tests knowledge base and web search functionality.
        <Class TestSearchKnowledgeBase>
          Test suite for search_knowledge_base tool

          Note: All tests in this class run in the same xdist worker to prevent
          shared state issues with settings/metrics mocking in parallel execution.

          Observability is initialized via session-scoped fixture in conftest.py.
          <Function test_search_with_query>
            Test search with a query string
          <Function test_search_with_different_limits>
            Test search with different result limits
          <Function test_search_default_limit>
            Test search uses default limit
          <Function test_search_empty_query>
            Test search with empty query
          <Function test_search_long_query>
            Test search handles long queries
          <Function test_search_with_qdrant_configured>
            Test search when Qdrant is properly configured
          <Function test_search_qdrant_connection_error>
            Test search handles Qdrant connection errors gracefully
          <Function test_search_qdrant_query_error>
            Test search handles Qdrant query errors
        <Class TestWebSearch>
          Test suite for web_search tool
          <Coroutine test_web_search_with_query>
            Test web search with a query
          <Coroutine test_web_search_different_result_counts>
            Test web search with different result counts
          <Coroutine test_web_search_default_results>
            Test web search uses default result count
          <Coroutine test_web_search_shows_api_notice>
            Test that updated implementation shows API configuration notice
          <Coroutine test_web_search_tavily_success>
            Test successful web search with Tavily API
          <Coroutine test_web_search_serper_success>
            Test successful web search with Serper API
          <Coroutine test_web_search_tavily_network_error>
            Test web search handles Tavily network errors gracefully
          <Coroutine test_web_search_serper_api_error>
            Test web search handles Serper API errors
          <Coroutine test_web_search_tavily_invalid_response>
            Test web search handles invalid Tavily JSON response
          <Coroutine test_web_search_timeout_handling>
            Test web search handles timeout errors
        <Class TestSearchToolSchemas>
          Test search tool schemas
          <Function test_search_knowledge_base_schema>
            Test search_knowledge_base has proper schema
          <Function test_web_search_schema>
            Test web_search has proper schema
      <Module test_security_headers.py>
        Unit tests for HTTP header security utilities.

        This module tests the sanitization of user-controlled data in HTTP headers,
        specifically preventing CWE-113 (HTTP Response Splitting) vulnerabilities.
        <Class TestHeaderSanitization>
          Test suite for HTTP header sanitization to prevent CWE-113
          <Function test_sanitize_header_value_removes_cr_characters>
            Test that carriage return (CR) characters are removed
          <Function test_sanitize_header_value_removes_lf_characters>
            Test that line feed (LF) characters are removed
          <Function test_sanitize_header_value_removes_crlf_sequence>
            Test that CRLF sequence is removed to prevent header injection
          <Function test_sanitize_header_value_preserves_safe_characters>
            Test that safe alphanumeric and common filename characters are preserved
          <Function test_sanitize_header_value_removes_null_bytes>
            Test that null bytes are removed
          <Function test_sanitize_header_value_handles_empty_string>
            Test that empty string is handled correctly
          <Function test_sanitize_header_value_enforces_max_length>
            Test that excessively long values are truncated
          <Function test_sanitize_header_value_with_realistic_attack_payload>
            Test with realistic HTTP response splitting attack
          <Function test_sanitize_header_value_removes_tab_characters>
            Test that tab characters are removed
          <Function test_sanitize_header_value_with_unicode_characters>
            Test that unicode characters are handled safely
        <Class TestHeaderSanitizationForFilenames>
          Test header sanitization specifically for Content-Disposition filenames
          <Function test_sanitize_for_content_disposition_filename>
            Test sanitization for Content-Disposition filename parameter
          <Function test_sanitize_for_content_disposition_prevents_directory_traversal>
            Test that path traversal characters are removed from filenames
          <Function test_sanitize_rfc2231_encoding_alternative>
            Test that RFC 2231 encoding can be used as alternative approach
        <Class TestIntegrationWithGDPREndpoint>
          Integration tests for header security in GDPR data export endpoint
          <Function test_gdpr_export_filename_is_safe>
            Test that GDPR export endpoint uses sanitized filenames
      <Module test_security_logging.py>
        Unit tests for security-focused logging utilities.

        This module tests the sanitization of sensitive data in logs, specifically:
        - JWT token redaction (CWE-200/CWE-532 prevention)
        - Sensitive field stripping from log payloads
        - Hash/truncation of large text fields
        <Class TestLogSanitization>
          Test suite for log sanitization to prevent CWE-200/CWE-532 vulnerabilities
          <Function test_sanitize_for_logging_redacts_token_field>
            Test that JWT token is redacted from log payload
          <Function test_sanitize_for_logging_handles_missing_token>
            Test that sanitization works when token field is absent
          <Function test_sanitize_for_logging_truncates_long_message>
            Test that very long messages are truncated in logs
          <Function test_sanitize_for_logging_preserves_short_message>
            Test that short messages are not truncated
          <Function test_sanitize_for_logging_truncates_query_field>
            Test that query field is also truncated
          <Function test_sanitize_for_logging_creates_shallow_copy>
            Test that sanitization doesn't modify original dict
          <Function test_sanitize_for_logging_handles_nested_token>
            Test that nested token fields are not redacted (shallow only)
          <Function test_sanitize_for_logging_handles_none_value>
            Test that None token value is handled gracefully
          <Function test_sanitize_for_logging_handles_empty_dict>
            Test that empty dict is handled correctly
          <Function test_sanitize_for_logging_with_multiple_sensitive_fields>
            Test sanitization with multiple large text fields
          <Function test_sanitize_for_logging_preserves_non_string_types>
            Test that non-string field types are preserved
          <Function test_sanitize_for_logging_redacts_session_id>
            Test that session_id is redacted to prevent session hijacking
          <Function test_sanitize_for_logging_redacts_user_id>
            Test that user_id is redacted for GDPR/CCPA compliance
          <Function test_sanitize_for_logging_redacts_username>
            Test that username is redacted for privacy protection
          <Function test_sanitize_for_logging_handles_all_sensitive_fields>
            Test comprehensive redaction of all sensitive fields
        <Class TestLogSanitizationWithRealWorldPayloads>
          Test log sanitization with realistic MCP tool call payloads
          <Function test_sanitize_chat_tool_payload>
            Test sanitization of actual chat tool call arguments
          <Function test_sanitize_search_tool_payload>
            Test sanitization of search tool call arguments
          <Function test_sanitize_error_payload_with_validation_failure>
            Test sanitization when logging validation errors
      <Module test_security_practices.py>
        Test security best practices and vulnerability prevention.

        This test suite validates that security best practices are followed:
        - No weak cryptographic hashes for security-sensitive operations
        - No hardcoded unsafe temporary directories
        - SQL injection prevention through proper parameterization

        These tests follow TDD principles:
        RED Phase: Tests fail when insecure patterns are present
        GREEN Phase: Tests pass after security fixes are applied
        <Class TestCryptographicSecurity>
          Test that cryptographic operations use secure algorithms.
          <Function test_kubernetes_sandbox_does_not_use_insecure_md5>
            Test that Kubernetes job name generation doesn't use MD5 insecurely.

            FAILS when: MD5 is used without usedforsecurity=False
            PASSES when: SHA-256 is used OR MD5 has usedforsecurity=False
          <Function test_job_name_hash_produces_valid_kubernetes_names>
            Test that job name generation produces valid Kubernetes resource names.

            This ensures the hash function (MD5 or SHA-256) produces acceptable names.
        <Class TestTemporaryDirectorySecurity>
          Test that temporary directory usage follows security best practices.
          <Function test_builder_api_does_not_use_hardcoded_tmp>
            Test that builder API doesn't hardcode /tmp directory.

            FAILS when: "/tmp" is hardcoded as default
            PASSES when: Using tempfile.gettempdir() or similar secure method
          <Function test_builder_output_directory_has_safe_default>
            Test that the default output directory is secure.

            FAILS when: Default is world-writable /tmp without protection
            PASSES when: Default uses application-specific secure directory
          <Function test_path_validation_prevents_directory_traversal>
            Test that path validation prevents directory traversal attacks.

            This ensures that even if /tmp is used, proper validation prevents escaping.
        <Class TestSQLInjectionPrevention>
          Test that SQL operations properly prevent injection attacks.
          <Coroutine test_postgres_storage_uses_parameterized_queries>
            Test that PostgreSQL storage uses parameterized queries.

            FAILS when: String concatenation is used for SQL values
            PASSES when: Parameterized queries ($1, $2, etc.) are used
          <Coroutine test_field_name_validation_uses_allowlist>
            Test that dynamic field names are validated against allowlists.

            FAILS when: User input is directly used in SQL field names
            PASSES when: Field names are validated against a predefined allowlist
          <Coroutine test_sql_injection_attempt_is_rejected>
            Test that SQL injection attempts in field names are rejected.

            FAILS when: Injection attempts are processed
            PASSES when: Invalid field names are rejected
          <Coroutine test_malicious_sql_values_are_safely_escaped>
            Test that malicious SQL values are properly parameterized.

            FAILS when: Values are concatenated into SQL strings
            PASSES when: Values are passed as parameters (preventing injection)
        <Class TestSecurityDocumentation>
          Test that security-sensitive code has proper documentation.
          <Function test_postgres_storage_has_security_comments>
            Test that PostgreSQL storage has security documentation.

            FAILS when: No comments explain SQL injection prevention
            PASSES when: Security comments explain the mitigation strategy
          <Function test_nosec_suppressions_are_documented>
            Test that bandit nosec suppressions include explanatory comments.

            FAILS when: nosec is used without explanation
            PASSES when: Comments explain why suppression is safe
      <Module test_serializable_mocks_validation.py>
        Test Suite for Serializable Mock Validation

        This test module ensures that serializable mocks used in performance regression
        tests maintain compatibility with LangGraph's checkpoint serialization system.

        These tests prevent the following classes of issues:
        1. Mixing Python @dataclass with Pydantic BaseModel (incompatible)
        2. Missing Pydantic Field definitions for mutable defaults
        3. Serialization/deserialization failures in LangGraph checkpoints

        Test-Driven Development (TDD) Approach:
        - RED: These tests would fail if @dataclass is used with BaseChatModel
        - GREEN: Tests pass with proper Pydantic Field usage
        - REFACTOR: Validates best practices for mock object design
        <Class TestSerializableLLMMockValidation>
          Validation tests for SerializableLLMMock to prevent Pydantic conflicts.
          <Function test_serializable_llm_mock_not_using_dataclass>
            CRITICAL: Ensure SerializableLLMMock does NOT use @dataclass decorator.

            Background:
            - BaseChatModel (from LangChain) inherits from Pydantic BaseModel
            - Python @dataclass and Pydantic BaseModel are incompatible
            - Mixing them causes: ValueError: mutable default ... use default_factory

            This test prevents regression to the bug found in run 19148140358.
          <Function test_serializable_llm_mock_is_pydantic_model>
            Ensure SerializableLLMMock properly inherits from Pydantic BaseModel.
          <Function test_serializable_llm_mock_uses_pydantic_fields>
            Validate that SerializableLLMMock uses Pydantic Field() for all attributes.
          <Function test_serializable_llm_mock_instantiation>
            Test that SerializableLLMMock can be instantiated without errors.

            This would fail if there were Pydantic/dataclass conflicts.
          <Function test_serializable_llm_mock_serialization>
            Test that SerializableLLMMock can be serialized/deserialized.

            This is critical for LangGraph checkpoint system.
          <Function test_serializable_llm_mock_response_generation>
            Test that SerializableLLMMock generates responses correctly.
          <Coroutine test_serializable_llm_mock_async_generation>
            Test async response generation.
          <Function test_serializable_llm_mock_call_counting>
            Test that call_count tracking works correctly.
          <Function test_serializable_llm_mock_reset>
            Test reset functionality.
        <Class TestSerializableToolMockValidation>
          Validation tests for SerializableToolMock.
          <Function test_serializable_tool_mock_is_plain_class>
            SerializableToolMock can use @dataclass since it doesn't inherit from Pydantic.
          <Function test_serializable_tool_mock_instantiation>
            Test tool mock instantiation.
          <Function test_serializable_tool_mock_invocation>
            Test tool invocation and call tracking.
          <Coroutine test_serializable_tool_mock_async_invocation>
            Test async tool invocation.
        <Class TestMockCompatibilityGuards>
          Guard tests to prevent incompatible patterns in mock objects.
          <Function test_no_dataclass_on_pydantic_subclasses>
            META-TEST: Scan all test fixtures for @dataclass on Pydantic subclasses.

            This test prevents the entire class of bugs where @dataclass is
            accidentally mixed with Pydantic BaseModel inheritance.
          <Function test_pydantic_mocks_have_required_methods>
            Validate that Pydantic-based mocks implement required LangChain methods.
      <Module test_server_streamable_init.py>
        Unit tests for server_streamable module initialization.

        Tests verify that the module uses standard logging for module-level code
        and observability logger only after lifespan initialization.

        NOTE: These are simplified verification tests. Full module import testing
        is complex due to dependency injection and is better validated via
        integration tests and production deployment verification.
        <Function test_module_level_logger_uses_standard_logging>
          Test that server_streamable.py uses standard logging.getLogger() for module-level code.

          This prevents RuntimeError when importing the module before observability is initialized.

          CRITICAL: Module-level code should NEVER use observability logger/tracer/metrics.
        <Function test_rate_limiting_logs_use_standard_logger>
          Verify that rate limiting initialization uses standard logger, not observability logger.

          This is the specific fix for the production crash:
          RuntimeError: Observability not initialized at line 187
        <Function test_observability_logger_not_used_at_module_level>
          Verify that observability logger is not used at module level (outside functions).

          This prevents the RuntimeError that was causing production pod crashes.
      <Module test_time_provider.py>
        Unit tests for TimeProvider abstraction and VirtualClock implementation.

        These tests validate the time provider pattern that enables fast-forwarding time
        in tests without real sleep() calls, eliminating 50+ seconds of sleep overhead.

        TDD: These tests are written FIRST (RED phase) before implementing the time provider.
        <Function test_time_provider_protocol_defines_required_methods>
          TimeProvider protocol must define time(), sleep(), and monotonic() methods.
        <Class TestRealTimeProvider>
          Tests for RealTimeProvider (wrapper around Python's time module).
          <Function test_real_time_provider_returns_current_time>
            RealTimeProvider.time() should return current Unix timestamp.
          <Function test_real_time_provider_sleep_blocks_for_duration>
            RealTimeProvider.sleep() should block for specified duration.
          <Function test_real_time_provider_monotonic_increases>
            RealTimeProvider.monotonic() should return monotonically increasing values.
        <Class TestVirtualClock>
          Tests for VirtualClock (fast-forward time without real sleep).
          <Function test_virtual_clock_starts_at_zero>
            VirtualClock should start at time=0.0 by default.
          <Function test_virtual_clock_starts_at_custom_time>
            VirtualClock should allow custom start time.
          <Function test_virtual_clock_advance_increases_time>
            VirtualClock.advance() should increase current time.
          <Function test_virtual_clock_sleep_advances_time_instantly>
            VirtualClock.sleep() should advance time without real blocking.
          <Function test_virtual_clock_sleep_zero_does_nothing>
            VirtualClock.sleep(0) should not change time.
          <Function test_virtual_clock_sleep_negative_raises_error>
            VirtualClock.sleep() with negative duration should raise ValueError.
          <Function test_virtual_clock_monotonic_tracks_time>
            VirtualClock.monotonic() should track current time.
          <Function test_virtual_clock_multiple_instances_independent>
            Multiple VirtualClock instances should be independent.
        <Class TestAsyncVirtualClock>
          Tests for VirtualClock with async sleep support.
          <Coroutine test_virtual_clock_async_sleep_advances_time_instantly>
            VirtualClock.async_sleep() should advance time without real blocking.
          <Coroutine test_virtual_clock_async_sleep_negative_raises_error>
            VirtualClock.async_sleep() with negative duration should raise ValueError.
          <Coroutine test_virtual_clock_async_sleep_multiple_tasks>
            VirtualClock.async_sleep() should advance time consistently across tasks.
        <Class TestTimeProviderIntegration>
          Integration tests proving TimeProvider works with real components.
          <Function test_time_provider_protocol_satisfied_by_implementations>
            Verify RealTimeProvider and VirtualClock satisfy TimeProvider protocol.
          <Function test_virtual_clock_enables_instant_ttl_testing>
            Demonstrate that VirtualClock eliminates sleep overhead in TTL tests.
      <Module test_tools_catalog.py>
        Unit tests for tools catalog and registry

        Tests tool organization and access patterns.
        <Class TestToolsCatalog>
          Test suite for tools catalog
          <Function test_all_tools_count>
            Test that ALL_TOOLS contains expected number of tools
          <Function test_all_tools_are_base_tool>
            Test that all tools are BaseTool instances
          <Function test_all_tools_have_unique_names>
            Test that all tools have unique names
          <Function test_calculator_tools_count>
            Test calculator tools group
          <Function test_search_tools_count>
            Test search tools group
          <Function test_filesystem_tools_count>
            Test filesystem tools group
          <Function test_get_tools_all>
            Test get_tools returns all tools when no categories specified
          <Function test_get_tools_by_category>
            Test get_tools with specific categories
          <Function test_get_tools_multiple_categories>
            Test get_tools with multiple categories
          <Function test_get_tools_invalid_category>
            Test get_tools with invalid category returns empty list
          <Function test_get_tool_by_name_existing>
            Test getting tool by name
          <Function test_get_tool_by_name_nonexistent>
            Test getting nonexistent tool returns None
          <Function test_all_tools_have_descriptions>
            Test that all tools have descriptions
          <Function test_all_tools_have_args_schema>
            Test that all tools have args schema
          <Function test_tool_names_match_expected>
            Test that tool names match expected values
        <Class TestToolInvocation>
          Test tool invocation patterns
          <Function test_tools_are_invocable>
            Test that all tools can be invoked
          <Function test_tools_handle_errors_gracefully>
            Test that tools handle errors without crashing
          <Function test_tool_schemas_are_serializable>
            Test that tool schemas can be serialized to JSON
      <Module test_user_id_normalization.py>
        Test user_id normalization for handling multiple formats.

        Ensures that both plain usernames and OpenFGA-prefixed IDs work correctly.
        <Function test_normalize_plain_username>
          Test normalization of plain username.
        <Function test_normalize_openfga_user_format>
          Test normalization of OpenFGA user:username format.
        <Function test_normalize_other_prefixes>
          Test normalization with other prefix formats.
        <Function test_normalize_multiple_colons>
          Test normalization when ID contains multiple colons.
        <Function test_normalize_empty_and_none>
          Test normalization with edge cases.
        <Coroutine test_authenticate_accepts_both_formats>
          Test that authenticate() accepts both username formats.
        <Coroutine test_authenticate_normalization_with_unknown_user>
          Test that authentication fails correctly for unknown users in both formats.
        <Function test_normalize_user_id_idempotent>
          Test that normalization is idempotent.
      <Module test_user_provider_verify_password.py>
        Unit tests for UserProvider.verify_password() method.

        Tests verify that the verify_password method properly validates user credentials
        and returns structured PasswordVerification responses with appropriate error handling.
        <Class TestUserProviderVerifyPassword>
          Test verify_password functionality across UserProvider implementations
          <Coroutine test_verify_password_success>
            Test successful password verification returns valid=True with user data.

            Expected behavior: Correct username + password returns structured response.
          <Coroutine test_verify_password_wrong_password>
            Test that wrong password returns valid=False with appropriate error.

            Security: Should not reveal whether username exists or password is wrong.
            Uses generic "Invalid credentials" message to prevent information disclosure.
          <Coroutine test_verify_password_nonexistent_user>
            Test that non-existent username returns valid=False.

            Security: Should not reveal whether username exists (timing attacks).
          <Coroutine test_verify_password_empty_username>
            Test that empty username is handled gracefully.

            Edge case: Empty string should not cause exceptions.
          <Coroutine test_verify_password_empty_password>
            Test that empty password is handled gracefully.

            Edge case: Empty password should fail validation.
          <Coroutine test_verify_password_with_hashing_enabled>
            Test verify_password respects password hashing settings.

            Uses bcrypt hashing for production-like testing.
          <Coroutine test_verify_password_case_sensitive>
            Test that username comparison is case-sensitive.

            Security: Usernames should be treated as case-sensitive.
          <Coroutine test_verify_password_multiple_users>
            Test that verify_password works correctly with multiple users.

            Ensures no cross-user contamination.
          <Coroutine test_password_verification_model_structure>
            Test that PasswordVerification model has expected structure.

            Ensures contract compliance for consumers.
          <Coroutine test_verify_password_does_not_modify_user_db>
            Test that verify_password is read-only (doesn't modify user database).

            Ensures verification doesn't have side effects.
      <Module test_version_sync.py>
        Test version synchronization across all modules.

        Ensures that version is consistent across:
        - pyproject.toml (single source of truth)
        - __init__.py
        - config.py
        - observability telemetry
        <Function test_version_matches_pyproject>
          Test that __version__ in __init__.py matches pyproject.toml.
        <Function test_config_version_matches>
          Test that config.service_version matches package version.
        <Function test_telemetry_version_matches>
          Test that telemetry resource uses correct version.
        <Function test_version_format>
          Test that version follows semantic versioning format.
      <Module test_vertex_ai_provider_detection.py>
        Unit tests for Vertex AI provider detection in LLMFactory.

        Tests the _get_provider_from_model method to ensure it correctly identifies
        Vertex AI models for both Anthropic Claude and Google Gemini models.
        <Class TestVertexAIProviderDetection>
          Test Vertex AI provider detection for various model names.
          <Function test_detect_vertex_ai_claude_sonnet_with_prefix>
            Test detection of Vertex AI Claude Sonnet with vertex_ai/ prefix.
          <Function test_detect_vertex_ai_claude_haiku_with_prefix>
            Test detection of Vertex AI Claude Haiku with vertex_ai/ prefix.
          <Function test_detect_vertex_ai_claude_opus_with_prefix>
            Test detection of Vertex AI Claude Opus with vertex_ai/ prefix.
          <Function test_detect_vertex_ai_gemini_pro_with_prefix>
            Test detection of Vertex AI Gemini 3.0 Pro with vertex_ai/ prefix.
          <Function test_detect_vertex_ai_gemini_flash_with_prefix>
            Test detection of Vertex AI Gemini Flash with vertex_ai/ prefix.
          <Function test_detect_anthropic_direct_claude_sonnet>
            Test detection of direct Anthropic API (without vertex_ai/ prefix).
          <Function test_detect_anthropic_direct_claude_haiku>
            Test detection of direct Anthropic API for Haiku.
          <Function test_detect_google_direct_gemini_pro>
            Test detection of Google AI Studio API (without vertex_ai/ prefix).
          <Function test_detect_google_direct_gemini_flash>
            Test detection of Google AI Studio API for Flash.
          <Function test_vertex_ai_prefix_takes_precedence_over_pattern>
            Test that vertex_ai/ prefix takes precedence over model name pattern.
          <Function test_detect_openai_models>
            Test detection of OpenAI models (should not be affected).
          <Function test_detect_azure_models>
            Test detection of Azure OpenAI models.
          <Function test_detect_bedrock_models>
            Test detection of AWS Bedrock models.
          <Function test_detect_ollama_models>
            Test detection of Ollama models.
          <Function test_case_insensitive_detection>
            Test that provider detection is case-insensitive.
          <Function test_fallback_to_default_provider>
            Test fallback to default provider for unknown models.
        <Class TestVertexAIModelNamingConventions>
          Test various Vertex AI model naming conventions.
          <Function test_claude_3_family_models_via_vertex_ai>
            Test older Claude 3 family models via Vertex AI.
          <Function test_claude_4_family_models_via_vertex_ai>
            Test latest Claude 4 family models via Vertex AI.
          <Function test_gemini_model_variants>
            Test various Gemini model variants via Vertex AI.
          <Function test_model_name_with_unusual_spacing>
            Test model names with extra whitespace (should be handled).
      <Dir tools>
        <Module test_code_execution_tools.py>
          Unit tests for code execution tools

          Tests execute_python tool with mocked sandbox.
          Following TDD best practices - these tests should FAIL until implementation is complete.
          <Class TestExecutePythonTool>
            Test execute_python tool
            <Function test_tool_has_correct_metadata>
              Test tool has name and description
            <Function test_tool_schema_has_code_parameter>
              Test tool schema includes code parameter
            <Function test_simple_code_execution>
              Test executing simple code
            <Function test_code_validation_before_execution>
              Test that code is validated before execution
            <Function test_empty_code_rejection>
              Test that empty code is rejected
            <Function test_execution_error_handling>
              Test handling of execution errors
            <Function test_timeout_handling>
              Test handling of timeout
            <Function test_output_truncation>
              Test that long output is truncated
            <Function test_custom_timeout>
              Test executing with custom timeout
          <Class TestCodeExecutionToolIntegration>
            Test integration with other components
            <Function test_uses_configuration_settings>
              Test that tool uses settings from config
            <Function test_backend_selection_docker>
              Test Docker backend selection
            <Function test_backend_selection_kubernetes>
              Test Kubernetes backend selection

==================================== ERRORS ====================================
_______ ERROR collecting tests/integration/api/test_app_configuration.py _______
tests/integration/api/test_app_configuration.py:17: in <module>
    from mcp_server_langgraph.app import create_app
src/mcp_server_langgraph/app.py:152: in <module>
    app = create_app(skip_startup_validation=_is_pytest_session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/mcp_server_langgraph/app.py:132: in create_app
    run_startup_validation()
src/mcp_server_langgraph/api/health.py:201: in run_startup_validation
    raise SystemValidationError(error_msg)
E   mcp_server_langgraph.api.health.SystemValidationError: Startup validation failed: database_connectivity: PostgreSQL connection failed: Multiple exceptions: [Errno 111] Connect call failed ('127.0.0.1', 5432), [Errno 101] Network is unreachable
____ ERROR collecting tests/integration/test_database_connectivity_real.py _____
.venv/lib/python3.12/site-packages/_pytest/python.py:498: in importtestmodule
    mod = import_path(
.venv/lib/python3.12/site-packages/_pytest/pathlib.py:587: in import_path
    importlib.import_module(module_name)
/usr/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:177: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:357: in _rewrite_test
    tree = ast.parse(source, filename=strfn)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.12/ast.py:52: in parse
    return compile(source, filename, mode, flags,
E     File "/home/vishnu/git/vishnu2kmohan/worktrees/mcp-server-langgraph-session-20251118-221044/tests/integration/test_database_connectivity_real.py", line 111
E       is_healthy, message = await check_database_connectivity(postgres_url, timeout=5.0)
E   IndentationError: unexpected indent
=========================== short test summary info ============================
ERROR tests/integration/api/test_app_configuration.py - mcp_server_langgraph....
ERROR tests/integration/test_database_connectivity_real.py
!!!!!!!!!!!!!!!!!!! Interrupted: 2 errors during collection !!!!!!!!!!!!!!!!!!!!
======== 4272/5174 tests collected (902 deselected), 2 errors in 5.48s =========
