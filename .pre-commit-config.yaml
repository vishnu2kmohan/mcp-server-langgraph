repos:
- repo: https://github.com/pre-commit/pre-commit-hooks
  rev: v6.0.0
  hooks:
  - id: trailing-whitespace
  - id: end-of-file-fixer
  - id: check-yaml
    args:
    - --allow-multiple-documents
    exclude: ^deployments/helm/mcp-server-langgraph/templates/
  - id: check-added-large-files
    args:
    - --maxkb=500
    exclude: ^uv\.lock$
  - id: check-merge-conflict
  - id: detect-private-key
  - id: check-json
  - id: check-toml
  - id: mixed-line-ending
  - id: check-ast
    name: Check Python AST (Syntax Validation)
    description: 'Validates Python files can be parsed (no SyntaxError).

      Prevents commits of syntactically invalid Python code.

      Regression prevention for commit a57fcc95 (pytestmark inside imports).
      '
- repo: https://github.com/astral-sh/ruff-pre-commit
  rev: v0.8.4
  hooks:
  - id: ruff
    name: Ruff Linter (replaces isort + flake8)
    args:
    - --fix
    exclude: ^clients/(python|go|typescript)/
  - id: ruff-format
    name: Ruff Formatter (replaces Black)
    args:
    - --line-length=127
    exclude: ^clients/(python|go|typescript)/
- repo: https://github.com/pycqa/bandit
  rev: 1.8.6
  hooks:
  - id: bandit
    args:
    - -lll
    - --skip
    - B608
- repo: https://github.com/pre-commit/mirrors-mypy
  rev: v1.13.0
  hooks:
  - id: mypy
    name: MyPy Type Checking (Blocking)
    entry: uv run mypy src/mcp_server_langgraph
    language: system
    types: [python]
    args:
    - --config-file=pyproject.toml
    - --show-error-codes
    - --pretty
    pass_filenames: false
    # MyPy Configuration Status (2025-11-23)
    #
    # ✅ FIXED: All 46 type errors resolved!
    # ✅ ENVIRONMENT: Uses system Python with full project dependencies via uv
    # Stage: pre-push (BLOCKING - maintains local/CI parity)
    #
    # Modern best practice: Per-module overrides in pyproject.toml
    #   - Strict mode for internal code (ignore_missing_imports = false)
    #   - Lenient mode for third-party libraries (per-module overrides)
    #
    # Environment approach:
    #   - language: system (uses uv-managed environment)
    #   - Full project dependencies available
    #   - Matches local development and CI/CD exactly
    #
    # Test: tests/meta/test_mypy_enforcement.py::test_mypy_passes_on_current_codebase ✅
    stages:
    - pre-push
- repo: https://github.com/gitleaks/gitleaks
  rev: v8.28.0
  hooks:
  - id: gitleaks
- repo: local
  hooks:
  # NOTE: mypy-non-blocking hook REMOVED (2025-11-23)
  # Reason: Redundant now that main MyPy hook is enabled and blocking
  # Main MyPy hook runs on pre-push stage and fails on type errors
  # This maintains local/CI parity (no more non-blocking warnings)
  - id: check-subprocess-timeout
    name: Enforce subprocess timeout parameters
    description: 'Ensures all subprocess.run() calls include timeout parameter to prevent test hangs.


      Prevents regression of 119 subprocess.run() calls without timeout.

      Fix: Add timeout=60 parameter to subprocess.run() calls.

      '
    entry: python .pre-commit-hooks/check_subprocess_timeout.py
    language: system
    types: [python]
    files: ^tests/.*\.py$
    stages:
    - pre-commit
    - pre-push
  - id: check-banned-imports
    name: Validate no banned/deprecated imports
    description: 'Detects usage of deprecated imports (e.g., toml instead of tomllib).


      Prevents issues like:
      - import toml (deprecated in Python 3.11+)
      - import imp (removed in Python 3.12+)

      '
    entry: python .pre-commit-hooks/check_banned_imports.py
    language: system
    types: [python]
    stages:
    - pre-commit
    - pre-push
  - id: uv-lock-check
    name: Validate uv.lock synchronized with pyproject.toml
    entry: bash -c 'uv lock --check || (echo "ERROR - uv.lock out of sync. Run uv lock"; exit 1)'
    language: system
    files: ^(pyproject\.toml|uv\.lock)$
    pass_filenames: false
    stages:
    - pre-push
  - id: uv-pip-check
    name: Validate no dependency conflicts
    entry: bash -c 'uv pip check || (echo "ERROR - Dependency conflicts detected. Run uv pip check to see details"; exit 1)'
    language: system
    pass_filenames: false
    stages:
    - pre-push
    description: 'Validates that installed dependencies have no conflicts.


      Why this check is critical:
      - uv lock --check validates lockfile is current
      - uv pip check validates no dependency conflicts exist
      - Both are needed for comprehensive dependency validation

      Example failure this prevents:
      1. Package A requires foo>=2.0, Package B requires foo<2.0
      2. uv lock succeeds and generates lockfile (lock check passes)
      3. uv pip check FAILS (conflict detected)
      4. Without this hook: conflict pushed to CI and fails there

      Matches CI validation in .github/workflows/ci.yaml
      Test: tests/meta/test_local_ci_parity.py::test_pre_push_includes_uv_pip_check

      '
  - id: run-pre-push-tests
    name: Run Pre-Push Tests (consolidated - dev profile)
    entry: scripts/run_pre_push_tests.py
    language: system
    pass_filenames: false
    files: ^(src/.*\.py|tests/.*\.py|pyproject\.toml)$
    always_run: false
    stages:
    - pre-push
    description: 'Consolidated pre-push test suite (OpenAI Codex Finding 2a fix).


      Combines 5 separate pytest sessions into 1 for efficiency:

      - Combined marker: -m "(unit or api or property) and not llm"

      - This covers: unit tests, smoke tests, API tests, MCP server tests, property tests

      - Property tests use dev profile (25 examples) locally, ci profile (100 examples) in CI


      Time savings: 10-25 seconds vs separate sessions (no duplicate test discovery)


      CI_PARITY mode: Set CI_PARITY=1 to include integration tests (requires Docker)

      Example: CI_PARITY=1 git push


      Benefits:

      - Single test discovery (2-5s instead of 10-25s for 5 sessions)

      - No pytest cache/coverage lock contention

      - Fail-fast across all test categories

      - Parallel execution with pytest-xdist (-n auto)


      CI Parity: CI runs with HYPOTHESIS_PROFILE=ci (100 examples)

      Local Speed: 3-5min (vs 8-12min with CI profile)


      Full CI-equivalent validation: make validate-pre-push or CI_PARITY=1 git push

      '
  - id: run-integration-tests
    name: Run Integration Tests (manual - requires Docker)
    entry: bash -c 'OTEL_SDK_DISABLED=true uv run pytest -n auto tests/integration/ -v --tb=short'
    language: system
    pass_filenames: false
    files: ^(src/.*\.py|tests/integration/.*\.py)$
    always_run: false
    stages:
    - manual
    description: 'Runs integration tests with real infrastructure (Docker required).


      Moved to manual stage 2025-11-16 per OpenAI Codex finding - integration

      tests require Docker stack and add 2-3min to pre-push duration.


      Run manually: SKIP= pre-commit run run-integration-tests --all-files

      Or via make: make test-integration

      Or with pre-push: CI_PARITY=1 git push (auto-includes if Docker available)


      CI runs these automatically. Use for local validation of integration changes.

      '
- repo: https://github.com/python-jsonschema/check-jsonschema
  rev: 0.29.4
  hooks:
  - id: check-github-workflows
    name: Validate GitHub Actions workflows
    description: Validates GitHub Actions workflow syntax
    files: ^\.github/workflows/.*\.ya?ml$
- repo: local
  hooks:
  - id: actionlint-workflow-validation
    name: Validate GitHub Actions with actionlint
    entry: >
      bash -c 'if command -v actionlint >/dev/null 2>&1; then
      actionlint -no-color -shellcheck= .github/workflows/*.yml .github/workflows/*.yaml 2>&1;
      else echo "Skipping: actionlint not installed (install from https://github.com/rhysd/actionlint)";
      exit 0; fi'
    language: system
    files: ^\.github/workflows/.*\.ya?ml$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: 'Advanced GitHub Actions workflow validation using actionlint.


      CLI Availability Guard: Gracefully skips if actionlint is not installed.


      Catches issues like:

      - Invalid context usage (secrets.* in job-level if conditions)

      - Missing job dependencies (needs declarations)

      - Expression syntax errors

      - Invalid workflow syntax


      Complements check-github-workflows with deeper validation.

      '
  - id: check-test-sleep-duration
    name: Check Test Sleep Durations
    entry: uv run python scripts/validation/check_test_sleep_duration.py
    language: system
    files: ^tests/.*\.py$
    exclude: ^tests/meta/test_sleep_duration_linter\.py$
    pass_filenames: true
    description: 'Prevents test performance regressions by detecting excessive sleep() calls.


      Enforces:

      - Unit tests: max 0.5s sleep

      - Integration tests: max 2.0s sleep


      Use VirtualClock for instant time advancement instead of real sleep.

      '
  - id: validate-fast
    name: Run Fast Validators (Consolidated)
    entry: uv run python scripts/validation/validate_fast.py
    language: system
    pass_filenames: false
    always_run: true
    stages:
    - pre-push
    description: "Consolidated runner for fast validation scripts to reduce overhead.\nIncludes: pytest-config, pre-push-hook, repo-root, time-bombs, async-fixture-scope, migration-idempotency, api-schemas."

  - id: validate-mdx-extensions
    name: Validate MDX File Extensions in docs/
    entry: uv run python scripts/validation/mdx_extension_validator.py
    language: system
    files: ^docs/.*\.(md|mdx)$
    pass_filenames: false
    always_run: false
    description: 'Validates all files in docs/ use .mdx extension (TDD):

      - Detects .md files in docs/ directory

      - Ensures Mintlify compatibility

      - Excludes template and node_modules directories


      Prevents regression of issues fixed in 2025-11-12 audit:

      - TRY_EXCEPT_PASS_ANALYSIS.md (converted to .mdx)

      - SECRETS.md (converted to .mdx)

      - gke-autopilot-resource-constraints.md (converted to .mdx)


      Solution: Convert .md to .mdx or move outside docs/

      See: tests/unit/documentation/test_mdx_extension_validator.py

      '
  - id: check-frontmatter-quotes
    name: Check Frontmatter Quote Style
    entry: uv run python scripts/docs/standardize_frontmatter.py docs --dry-run
    language: system
    files: \.mdx$
    pass_filenames: false
    always_run: false
  - id: validate-file-naming-conventions
    name: Validate Documentation File Naming Conventions
    entry: uv run python scripts/validation/file_naming_validator.py --path docs
    language: system
    files: ^docs/.*\.(md|mdx)$
    pass_filenames: false
    always_run: false
    description: |
      Validates that documentation files follow kebab-case naming conventions.

      Enforces:
      - All .mdx files must be lowercase
      - Use hyphens (-) not underscores (_)
      - No spaces in filenames
      - No .md files in docs/ directory (must use .mdx)

      Prevents regression of uppercase file naming issues.

      See: docs/references/documentation-authoring-guide
      Tests: tests/unit/documentation/test_file_naming_validator.py
  - id: audit-todo-fixme-markers
    name: Audit TODO/FIXME/XXX Markers in Documentation
    entry: uv run python scripts/validation/todo_audit.py --quiet
    language: system
    files: ^docs/.*\.(md|mdx)$
    pass_filenames: false
    always_run: false
    stages:
    - manual
    description: "Audits documentation for TODO/FIXME/XXX markers (TDD):\n- Finds all TODO and FIXME comments\n- Detects XXX\
      \ placeholders in examples\n- Reports location and content\n- Tracks outstanding work items\n\nNote: This is informational\
      \ only (exit code 0). Use for audits.\nRun manually: python scripts/validation/todo_audit.py --docs-dir docs\nTests:\
      \ tests/unit/documentation/test_todo_audit.py (14 tests \u2705)\n"
  - id: validate-adr-sync
    name: "Validate ADR Synchronization (/adr \u2194 /docs/architecture)"
    entry: uv run python scripts/validation/adr_sync_validator.py
    language: system
    files: ^(adr/.*\.md|docs/architecture/.*\.mdx)$
    pass_filenames: false
    always_run: false
    description: 'Validates ADR synchronization between source and docs (TDD):

      - All ADRs in /adr exist in /docs/architecture

      - All ADRs in /docs/architecture exist in /adr

      - Reports missing or orphaned ADRs

      - Prevents documentation drift


      Ensures Architecture Decision Records stay synchronized.

      Run: python scripts/validation/adr_sync_validator.py

      Exit code 1 if out of sync, 0 if synchronized

      '
  - id: mintlify-broken-links-check
    name: Mintlify Broken Links Validation (PRIMARY Validator)
    entry: >
      bash -c 'if command -v npm >/dev/null 2>&1; then
      REPO_ROOT=$(git rev-parse --show-toplevel) && cd "$REPO_ROOT/docs" && npm exec -- mintlify broken-links;
      else echo "Skipping: npm/Node.js not installed (install from https://nodejs.org/)";
      exit 0; fi'
    language: system
    files: ^docs/.*\.mdx$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: "\U0001F3AF PRIMARY VALIDATOR for Mintlify documentation (docs/)\n\nThis is now the AUTHORITATIVE validator\
      \ for docs/ - runs on pre-push and in CI.\nReplaces 5 previous Python validators (link_validator, navigation_validator,\n\
      image_validator, frontmatter_validator, check_internal_links).\n\nComprehensive Checks (2025-11-15 - Simplified Validation):\n\
      \u2705 All internal links resolve correctly\n\u2705 Anchor links are valid (#section references)\n\u2705 Navigation\
      \ references work (docs.json \u2194 MDX files)\n\u2705 Page cross-references are accurate\n\u2705 Image references are\
      \ valid\n\u2705 Frontmatter is complete (title, description)\n\u2705 MDX syntax is valid\n\u2705 No orphaned pages\n\
      \nRuns automatically on git push (pre-push stage):\n- Duration: ~8-12s (acceptable within pre-push 8-12 min budget)\n\
      - Catches MDX syntax errors BEFORE pushing to remote\n- Prevents broken documentation from reaching repository\n- More\
      \ reliable than custom Python validators\n\nManual run for immediate validation:\n$ SKIP= pre-commit run mintlify-broken-links-check\
      \ --all-files\nOR\n$ make docs-validate-mintlify\nOR\n$ cd docs && npx mintlify broken-links\n\nMigration (2025-11-15):\n\
      - Mintlify CLI is now PRIMARY validator (pre-push + CI)\n- Removed redundant Python validators from pre-push\n- Moved\
      \ from manual to pre-push to catch errors before remote push\n- See: docs-internal/DOCS_VALIDATION_SIMPLIFICATION.md\n"
  - id: validate-documentation-integrity
    name: Validate Documentation Integrity (ADRs, Monitoring, Mermaid)
    entry: uv run pytest tests/meta/validation/test_documentation_integrity.py -v --tb=short
    language: system
    files: \.(md|mdx|json)$|^monitoring/.*README\.md$
    pass_filenames: false
    always_run: false
    stages:
    - manual
    description: 'Documentation integrity and completeness tests (TDD):

      - ADR synchronization between adr/ and docs/architecture/

      - Architecture overview ADR count accuracy

      - Mermaid diagram structure validation

      - Monitoring subdirectories have comprehensive READMEs (>50 lines)

      - No HTML comments in MDX (use JSX comments)

      - JSX comments properly closed

      Ensures documentation standards maintained (2025-11-07 audit).

      '
  - id: validate-documentation-structure
    name: Validate Documentation Structure (Orphaned Files, ADR Numbering, TODOs, Badges, Links)
    entry: uv run pytest tests/regression/test_documentation_structure.py -v --tb=short
    language: system
    files: \.(md|mdx|json)$|^adr/.*\.md$|^README\.md$
    pass_filenames: false
    always_run: false
    stages:
    - manual
    description: 'Comprehensive documentation structure validation (TDD):

      - Prevents orphaned MDX files not in docs.json navigation

      - Detects duplicate ADR numbering

      - Validates ADR synchronization between adr/ and docs/architecture/

      - Validates README.md ADR badge count matches actual ADR count

      - Ensures public docs don''t have TODO/FIXME comments (excludes templates/archives)

      - Detects broken internal links between documentation pages

      - Validates frontmatter presence in MDX files

      - Checks version consistency across deployment files

      - Confirms essential root documentation files exist

      Regression prevention for issues fixed in documentation audit (2025-11-12).

      Enhanced 2025-11-12: Added badge validation, comprehensive TODO detection, link checking.

      '
  - id: validate-adr-index
    name: Validate ADR Index is Up-to-Date
    entry: python scripts/docs/generate_adr_index.py --check
    language: system
    files: ^adr/.*\.md$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: 'Ensures adr/README.md index is up-to-date with all ADR files.


      Validates:

      - All ADRs are listed in the index with correct metadata

      - Categories are accurate and complete

      - No duplicate ADR numbers

      - Index generation instructions are current


      To regenerate: python scripts/generate_adr_index.py


      Regression prevention for REC-001 from documentation audit (2025-11-12).

      '
  - id: validate-deployment-secrets
    name: Validate deployment secret keys alignment
    entry: uv run pytest tests/deployment/test_helm_configuration.py::test_deployment_secret_keys_exist_in_template -v --tb=short
    language: system
    files: ^deployments/helm/.*\.(yaml|yml)$
    pass_filenames: false
    stages:
    - pre-push
  - id: validate-cors-security
    name: Validate CORS security configuration
    entry: uv run pytest tests/deployment/test_helm_configuration.py::test_kong_cors_not_wildcard_with_credentials -v --tb=short
    language: system
    files: ^deployments/(kong|base)/.*\.(yaml|yml)$
    pass_filenames: false
    stages:
    - pre-push
  - id: check-hardcoded-credentials
    name: Check for hard-coded credentials
    entry: uv run pytest tests/deployment/test_helm_configuration.py::test_no_hardcoded_credentials_in_configmap -v --tb=short
    language: system
    files: ^deployments/base/configmap\.yaml$
    pass_filenames: false
    stages:
    - pre-push
  - id: validate-redis-password-required
    name: Ensure Redis password is mandatory
    entry: uv run pytest tests/deployment/test_helm_configuration.py::test_redis_password_not_optional -v --tb=short
    language: system
    files: ^deployments/base/redis-session-deployment\.yaml$
    pass_filenames: false
    stages:
    - pre-push
  - id: trivy-scan-k8s-manifests
    name: Trivy Security Scan for Kubernetes Manifests
    entry: "bash -c 'if ! command -v trivy &> /dev/null; then\n  echo \"\u26A0\uFE0F  Trivy not installed - skipping security\
      \ scan. Install: brew install trivy\";\n  exit 0;\nfi;\ntrivy config deployments --severity CRITICAL,HIGH --skip-dirs\
      \ \"**/charts\" --exit-code\ 1 --quiet ||\n(echo \"\u274C Security vulnerabilities found. Run: trivy config deployments\
      \ --severity CRITICAL,HIGH --skip-dirs charts\"; exit 1)'\n"
    language: system
    files: ^deployments/.*\.(yaml|yml)$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: 'Scans Kubernetes manifests for security misconfigurations using Trivy.


      Detects issues like:

      - readOnlyRootFilesystem: false (AVD-KSV-0014)

      - Missing security contexts

      - Privileged containers

      - Missing resource limits


      Install Trivy:

      - macOS: brew install trivy

      - Linux: See https://aquasecurity.github.io/trivy/latest/getting-started/installation/


      If Trivy is not installed, the hook will skip scanning (not fail).

      CI/CD environments should have Trivy installed for mandatory scans.


      Regression prevention for: Deploy to GKE Staging failure (Run #19309378657)

      '
  - id: check-mermaid-styling
    name: Check Mermaid Diagrams for ColorBrewer2 Set3 Styling
    entry: uv run python scripts/validation/check_mermaid_styling.py
    language: system
    files: ^docs/.*\.mdx$
    pass_filenames: true
    always_run: false
    stages:
    - manual
    description: 'Validates Mermaid diagram styling consistency (optional quality check).

      Run manually: SKIP= pre-commit run check-mermaid-styling --all-files

      Moved to manual stage 2025-11-16 (CI/CD optimization - Phase 1)

      '
  - id: prevent-local-config-commits
    name: Prevent Local Config Files from Being Committed
    entry: uv run pytest tests/meta/validation/test_gitignore_validation.py -v
    language: system
    pass_filenames: false
    always_run: true
    description: 'Prevents accidental commits of local configuration files like:

      - .claude/settings.local.json

      - *.local.json

      - .env.local

      Per TDD best practices, this ensures local configs never reach version control.

      '
  - id: validate-github-workflows-comprehensive
    name: Validate GitHub Workflows (Comprehensive)
    entry: uv run python scripts/validation/validate_github_workflows.py
    language: system
    files: ^(\.github/workflows/.*\.ya?ml|\.github/actions/.*/action\.yml)$
    pass_filenames: false
    stages:
      - pre-push
    description: "Comprehensive GitHub Actions workflow validation (consolidated):\n\n\
      Replaces: validate-github-workflows + validate-github-action-versions\n\n\
      Validations performed:\n\
      1. YAML syntax validation\n\
      2. Context usage (github.event.* against enabled triggers)\n\
      3. Action version validation (published tags, no invalid versions)\n\
      4. Permissions validation (workflows creating issues have 'issues: write')\n\n\
      Prevents:\n\
      - Using github.event.pull_request.* without pull_request trigger\n\
      - Invalid action versions (e.g., astral-sh/setup-uv@v7.1.1)\n\
      - Missing permissions for issue creation\n\n\
      Consolidation: Part of validator consolidation effort (2025-11-17)\n\
      See: docs-internal/DUPLICATE_VALIDATOR_ANALYSIS_2025-11-16.md\n\
      Test: tests/meta/test_consolidated_workflow_validator.py\n"
  - id: validate-gke-autopilot-compliance
    name: Validate GKE Autopilot Resource Compliance
    entry: uv run python scripts/validation/validate_gke_autopilot_compliance.py
    language: system
    files: ^deployments/.*\.ya?ml$
    pass_filenames: false
    stages:
    - pre-push
    description: "Validates that Kubernetes manifests comply with GKE Autopilot constraints:\n- CPU limit/request ratio \u2264\
      \ 4.0x\n- Memory limit/request ratio \u2264 4.0x\n- No environment variables with both 'value' and 'valueFrom'\n\nPrevents\
      \ deployment failures and pod creation errors on GKE Autopilot.\n\nRegression prevention for:\n- Run #19310965220: GKE\
      \ Autopilot validation failure\n- Run #19310965206: Deployment validation failure\n- Run #19310965249: Deploy to GKE\
      \ Staging failure\n"
  - id: validate-dependency-injection
    name: Validate Dependency Injection Configuration
    entry: python3 .githooks/pre-commit-dependency-validation
    language: system
    pass_filenames: false
    files: ^(src/mcp_server_langgraph/(core/dependencies|core/cache|auth/service_principal)\.py|tests/integration/test_(dependencies_wiring|cache_redis_config)\.py)$
    always_run: false
    stages:
    - pre-push
    description: 'Validates that dependency injection is properly configured to prevent:

      1. Missing Keycloak admin credentials

      2. OpenFGA client created with None store_id

      3. Service principal crashes when OpenFGA disabled

      4. L2 cache ignoring secure Redis settings

      5. Missing critical test coverage

      See ADR-0042 for details on these critical production bugs.

      '
  - id: validate-test-fixtures
    name: Validate Test Fixtures for Common Issues
    entry: uv run python scripts/validation/validate_test_fixtures.py
    language: system
    files: ^tests/.*test_.*\.py$
    pass_filenames: true
    always_run: false
    stages:
    - pre-push
    description: 'Validates test fixtures to prevent common failures:

      1. Missing FastAPI dependency overrides (causes 401 errors)

      2. Invalid Ollama model names (missing ollama/ prefix)

      3. Circuit breaker tests without proper isolation markers

      Errors block commits, warnings are informational only.

      See: tests/API_TESTING.md for detailed guidance.

      '
  - id: shellcheck
    name: Shellcheck - Bash Script Linting
    entry: shellcheck
    language: system
    types:
    - shell
    args:
    - -x
    - --severity=warning
    description: 'Validates bash scripts for common errors and best practices.

      Prevents runtime errors like undefined functions, integer comparison issues,

      and improper variable handling. See staging-smoke-tests.sh fixes as examples.

      '
  - id: validate-pytest-markers
    name: Validate Pytest Markers & Placement
    entry: uv run python scripts/validation/validate_pytest_markers.py
    language: system
    pass_filenames: false
    files: ^(tests/.*\.py|pyproject\.toml)$
    description: 'Validates pytest marker registration and pytestmark placement.


      1. Checks all pytest.mark.* decorators are registered in pyproject.toml

         (prevents "PytestUnknownMarkWarning" errors)


      2. Validates pytestmark appears AFTER all imports (prevents SyntaxError)


      CODEX FINDING REGRESSION PREVENTION (2025-11-20):

      - Prevents pytestmark inside import blocks causing SyntaxError

      - Uses AST parsing to detect misplaced module-level pytestmark

      - See: docs-internal/PYTESTMARK_GUIDELINES.md for placement rules

      '
  - id: validate-test-collection
    name: Validate Pytest Can Collect All Tests
    entry: bash -c 'uv run pytest --collect-only tests/ -q --quiet 2>&1 | grep -q "tests collected" || exit 1'
    language: system
    files: ^tests/.*\.py$
    pass_filenames: false
    description: 'Validates that pytest can successfully collect all test files.

      Prevents test collection failures from:

      - SyntaxError in test files

      - ImportError from missing modules

      - Invalid pytest markers


      Runs test collection without executing tests (fast check).


      CODEX FINDING REGRESSION PREVENTION (2025-11-20):

      - Prevents pytestmark inside imports causing collection failure

      - Prevents ImportError from module relocations

      - Ensures test suite integrity before commit


      Regression prevention for commit a57fcc95.

      '
  - id: validate-fixture-organization
    name: Validate Pytest Fixture Organization (No Duplicates)
    entry: uv run pytest tests/meta/test_fixture_organization.py -v --tb=short
    language: system
    pass_filenames: false
    files: ^tests/.*\.py$
    always_run: false
    stages:
    - pre-push
    description: 'Prevents duplicate autouse fixtures across test files.


      Issue fixed: 25 duplicate init_test_observability fixtures were found

      across test modules, causing duplicate initialization overhead.


      Best practice: Module/session-scoped autouse fixtures should be defined

      in tests/conftest.py to avoid initialization conflicts and improve test

      performance. This hook ensures fixtures remain consolidated.


      See: docs-internal/CODEX_FINDINGS_VALIDATION_REPORT.md

      '



  - id: regression-prevention-tests
    name: CI/CD Regression Prevention Tests
    entry: uv run pytest tests/meta/test_regression_prevention.py -v --tb=short
    language: system
    pass_filenames: false
    files: ^(tests/.*\.py|\.github/workflows/.*\.ya?ml)$
    always_run: false
    stages:
    - pre-push
    description: 'Prevents regression of critical CI/CD and testing infrastructure issues.


      Issues prevented:

      1. Missing pytest fixture decorators (Issue #6 - Quality Tests)

      2. Invalid class-scoped fixtures (Issue #6)

      3. Settings singleton not reloaded after monkeypatch (Issue #7 - Smoke Tests)

      4. Use of archived tools like kubeval (Issue #8 - Deployment Workflows)

      5. Outdated GitHub Action versions

      6. Invalid workflow YAML syntax


      Following TDD principles, these tests FAIL if regressions are introduced.

      See: docs/CRITICAL_FIXES_SUMMARY.md for full details.

      '
  - id: detect-dead-test-code
    name: Detect Dead Code in Test Fixtures (Codex P0)
    entry: uv run python scripts/detect_dead_test_code.py tests/
    language: system
    files: ^tests/.*test_.*\.py$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: "Detects dead code after return statements in pytest fixtures.\nThis code never executes and represents lost\
      \ test coverage.\n\nPattern detected:\n    @pytest.fixture\n    def my_fixture():\n        return value\n\n        #\
      \ Dead code - never executes!\n        assert something\n\nFix: Extract dead code into separate test function with test_\
      \ prefix.\n\nRegression prevention for Codex P0 finding:\n- test_code_generator.py:33-64 had 18 lines of dead code\n\
      - test_server.py:75-99 had 9 lines of dead code\nThese tests never executed, losing critical test coverage.\n\nMeta-test:\
      \ tests/meta/test_codex_regression_prevention.py::TestDeadCodeInFixtures\n"


  - id: check-e2e-completion
    name: Check E2E Test Implementation Progress
    entry: uv run python scripts/validation/check_e2e_completion.py --min-percent 25
    language: system
    pass_filenames: false
    files: ^tests/e2e/test_full_user_journey\.py$
    always_run: false
    stages:
    - manual
    description: "Tracks E2E test implementation progress to prevent regression of\nCodex Finding #1: \"E2E suite effectively\
      \ inert\u201449 scenarios are xfail placeholders\"\n\nMonitors:\n- Total E2E tests vs implemented tests\n- Completion\
      \ percentage (current: 35%, target: 80%)\n- Prevents decrease in implementation progress\n\nMin: 25% | Current: 35%\
      \ | Target: 80%\n"
  - id: check-test-sleep-budget
    name: Monitor Test Wall-Clock Sleep Time Budget
    entry: uv run python scripts/validation/check_test_sleep_budget.py --max-seconds 60 --warn-seconds 45
    language: system
    pass_filenames: false
    files: ^tests/.*test_.*\.py$
    always_run: false
    stages:
    - manual
    description: 'Monitors total wall-clock sleep time across test suite.


      Related to Codex Finding #5: Wall-clock sleeps make suite slow and flaky


      Current status: 36s in active tests (acceptable)

      Budget: 60s max, 45s warning threshold


      Note: Many sleep calls are in skipped tests or testing actual sleep functionality.

      '
  - id: validate-meta-test-quality
    name: Run Meta-Tests for Test Quality Validation
    entry: uv run pytest tests/meta/test_property_test_quality.py tests/meta/test_context_manager_quality.py tests/meta/test_kubectl_safety.py
      -v --tb=short
    language: system
    pass_filenames: false
    files: ^tests/.*test_.*\.py$
    always_run: false
    stages:
    - pre-push
    description: 'Runs meta-tests that validate test suite quality (Codex Findings Prevention):


      1. Property Test Quality: Ensures @given tests have assertions

      2. Context Manager Quality: Validates __exit__ assertions

      3. Kubectl Safety: Enforces --dry-run or safety guards


      Prevents regression of Codex Findings #3, #4, #9

      '
  # NOTE: validate-github-action-versions hook REMOVED (2025-11-17)
  # Consolidated into validate-github-workflows-comprehensive (above)
  # See: docs-internal/DUPLICATE_VALIDATOR_ANALYSIS_2025-11-16.md
  - id: validate-kustomize-builds
    name: Validate Kustomize Overlays Build Successfully
    entry: uv run pytest tests/deployment/test_kustomize_builds.py -v --tb=short
    language: system
    pass_filenames: false
    files: ^deployments/.*\.(yaml|yml)$
    always_run: false
    stages:
    - pre-push
    description: 'Validates that all Kustomize overlays build successfully without errors.


      Prevents critical deployment blockers identified in Codex audit:

      1. NetworkPolicy port mismatches (3307 vs 5432 for PostgreSQL)

      2. Redis StatefulSet deletion using wrong kind (Deployment vs StatefulSet)

      3. Service Account namespace mismatches

      4. Cloud SQL Proxy missing health check flags

      5. PodDisruptionBudget namespace mismatches

      6. Istio config inline comments in host strings

      7. Orphaned ResourceQuotas for undefined namespaces


      TDD-based validation suite ensures these issues never recur.

      See: docs-internal/CLOUD_SQL_CONNECTION_STRATEGY.md

      '
  - id: validate-network-policies
    name: Validate NetworkPolicy Port Configurations
    entry: uv run pytest tests/deployment/test_network_policies.py -v --tb=short
    language: system
    pass_filenames: false
    files: ^deployments/.*network-policy\.(yaml|yml)$
    always_run: false
    stages:
    - pre-push
    description: 'Validates NetworkPolicy configurations use correct database ports.


      Critical validations:

      - PostgreSQL uses port 5432 (NOT 3307 for MySQL)

      - Redis uses port 6379

      - Cloud SQL connections properly configured

      - No overly permissive egress rules


      Prevents production connectivity failures from port misconfigurations.

      '
  - id: validate-service-accounts
    name: Validate Service Account Separation and RBAC
    entry: uv run pytest tests/deployment/test_service_accounts.py -v --tb=short
    language: system
    pass_filenames: false
    files: ^deployments/.*serviceaccount.*\.(yaml|yml)$
    always_run: false
    stages:
    - pre-push
    description: 'Validates Service Account configuration follows least-privilege principles.


      Checks:

      - Components use separate ServiceAccounts (postgres, redis, keycloak, etc.)

      - Workload Identity annotations are correctly formatted

      - RoleBindings reference existing ServiceAccounts

      - No overly permissive RBAC rules (wildcards)


      Ensures security best practices and prevents privilege escalation.

      '

  - id: check-helm-placeholders
    name: Check Helm values for unresolved placeholders
    entry: 'bash -c ''if grep -r "YOUR_.*_PROJECT_ID\|@PROJECT_ID\." deployments/helm/values-*.yaml | grep -v "^#" | grep
      -v "TODO" | grep -v "# For" | grep -v ".local.yaml"; then echo "ERROR: Found unresolved PROJECT_ID placeholders in Helm
      values files."; echo "Use either @DOLLAR{GCP_PROJECT_ID} or actual project IDs."; echo "For deployment-specific values,
      create values-*.local.yaml files."; exit 1; else echo "No dangerous placeholders found"; fi''

      '
    language: system
    pass_filenames: false
    files: ^deployments/helm/values-.*\.(yaml|yml)$
    always_run: false
    stages:
    - pre-push
    description: 'Prevents committing unresolved PROJECT_ID placeholders in Helm values.


      Dangerous patterns blocked:

      - YOUR_STAGING_PROJECT_ID

      - YOUR_GCP_PROJECT_ID

      - @PROJECT_ID.iam.gserviceaccount.com (not using variable substitution)


      Safe patterns allowed:

      - @${GCP_PROJECT_ID}.iam.gserviceaccount.com (variable substitution)

      - vishnu-sandbox-20250310 (actual project ID)

      - Commented placeholders with TODO or "# For" documentation


      Best practice: Create values-*.local.yaml for deployment-specific configs.

      Add *.local.yaml to .gitignore to prevent committing secrets.


      Prevents accidental deployment failures and security issues.

      Regression prevention for Codex findings: values-staging.yaml:108, values-production.yaml:139

      '
  - id: validate-docker-compose-health-checks
    name: Validate Docker Compose health check configurations
    entry: uv run pytest tests/security/test_deployment_security_regression.py::TestQdrantHealthCheckSecurity::test_qdrant_uses_tcp_health_check
      -v --tb=short
    language: system
    files: ^.*docker-compose.*\.(yaml|yml)$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: 'Validates Docker Compose health check configurations (TDD).


      Critical checks:

      - Qdrant services MUST use grpc_health_probe (not wget/curl)

      - Prevents using commands not available in container images

      - Validates health check syntax and structure


      Background:

      Qdrant v1.15+ removed wget/curl for security (GitHub issue #3491).

      This hook prevents health checks from failing due to missing commands.


      Regression prevention for Codex finding: docker-compose.test.yml:227-232

      '
  - id: validate-keycloak-config
    name: Validate Keycloak service configuration in docker-compose.test.yml
    entry: uv run python scripts/validation/validate_keycloak_config.py docker/docker-compose.test.yml
    language: system
    files: ^docker/docker-compose\.test\.yml$
    pass_filenames: false
    always_run: false
    description: 'Validates Keycloak service configuration in docker-compose.test.yml (TDD).


      Critical checks:

      - keycloak-test service exists and is uncommented

      - Health check configuration is present

      - Required environment variables are configured (KEYCLOAK_ADMIN, KC_DB, KC_DB_URL, KC_HEALTH_ENABLED)

      - start_period is adequate (60s minimum for Keycloak initialization)


      Prevents regression of Codex Finding #2: Keycloak service unavailable

      causing TestStandardUserJourney::test_01_login failures.


      Trade-off: +60s startup time vs. comprehensive E2E auth testing coverage.


      See: ADR-0053, tests/meta/test_precommit_keycloak_validation.py

      '
  - id: validate-docker-image-contents
    name: Validate Docker image contents in Dockerfile (final-test stage)
    entry: uv run python scripts/validation/validate_docker_image_contents.py docker/Dockerfile
    language: system
    files: ^docker/Dockerfile$
    pass_filenames: false
    always_run: false
    description: 'Validates Docker image contents in Dockerfile (TDD).


      Required COPY commands in final-test stage:

      - src/ (application source code)


      - pyproject.toml (project configuration)


      Excluded (meta-tests run on host, not in Docker):

      - scripts/ (meta-test validation scripts)

      - deployments/ (Kubernetes manifests, Helm charts)


      Prevents regression of Codex Findings #4 & #5:

      - ModuleNotFoundError for ''scripts'' module

      - FileNotFoundError for /app/deployments


      Design rationale:

      - Integration tests run IN Docker: need src/, tests/, pyproject.toml

      - Meta-tests run ON host: need full repo (scripts/, deployments/)

      - Separation prevents Docker image bloat


      See: ADR-0053, tests/meta/test_precommit_docker_image_validation.py

      '
  - id: helm-lint
    name: Helm Lint Validation
    entry: >
      bash -c 'if command -v helm >/dev/null 2>&1; then
      helm lint deployments/helm/mcp-server-langgraph;
      else echo "Skipping: helm not installed (install from https://helm.sh/docs/intro/install/)";
      exit 0; fi'
    language: system
    files: ^deployments/helm/.*\.(yaml|yml)$
    pass_filenames: false
    stages:
    - pre-push
    description: 'Validates Helm chart passes lint checks.


      CLI Availability Guard: Gracefully skips if helm is not installed.


      Prevents:

      - Hyphenated key parsing errors (e.g., .Values.kube-prometheus-stack.enabled)

      - Template syntax errors

      - Invalid Kubernetes resource schemas

      - Missing required values


      Fix: Use index .Values "kube-prometheus-stack" "enabled" for hyphenated keys


      Regression prevention for Codex Finding #1 (P0 Blocker)

      '
  - id: validate-cloud-overlays
    name: Validate Cloud Overlays Build Successfully
    entry: >
      bash -c 'if command -v kubectl >/dev/null 2>&1; then
      kubectl kustomize deployments/kubernetes/overlays/aws &&
      kubectl kustomize deployments/kubernetes/overlays/gcp &&
      kubectl kustomize deployments/kubernetes/overlays/azure;
      else echo "Skipping: kubectl not installed (install from https://kubernetes.io/docs/tasks/tools/)";
      exit 0; fi'
    language: system
    files: ^deployments/kubernetes/overlays/(aws|gcp|azure)/.*\.(yaml|yml)$
    pass_filenames: false
    stages:
    - pre-push
    description: 'Validates cloud-specific Kustomize overlays build without errors.


      CLI Availability Guard: Gracefully skips if kubectl is not installed.


      Prevents:

      - ConfigMap generator issues with behavior: replace

      - Missing ConfigMaps or resources

      - YAML syntax errors in cloud configs


      Regression prevention for Codex Finding #2 (P0 Blocker)

      '
  - id: validate-no-placeholders
    name: Check for Placeholders in Production Overlays
    entry: >
      bash -c 'if command -v kubectl >/dev/null 2>&1; then
      kubectl kustomize deployments/overlays/production-gke | grep -E "PLACEHOLDER_|PRODUCTION_DOMAIN" && exit 1 || exit 0;
      else echo "Skipping: kubectl not installed (install from https://kubernetes.io/docs/tasks/tools/)";
      exit 0; fi'
    language: system
    files: ^deployments/overlays/production-gke/.*\.(yaml|yml)$
    pass_filenames: false
    stages:
    - pre-push
    description: 'Validates production overlays don''t contain unresolved placeholders.


      Prevents:

      - PLACEHOLDER_GCP_PROJECT_ID in service accounts

      - PLACEHOLDER_SET_VIA_ENV in environment variables

      - PRODUCTION_DOMAIN in configuration


      Ensures production deployments have valid, runtime-ready configuration.


      Regression prevention for Codex Finding #3 (P0 Blocker)

      '


  - id: check-async-mock-configuration
    name: Check AsyncMock Configuration (prevent authorization bypass)
    entry: uv run python scripts/validation/check_async_mock_configuration.py
    language: system
    files: ^tests/.*test_.*\.py$
    pass_filenames: true
    always_run: false
    stages:
    - manual
    description: 'Validates that all AsyncMock instances have explicit return_value or side_effect configuration.


      SECURITY CRITICAL: Unconfigured AsyncMock returns truthy values, causing authorization

      checks to incorrectly pass. This was the root cause of SCIM security bug (commit abb04a6a).


      Issue prevented:

      - Unconfigured AsyncMock() returns <AsyncMock> (truthy) instead of False

      - Authorization checks like "if await openfga.check_permission()" evaluate to True

      - Security controls bypassed, granting access when it should be denied

      - Tests pass incorrectly, hiding authorization bugs


      Required pattern:

      - Authorization denial: mock.check_permission.return_value = False

      - Authorization grant: mock.check_permission.return_value = True

      - Void functions: mock.write_tuples.return_value = None

      - Exceptions: mock.method.side_effect = Exception("error")


      Current status: 65 unconfigured high-risk instances (authorization/permission checks)

      Future: Will move to pre-push stage after fixing all violations


      See: tests/ASYNC_MOCK_GUIDELINES.md for complete guide

      Meta-test: tests/meta/test_async_mock_configuration.py validates this

      '
  - id: check-asyncmock-instantiation
    name: Check AsyncMock Instantiation (prevent class assignment)
    entry: uv run python scripts/validation/check_asyncmock_usage.py
    language: system
    files: ^tests/.*test_.*\.py$
    exclude: ^tests/meta/test_asyncmock_validation\.py$
    pass_filenames: true
    always_run: false
    description: 'Prevents AsyncMock class assignment instead of instance creation.

      Issue prevented:

      - Assigning AsyncMock class instead of AsyncMock() instance

      - Causes "object AsyncMock can''t be used in ''await'' expression" errors

      - Often appears with noqa comments documenting intended config but not applied


      Required pattern:

      - CORRECT: mock.method = AsyncMock(return_value=1)

      - CORRECT: mock.method = AsyncMock()

      - WRONG: mock.method = AsyncMock  # noqa: async-mock-config(return_value=1)


      This hook catches the specific bug where developers put the configuration

      in a noqa comment but forget to actually instantiate the AsyncMock.


      See: tests/unit/health/test_database_checks.py (fixed instances)

      Meta-test: tests/meta/test_asyncmock_validation.py validates this

      '
  - id: validate-test-isolation
    name: Validate Test Isolation for Pytest-xdist
    entry: python scripts/validation/validate_test_isolation.py tests/
    language: system
    files: ^tests/.*test_.*\.py$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: 'Validates that test files follow best practices for pytest-xdist parallel execution.


      Prevents regression of bug fixed in commit 079e82e where async dependencies were

      overridden with sync lambda functions, causing intermittent 401 authentication failures.


      Critical validations:

      - Async dependencies MUST be overridden with async functions (not lambdas)

      - FastAPI dependency_overrides.clear() called in fixture teardown

      - Test classes use @pytest.mark.xdist_group marker for related tests

      - teardown_method() includes gc.collect() to prevent memory leaks


      Without these patterns:

      - Tests pass when run with pytest -xvs (single process)

      - Tests fail intermittently with pytest -n auto (parallel workers)

      - Difficult to debug due to race conditions


      See: tests/PYTEST_XDIST_BEST_PRACTICES.md for complete guidance

      Regression tests: tests/regression/test_pytest_xdist_isolation.py

      '
  - id: validate-test-dependencies
    name: Validate Test Import Dependencies
    entry: uv run pytest tests/regression/test_dev_dependencies.py::test_test_imports_have_dev_dependencies -v --tb=short
    language: system
    files: ^(tests/.*\.py|pyproject\.toml)$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: "Prevents CI failures from missing test dependencies by validating that\nevery package imported in tests/\
      \ is available in project dependencies.\n\nThis hook catches the issue that caused 10 CI job failures on 2025-11-12:\n\
      - ModuleNotFoundError: docker.errors in 7 quality test jobs\n- Same error in E2E tests\n- Root cause: docker/kubernetes\
      \ in code-execution extras, not dev extras\n\nValidations:\n- All test imports have corresponding dependencies (main\
      \ or optional)\n- Package vs import name mismatches (PyJWT\u2192jwt, pyyaml\u2192yaml)\n- Stdlib backports handled (tomli\u2192\
      tomllib)\n- Third-party helpers excluded (bats-core)\n\nQuick fix: Add missing package to appropriate extras in pyproject.toml\n\
      - Test dependencies \u2192 [project.optional-dependencies.dev]\n- Code execution \u2192 [project.optional-dependencies.code-execution]\n\
      - CLI tools \u2192 [project.optional-dependencies.cli]\n\nRegression prevention for commit 7b51437 (2025-11-12)\nSee:\
      \ tests/regression/test_dev_dependencies.py for implementation\n"
  - id: validate-workflow-test-deps
    name: Validate Workflow Test Dependencies
    entry: uv run python scripts/validation/validate_workflow_test_deps.py
    language: system
    files: ^\.github/workflows/.*\.ya?ml$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: "Validates that GitHub Actions workflows installing dependencies for tests\ninclude the required 'dev' extras.\n\
      \nPrevents the configuration issue that caused 10 CI failures on 2025-11-12:\n- Workflows ran pytest but didn't install\
      \ dev extras\n- Missing docker/kubernetes packages broke test imports\n- Root cause: setup-python-deps used without\
      \ 'dev' in extras parameter\n\nChecks:\n- Workflows using setup-python-deps + pytest must install 'dev' extras\n- Skips\
      \ workflows using uv run (auto-installs) or other methods\n- Only validates workflows that explicitly use setup-python-deps\
      \ action\n\nQuick fix: Add or update 'extras' parameter in setup-python-deps step:\n  - uses: ./.github/actions/setup-python-deps\n\
      \    with:\n      extras: 'dev'  # or 'dev builder' for multiple\n\nRegression prevention for commit 7b51437 (2025-11-12)\n\
      See: scripts/validation/validate_workflow_test_deps.py for implementation\n"
  - id: validate-fixture-scopes
    name: Validate Pytest Fixture Scopes
    entry: uv run pytest tests/meta/test_fixture_validation.py::TestFixtureDecorators::test_fixture_scope_dependencies_are_compatible -v --tb=short
    language: system
    files: ^tests/conftest\.py$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: "Prevents pytest ScopeMismatch errors by validating fixture scope compatibility.\n\nValidates that fixtures\
      \ with wider scopes don't depend on fixtures with narrower scopes.\n\nScope hierarchy (widest to narrowest):\n- session:\
      \ Fixture runs once per test session\n- module: Fixture runs once per module\n- class: Fixture runs once per class\n\
      - function: Fixture runs once per test function (default)\n\nRULE: A fixture can only depend on fixtures with equal\
      \ or wider scope.\n\nCommon violations:\n- session-scoped fixture depending on function-scoped fixture \u274C\n- module-scoped\
      \ fixture depending on function-scoped fixture \u274C\n- function-scoped fixture depending on session-scoped fixture\
      \ \u2705 (OK)\n\nThis hook catches the issue that caused integration test failures with pytest-xdist:\n- ScopeMismatch:\
      \ session-scoped fixtures (postgres_connection_real, redis_client_real,\n  openfga_client_real) depended on function-scoped\
      \ fixture (integration_test_env)\n- Tests passed individually but failed in parallel execution\n- Root cause: integration_test_env\
      \ missing scope=\"session\" parameter\n\nQuick fix: Add or update scope parameter in fixture decorator:\n  @pytest.fixture(scope=\"\
      session\")\n  def my_fixture(dependency_fixture):\n      ...\n\nPrevention: This hook runs automatically on conftest.py\
      \ changes\nSee: tests/meta/test_fixture_validation.py for implementation\nRegression test added: 2025-11-13\n"
  - id: validate-minimum-coverage
    name: "Validate Minimum Test Coverage Threshold (\u2265 64%)"
    entry: uv run pytest tests/meta/test_coverage_enforcement.py -v --tb=short
    language: system
    pass_filenames: false
    files: ^(src/.*\.py|tests/.*\.py|pyproject\.toml)$
    always_run: false
    stages:
    - manual
    description: "Prevents coverage regression by enforcing minimum 64% threshold.\n\nValidates that:\n- Overall test coverage\
      \ \u2265 64% (CI threshold)\n- Coverage doesn't drop from Phase 1 baseline (65.78%)\n- All new code is adequately tested\n\
      \nCoverage targets:\n- Minimum: 64% (MUST NOT DROP BELOW)\n- Current: 65.78% (after Phase 1 improvements)\n- Target:\
      \ 80% (Codex recommendation)\n- Excellent: 90%+\n\nPhase 1 achievements:\n- prometheus_client.py: 44% \u2192 87% (+43%)\n\
      - budget_monitor.py: 47% \u2192 81% (+34%)\n- cost_api.py: 55% \u2192 91% (+36%)\n\nTo diagnose coverage issues:\n1.\
      \ Run: pytest --cov --cov-report=html\n2. Open: htmlcov/index.html\n3. Identify modules with low coverage\n4. Write\
      \ tests for uncovered code paths\n\nRegression prevention for Phase 1 (2025-11-15)\nSee: tests/meta/test_coverage_enforcement.py\
      \ for implementation\n"
  - id: validate-test-suite-performance
    name: Validate Test Suite Performance (< 120s)
    entry: uv run pytest tests/meta/test_performance_regression_suite.py -v --tb=short
    language: system
    pass_filenames: false
    files: ^tests/.*\.py$
    always_run: false
    stages:
    - manual
    description: 'Prevents test suite performance regression by enforcing < 120s duration.


      Validates that:

      - Unit test suite completes in < 120 seconds

      - No performance regression from baseline

      - Test parallelization remains effective


      Performance targets:

      - Current: 220s (TOO SLOW)

      - Target: < 120s (2 minutes)

      - Ideal: < 60s (1 minute)


      Known slow tests (tracked for optimization):

      - OpenFGA circuit breaker: 45s each (retry logic optimization needed)

      - Agent tests: 14-29s each (LangGraph mocking needed)

      - Retry timing: 14s (freezegun needed)


      To diagnose slow tests:

      1. Run: pytest -m unit --durations=20

      2. Identify tests > 5s

      3. Apply optimization strategies (see test documentation)


      Manual stage: Run with SKIP= pre-commit run validate-test-suite-performance

      See: tests/meta/test_performance_regression_suite.py for implementation

      Regression prevention for Phase 2 (2025-11-15)

      '
  - id: detect-slow-unit-tests
    name: Detect Individual Slow Tests (> 10s)
    entry: uv run pytest tests/meta/test_slow_test_detection.py -v --tb=short
    language: system
    pass_filenames: false
    files: ^tests/.*\.py$
    always_run: false
    stages:
    - manual
    description: 'Detects individual unit tests taking > 10 seconds.


      Performance guidelines:

      - Unit tests: < 1s (IDEAL)

      - Integration tests: < 5s (ACCEPTABLE)

      - E2E tests: < 30s (ACCEPTABLE)

      - Unit tests > 10s: NEEDS OPTIMIZATION


      Known slow tests (documented for future optimization):

      - OpenFGA circuit breaker tests: 45s (retry logic)

      - Agent tests: 14-29s (LangGraph execution)

      - Retry timing tests: 14s (real time delays)


      This hook prevents NEW slow tests from being introduced.

      Known slow tests are tracked and will be optimized separately.


      Optimization strategies:

      - Circuit breaker: Use fast_resilience_config fixture

      - Retry tests: Use freezegun to mock time

      - Agent tests: Mock LangGraph components

      - I/O operations: Use mocks instead of real I/O


      Manual stage: Run with SKIP= pre-commit run detect-slow-unit-tests

      See: tests/meta/test_slow_test_detection.py for implementation

      Regression prevention for Phase 2 (2025-11-15)

      '
  - id: validate-pytest-config
    name: Validate Pytest Configuration
    entry: uv run python scripts/validation/validate_pytest_config.py
    language: system
    files: ^pyproject\.toml$
    pass_filenames: false
    always_run: false
    stages:
    - pre-push
    description: "Validates pytest configuration in pyproject.toml."

  - id: check-test-memory-safety
    name: Check Test Memory Safety
    entry: uv run python scripts/validation/check_test_memory_safety.py
    language: system
    files: ^tests/.*\.py$
    pass_filenames: true
    always_run: false
    stages:
    - pre-push
    description: "Validates memory safety in tests (gc.collect in teardown)."

  - id: check-async-mock-usage
    name: Check Async Mock Usage
    entry: uv run python scripts/validation/check_async_mock_usage.py
    language: system
    files: ^tests/.*\.py$
    pass_filenames: true
    always_run: false
    stages:
    - pre-push
    description: "Validates that AsyncMock is used correctly."

  - id: validate-test-ids
    name: Validate Test IDs (xdist isolation)
    entry: uv run python scripts/validation/validate_test_ids.py
    language: system
    files: ^tests/.*\.py$
    pass_filenames: true
    always_run: false
    stages:
    - pre-push
    description: "Validates that integration tests use worker-safe ID helpers."

- repo: https://github.com/antonbabenko/pre-commit-terraform
  rev: v1.96.2
  hooks:
  - id: terraform_fmt
    name: Terraform format
    description: Format Terraform files
    files: \.tf$
  - id: terraform_validate
    name: Terraform validate
    description: Validate Terraform syntax
    files: \.tf$
    args:
    - --hook-config=--retry-once-with-cleanup=true
    - --tf-init-args=-backend=false
    stages:
    - pre-push
