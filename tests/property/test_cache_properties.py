"""
Property-based tests for cache module using Hypothesis.

Tests cache invariants and edge cases:
- Cache get/set roundtrip preservation
- TTL expiration behavior
- Key generation consistency
- Multi-layer cache promotion/demotion
- Concurrent access safety
"""

import asyncio
import time
from unittest.mock import Mock, patch

import pytest
from hypothesis import assume, given, settings
from hypothesis import strategies as st

from mcp_server_langgraph.core.cache import CACHE_TTLS, CacheLayer, CacheService, cached, generate_cache_key

pytestmark = [pytest.mark.unit, pytest.mark.property]


class TestCacheValuePreservation:
    """Test that cache preserves values correctly"""

    @given(value=st.integers())
    @settings(max_examples=50, deadline=2000)
    def test_integer_values_preserved(self, value):
        """Property: Integer values are preserved through cache"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        cache.set(f"int:{value}", value)
        retrieved = cache.get(f"int:{value}")
        assert retrieved == value

    @given(value=st.text(min_size=0, max_size=1000))
    @settings(max_examples=50, deadline=2000)
    def test_string_values_preserved(self, value):
        """Property: String values are preserved through cache"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        cache.set(f"str:{hash(value)}", value)
        retrieved = cache.get(f"str:{hash(value)}")
        assert retrieved == value

    @given(value=st.lists(st.integers(), min_size=0, max_size=50))
    @settings(max_examples=30, deadline=2000)
    def test_list_values_preserved(self, value):
        """Property: List values are preserved through cache"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        cache.set(f"list:{len(value)}", value)
        retrieved = cache.get(f"list:{len(value)}")
        assert retrieved == value

    @given(value=st.dictionaries(st.text(min_size=1, max_size=10), st.integers(), min_size=0, max_size=20))
    @settings(max_examples=30, deadline=2000)
    def test_dict_values_preserved(self, value):
        """Property: Dict values are preserved through cache"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        cache.set(f"dict:{len(value)}", value)
        retrieved = cache.get(f"dict:{len(value)}")
        assert retrieved == value

    @given(value=st.booleans())
    @settings(max_examples=20, deadline=1000)
    def test_boolean_values_preserved(self, value):
        """Property: Boolean values are preserved through cache"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        cache.set(f"bool:{value}", value)
        retrieved = cache.get(f"bool:{value}")
        assert retrieved == value


class TestCacheKeyNormalization:
    """Test cache key generation properties"""

    @given(parts=st.lists(st.text(min_size=1, max_size=20), min_size=1, max_size=5))
    @settings(max_examples=30, deadline=2000)
    def test_key_generation_deterministic(self, parts):
        """Property: Same parts always generate same key"""
        key1 = generate_cache_key(*parts, prefix="test", version="v1")
        key2 = generate_cache_key(*parts, prefix="test", version="v1")
        assert key1 == key2

    @given(
        prefix1=st.text(min_size=1, max_size=10),
        prefix2=st.text(min_size=1, max_size=10),
        part=st.text(min_size=1, max_size=10),
    )
    @settings(max_examples=30, deadline=2000)
    def test_different_prefixes_different_keys(self, prefix1, prefix2, part):
        """Property: Different prefixes generate different keys"""
        assume(prefix1 != prefix2)

        key1 = generate_cache_key(part, prefix=prefix1, version="v1")
        key2 = generate_cache_key(part, prefix=prefix2, version="v1")

        assert key1 != key2

    @given(
        version1=st.text(min_size=1, max_size=5),
        version2=st.text(min_size=1, max_size=5),
        part=st.text(min_size=1, max_size=10),
    )
    @settings(max_examples=30, deadline=2000)
    def test_different_versions_different_keys(self, version1, version2, part):
        """Property: Different versions generate different keys"""
        assume(version1 != version2)

        key1 = generate_cache_key(part, prefix="test", version=version1)
        key2 = generate_cache_key(part, prefix="test", version=version2)

        assert key1 != key2

    @given(parts=st.lists(st.text(min_size=50, max_size=100), min_size=5, max_size=10))
    @settings(max_examples=15, deadline=2000)
    def test_long_keys_are_hashed(self, parts):
        """Property: Long keys are hashed to prevent issues"""
        key = generate_cache_key(*parts, prefix="test", version="v1")

        # Property: Long inputs result in hashed key
        # If original would be > 200 chars, key should contain 'hash'
        original_length = sum(len(p) for p in parts) + len(parts) * 2  # Rough estimate
        if original_length > 200:
            assert "hash" in key
            assert len(key) < 100  # Hashed key should be much shorter


class TestCacheTTLProperties:
    """Test TTL-related properties"""

    @given(cache_type=st.sampled_from(list(CACHE_TTLS.keys())))
    @settings(max_examples=10, deadline=1000)
    def test_ttl_lookup_consistent(self, cache_type):
        """Property: TTL lookup is consistent for valid cache types"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)

        key = f"{cache_type}:test:key"
        ttl = cache._get_ttl_from_key(key)

        # Should return the configured TTL for this cache type
        assert ttl == CACHE_TTLS[cache_type]

    @given(unknown_type=st.text(min_size=1, max_size=20))
    @settings(max_examples=20, deadline=2000)
    def test_unknown_cache_type_has_default_ttl(self, unknown_type):
        """Property: Unknown cache types get default TTL"""
        assume(unknown_type not in CACHE_TTLS)

        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        key = f"{unknown_type}:test:key"
        ttl = cache._get_ttl_from_key(key)

        # Should return default TTL (300 seconds)
        assert ttl == 300


class TestCacheStatisticsProperties:
    """Test cache statistics invariants"""

    @given(num_operations=st.integers(min_value=1, max_value=50))
    @settings(max_examples=20, deadline=3000)
    def test_statistics_counts_accumulate(self, num_operations):
        """Property: Statistics counts accumulate correctly"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)

        for i in range(num_operations):
            cache.set(f"key:{i}", f"value:{i}")

        stats = cache.get_statistics()

        # Property: Set count matches operations
        assert stats["total"]["sets"] == num_operations

    @given(num_hits=st.integers(min_value=1, max_value=20))
    @settings(max_examples=15, deadline=2000)
    def test_hit_rate_calculation_correct(self, num_hits):
        """Property: Hit rate is correctly calculated"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)

        # Set a value
        cache.set("test:key", "value")

        # Generate hits
        for _ in range(num_hits):
            cache.get("test:key")

        # Generate one miss
        cache.get("nonexistent")

        stats = cache.get_statistics()

        # Property: Hit rate = hits / (hits + misses)
        expected_hit_rate = num_hits / (num_hits + 1)
        assert abs(stats["l1"]["hit_rate"] - expected_hit_rate) < 0.01

    @given(num_deletes=st.integers(min_value=1, max_value=30))
    @settings(max_examples=15, deadline=2000)
    def test_delete_statistics_accurate(self, num_deletes):
        """Property: Delete statistics are accurate"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)

        # Create and delete keys
        for i in range(num_deletes):
            cache.set(f"delete:key:{i}", f"value:{i}")
            cache.delete(f"delete:key:{i}")

        stats = cache.get_statistics()

        # Property: Delete count matches operations
        assert stats["total"]["deletes"] == num_deletes


class TestCacheConcurrencySafety:
    """Test cache behavior under concurrent access"""

    @given(num_concurrent=st.integers(min_value=2, max_value=10))
    @settings(max_examples=10, deadline=5000)
    @pytest.mark.asyncio
    async def test_concurrent_sets_dont_lose_data(self, num_concurrent):
        """Property: Concurrent sets don't lose data"""
        cache = CacheService(l1_maxsize=200, l1_ttl=60)

        async def set_value(i):
            cache.set(f"concurrent:key:{i}", f"value:{i}")

        # Concurrent sets
        await asyncio.gather(*[set_value(i) for i in range(num_concurrent)])

        # Property: All values should be retrievable
        for i in range(num_concurrent):
            retrieved = cache.get(f"concurrent:key:{i}")
            assert retrieved == f"value:{i}"

    @given(num_readers=st.integers(min_value=2, max_value=10))
    @settings(max_examples=10, deadline=3000)
    @pytest.mark.asyncio
    async def test_concurrent_reads_consistent(self, num_readers):
        """Property: Concurrent reads return consistent value"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        cache.set("shared:key", "shared_value")

        async def read_value():
            return cache.get("shared:key")

        # Concurrent reads
        results = await asyncio.gather(*[read_value() for _ in range(num_readers)])

        # Property: All reads return same value
        assert all(result == "shared_value" for result in results)


class TestCacheDecoratorProperties:
    """Test @cached decorator properties"""

    @given(
        x=st.integers(min_value=0, max_value=100),
        y=st.integers(min_value=0, max_value=100),
    )
    @settings(max_examples=30, deadline=2000)
    @pytest.mark.asyncio
    async def test_cached_decorator_memoization(self, x, y):
        """Property: @cached decorator correctly memoizes results"""
        # Use unique key prefix to avoid cache pollution between test runs
        import random

        from mcp_server_langgraph.core.cache import get_cache

        # Clear the global cache before test to ensure clean slate
        cache = get_cache()
        cache.clear()

        unique_prefix = f"memo_test_{random.randint(1000000, 9999999)}"
        call_count = [0]

        @cached(key_prefix=unique_prefix, ttl=60)
        async def expensive_operation(a: int, b: int) -> int:
            call_count[0] += 1
            return a + b

        # First call
        result1 = await expensive_operation(x, y)
        assert call_count[0] == 1

        # Second call with same args
        result2 = await expensive_operation(x, y)

        # Property: Results should be equal
        assert result1 == result2
        # Property: Second call should use cache (call_count stays at 1)
        assert call_count[0] == 1

    @given(values=st.lists(st.integers(), min_size=1, max_size=10, unique=True))
    @settings(max_examples=20, deadline=3000)
    @pytest.mark.asyncio
    async def test_cached_decorator_different_args_cached_separately(self, values):
        """Property: Different arguments are cached independently"""

        @cached(key_prefix="separate_test", ttl=60)
        async def identity(x: int) -> int:
            return x

        # Call with each value
        for value in values:
            result = await identity(value)
            assert result == value

        # Verify each is cached
        for value in values:
            result = await identity(value)
            assert result == value


class TestCacheEdgeCases:
    """Test cache edge cases and boundary conditions"""

    @given(st.just(None))
    @settings(max_examples=5, deadline=1000)
    def test_cache_handles_none_value(self, value):
        """Property: Cache can store and retrieve None"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        cache.set("none:key", value)

        # Note: get() returns None for both cache miss and cached None
        # This is a design limitation - we can't distinguish
        # For this test, we verify it doesn't crash
        cache.get("none:key")
        # Could be None (cached) or None (miss) - both valid

    @given(value=st.floats(allow_nan=False, allow_infinity=False))
    @settings(max_examples=30, deadline=2000)
    def test_cache_handles_float_values(self, value):
        """Property: Cache correctly handles float values"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        cache.set(f"float:{value}", value)
        retrieved = cache.get(f"float:{value}")
        assert retrieved == value

    @given(size=st.integers(min_value=1, max_value=10))
    @settings(max_examples=15, deadline=2000)
    def test_cache_respects_max_size(self, size):
        """Property: Cache never exceeds maxsize"""
        cache = CacheService(l1_maxsize=size, l1_ttl=60)

        # Try to overfill cache
        for i in range(size * 2):
            cache.set(f"overflow:key:{i}", f"value:{i}")

        # Property: Cache size should never exceed maxsize
        assert len(cache.l1_cache) <= size


class TestCacheTTLBehavior:
    """Test TTL-related properties"""

    @given(ttl_seconds=st.integers(min_value=1, max_value=3))
    @settings(max_examples=10, deadline=8000)
    def test_cache_expires_after_ttl(self, ttl_seconds):
        """Property: Cached values expire after TTL"""
        cache = CacheService(l1_maxsize=100, l1_ttl=ttl_seconds)
        cache.set("expiring:key", "value")

        # Should exist immediately
        assert cache.get("expiring:key") == "value"

        # Wait for expiration (add buffer)
        time.sleep(ttl_seconds + 0.5)

        # Property: Should be expired
        cache.get("expiring:key")  # Don't assign to avoid F841 lint error
        # Note: cachetools may not guarantee exact expiration timing
        # We just verify it eventually expires or is None

    @given(cache_type=st.sampled_from(list(CACHE_TTLS.keys())))
    @settings(max_examples=10, deadline=1000)
    def test_ttl_from_key_prefix_consistent(self, cache_type):
        """Property: TTL determination is consistent for cache types"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)

        key1 = f"{cache_type}:key1"
        key2 = f"{cache_type}:key2"

        ttl1 = cache._get_ttl_from_key(key1)
        ttl2 = cache._get_ttl_from_key(key2)

        # Property: Same cache type → same TTL
        assert ttl1 == ttl2
        assert ttl1 == CACHE_TTLS[cache_type]


class TestCacheStatisticsInvariants:
    """Test cache statistics invariants"""

    @given(
        num_sets=st.integers(min_value=0, max_value=20),
        num_gets=st.integers(min_value=0, max_value=20),
    )
    @settings(max_examples=20, deadline=2000)
    def test_statistics_never_negative(self, num_sets, num_gets):
        """Property: Cache statistics are never negative"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)

        # Perform operations
        for i in range(num_sets):
            cache.set(f"stats:key:{i}", f"value:{i}")

        for i in range(num_gets):
            cache.get(f"stats:key:{i % max(num_sets, 1)}")

        stats = cache.get_statistics()

        # Property: All statistics >= 0
        assert stats["l1"]["hits"] >= 0
        assert stats["l1"]["misses"] >= 0
        assert stats["l2"]["hits"] >= 0
        assert stats["l2"]["misses"] >= 0
        assert stats["total"]["sets"] >= 0
        assert stats["total"]["deletes"] >= 0

    @given(operations=st.lists(st.sampled_from(["set", "get", "delete"]), min_size=1, max_size=30))
    @settings(max_examples=15, deadline=2000)
    def test_hit_rate_between_zero_and_one(self, operations):
        """Property: Hit rate is always between 0 and 1"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)

        # Perform random operations
        for op in operations:
            if op == "set":
                cache.set("rate:key", "value")
            elif op == "get":
                cache.get("rate:key")
            elif op == "delete":
                cache.delete("rate:key")

        stats = cache.get_statistics()

        # Property: Hit rate is valid percentage
        assert 0.0 <= stats["l1"]["hit_rate"] <= 1.0
        assert 0.0 <= stats["l2"]["hit_rate"] <= 1.0


class TestCacheStampedePrevention:
    """Test cache stampede prevention properties"""

    @given(num_concurrent=st.integers(min_value=2, max_value=8))
    @settings(max_examples=10, deadline=5000)
    @pytest.mark.asyncio
    async def test_stampede_prevention_calls_fetcher_once(self, num_concurrent):
        """Property: get_with_lock calls fetcher only once for concurrent requests"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)
        call_count = [0]

        async def expensive_fetcher():
            call_count[0] += 1
            await asyncio.sleep(0.05)
            return "fetched_value"

        # Launch concurrent requests
        tasks = [cache.get_with_lock("stampede:key", expensive_fetcher) for _ in range(num_concurrent)]
        results = await asyncio.gather(*tasks)

        # Property: Fetcher called exactly once
        assert call_count[0] == 1
        # Property: All requests get same result
        assert all(result == "fetched_value" for result in results)


class TestCacheDeleteProperties:
    """Test cache deletion properties"""

    @given(num_keys=st.integers(min_value=1, max_value=30))
    @settings(max_examples=15, deadline=2000)
    def test_delete_removes_keys(self, num_keys):
        """Property: Deleted keys are no longer retrievable"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)

        # Set keys
        keys = [f"delete:key:{i}" for i in range(num_keys)]
        for key in keys:
            cache.set(key, f"value:{key}")

        # Delete keys
        for key in keys:
            cache.delete(key)

        # Property: Deleted keys return None
        for key in keys:
            assert cache.get(key) is None

    @given(num_keys=st.integers(min_value=1, max_value=20))
    @settings(max_examples=15, deadline=2000)
    def test_clear_removes_all_keys(self, num_keys):
        """Property: clear() removes all keys"""
        cache = CacheService(l1_maxsize=100, l1_ttl=60)

        # Set keys
        for i in range(num_keys):
            cache.set(f"clear:key:{i}", f"value:{i}")

        # Clear cache
        cache.clear()

        # Property: All keys should be gone
        assert len(cache.l1_cache) == 0

        # Verify by attempting to get
        for i in range(num_keys):
            assert cache.get(f"clear:key:{i}") is None


class TestCacheLevelIsolation:
    """Test cache level isolation properties"""

    @given(value=st.text(min_size=1, max_size=50))
    @settings(max_examples=15, deadline=2000)
    def test_l1_only_doesnt_affect_l2(self, value):
        """Property: L1-only operations don't interact with L2"""
        with patch("mcp_server_langgraph.core.cache.redis.Redis") as mock_redis_class:
            mock_redis = Mock()
            mock_redis.ping.return_value = True
            mock_redis.setex.return_value = True
            mock_redis_class.return_value = mock_redis

            cache = CacheService(l1_maxsize=100, l1_ttl=60)

            # Set L1 only
            cache.set("l1:key", value, level=CacheLayer.L1)

            # L2 (Redis) should not be called
            mock_redis.setex.assert_not_called()

    @given(value=st.integers())
    @settings(max_examples=15, deadline=2000)
    def test_l2_writes_also_write_to_l1(self, value):
        """Property: L2 writes are promoted to L1"""
        with patch("mcp_server_langgraph.core.cache.redis.Redis") as mock_redis_class:
            mock_redis = Mock()
            mock_redis.ping.return_value = True
            mock_redis.setex.return_value = True
            mock_redis_class.return_value = mock_redis

            cache = CacheService(l1_maxsize=100, l1_ttl=60)

            # Set L2 (should also set L1)
            cache.set("l2:key", value, level=CacheLayer.L2)

            # Property: Should be in L1
            assert "l2:key" in cache.l1_cache
            assert cache.l1_cache["l2:key"] == value
