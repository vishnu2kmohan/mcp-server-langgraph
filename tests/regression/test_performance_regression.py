"""
Performance Regression Tests

Tracks performance over time and alerts on regressions.
Metrics are compared against baseline_metrics.json.
"""

import json
import statistics
import time
from pathlib import Path
from typing import List
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from langchain_core.messages import HumanMessage


def load_baseline_metrics():
    """Load baseline performance metrics"""
    baseline_path = Path(__file__).parent / "baseline_metrics.json"
    with open(baseline_path) as f:
        data = json.load(f)
    return data["metrics"]


def check_regression(metric_name: str, measured_p95: float, unit: str = "seconds") -> dict:
    """
    Check if measured metric represents a regression

    Returns:
        dict with regression status and details
    """
    baselines = load_baseline_metrics()
    baseline = baselines.get(metric_name)

    if not baseline:
        return {
            "regression": False,
            "reason": "no_baseline",
            "message": f"No baseline found for {metric_name}",
        }

    threshold = baseline["threshold_p95"]
    alert_percent = baseline["alert_threshold_percent"]

    # Convert units if needed
    if baseline["unit"] != unit:
        if baseline["unit"] == "milliseconds" and unit == "seconds":
            measured_p95 = measured_p95 * 1000
        elif baseline["unit"] == "seconds" and unit == "milliseconds":
            measured_p95 = measured_p95 / 1000

    # Check if exceeds threshold
    if measured_p95 > threshold:
        regression_percent = ((measured_p95 - baseline["p95"]) / baseline["p95"]) * 100

        return {
            "regression": True,
            "reason": "threshold_exceeded",
            "message": f"{metric_name} p95 ({measured_p95:.2f}{baseline['unit']}) exceeds threshold ({threshold}{baseline['unit']})",  # noqa: E501
            "baseline_p95": baseline["p95"],
            "measured_p95": measured_p95,
            "regression_percent": regression_percent,
        }

    # Check for significant regression (but under threshold)
    regression_percent = ((measured_p95 - baseline["p95"]) / baseline["p95"]) * 100

    if regression_percent > alert_percent:
        return {
            "regression": True,
            "reason": "significant_slowdown",
            "message": f"{metric_name} p95 increased by {regression_percent:.1f}% (alert at {alert_percent}%)",
            "baseline_p95": baseline["p95"],
            "measured_p95": measured_p95,
            "regression_percent": regression_percent,
        }

    return {
        "regression": False,
        "reason": "within_baseline",
        "baseline_p95": baseline["p95"],
        "measured_p95": measured_p95,
        "improvement_percent": -regression_percent if regression_percent < 0 else 0,
    }


def measure_latency(func, *args, iterations: int = 10, **kwargs) -> dict:
    """
    Measure function latency with percentiles

    Returns:
        dict with p50, p95, p99, max latencies
    """
    latencies = []

    for _ in range(iterations):
        start = time.perf_counter()
        func(*args, **kwargs)
        end = time.perf_counter()
        latencies.append(end - start)

    latencies.sort()

    return {
        "p50": statistics.median(latencies),
        "p95": latencies[int(len(latencies) * 0.95)],
        "p99": latencies[int(len(latencies) * 0.99)] if len(latencies) >= 100 else latencies[-1],
        "max": max(latencies),
        "mean": statistics.mean(latencies),
        "samples": len(latencies),
    }


async def measure_latency_async(func, *args, iterations: int = 10, **kwargs) -> dict:
    """Async version of measure_latency"""
    latencies = []

    for _ in range(iterations):
        start = time.perf_counter()
        await func(*args, **kwargs)
        end = time.perf_counter()
        latencies.append(end - start)

    latencies.sort()

    return {
        "p50": statistics.median(latencies),
        "p95": latencies[int(len(latencies) * 0.95)],
        "p99": latencies[int(len(latencies) * 0.99)] if len(latencies) >= 100 else latencies[-1],
        "max": max(latencies),
        "mean": statistics.mean(latencies),
        "samples": len(latencies),
    }


@pytest.mark.regression
@pytest.mark.benchmark
class TestAgentPerformance:
    """Performance regression tests for agent"""

    @pytest.mark.slow
    @pytest.mark.skip(reason="Requires complex mocking of LangGraph checkpointing - skipping for now")
    def test_agent_response_time_p95(self):
        """Agent response p95 should be under 5 seconds"""
        # This test is skipped because it requires mocking LLM responses in a way that's
        # compatible with LangGraph's checkpoint serialization. The MagicMock objects
        # cannot be serialized by msgpack, causing TypeError.
        # TODO: Implement proper LLM mocking that works with LangGraph checkpointing
        pass

    @pytest.mark.slow
    def test_message_formatting_performance(self):
        """Message formatting should be fast (<2ms p95)"""
        from mcp_server_langgraph.llm.factory import LLMFactory

        factory = LLMFactory(provider="anthropic", model_name="test")

        messages = [HumanMessage(content=f"Message {i}") for i in range(10)]

        def format_messages():
            factory._format_messages(messages)

        stats = measure_latency(format_messages, iterations=100)

        # Convert to milliseconds
        result = check_regression("message_formatting", stats["p95"], unit="seconds")

        assert not result["regression"], result["message"]


@pytest.mark.regression
@pytest.mark.benchmark
class TestAuthPerformance:
    """Performance regression tests for auth/authz"""

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_jwt_validation_performance(self):
        """JWT validation should be very fast (<2ms p95)"""
        from mcp_server_langgraph.auth.middleware import AuthMiddleware

        auth = AuthMiddleware(secret_key="test-secret-key")
        token = auth.create_token("alice")

        async def validate():
            auth.verify_token(token)

        stats = await measure_latency_async(validate, iterations=100)

        result = check_regression("jwt_validation", stats["p95"], unit="seconds")

        assert not result["regression"], result["message"]

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_authorization_check_performance(self):
        """Authorization checks should be fast (<50ms p95)"""
        from mcp_server_langgraph.auth.middleware import AuthMiddleware

        auth = AuthMiddleware()

        # Mock OpenFGA
        with patch.object(auth, "openfga") as mock_openfga:
            mock_openfga.check_permission = AsyncMock(return_value=True)

            async def check_auth():
                await auth.authorize("user:alice", "executor", "tool:chat")

            stats = await measure_latency_async(check_auth, iterations=50)

            result = check_regression("authorization_check", stats["p95"], unit="seconds")

            # Don't fail on regression for mocked tests
            if result["regression"]:
                pytest.skip(f"Authorization regression (mocked): {result['message']}")


@pytest.mark.regression
@pytest.mark.benchmark
class TestLLMPerformance:
    """Performance regression tests for LLM calls"""

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_llm_call_latency(self):
        """LLM calls should complete reasonably fast"""
        from mcp_server_langgraph.llm.factory import LLMFactory

        factory = LLMFactory(provider="anthropic", model_name="test-model")

        messages = [HumanMessage(content="Hello")]

        # Mock LiteLLM
        with patch("mcp_server_langgraph.llm.factory.acompletion") as mock_completion:
            mock_response = MagicMock()
            mock_response.choices = [MagicMock()]
            mock_response.choices[0].message.content = "Hi there!"
            mock_response.usage = None
            mock_completion.return_value = mock_response

            async def call_llm():
                await factory.ainvoke(messages)

            stats = await measure_latency_async(call_llm, iterations=20)

            # This is mocked so should be fast
            # Real LLM calls would be slower
            assert stats["p95"] < 1.0, "Mocked LLM calls should be very fast"


@pytest.mark.regression
class TestRegressionReporting:
    """Tests for regression detection and reporting"""

    def test_regression_detection_threshold_exceeded(self):
        """Regression detected when threshold exceeded"""
        # Simulate p95 exceeding threshold
        result = check_regression("agent_response", 6.0, unit="seconds")  # Threshold is 5.0

        assert result["regression"] is True
        assert result["reason"] == "threshold_exceeded"

    def test_regression_detection_significant_slowdown(self):
        """Regression detected when slowdown exceeds alert percentage"""
        # Use message_formatting: baseline p95=0.5ms, alert=30%, threshold=2.0ms
        # 30% increase = 0.5 * 1.3 = 0.65ms (above alert, well under threshold)
        result = check_regression("message_formatting", 0.7, unit="milliseconds")

        assert result["regression"] is True
        assert result["reason"] == "significant_slowdown"

    def test_no_regression_within_baseline(self):
        """No regression when within baseline"""
        result = check_regression("agent_response", 4.0, unit="seconds")  # Under baseline

        assert result["regression"] is False
        assert result["reason"] == "within_baseline"

    def test_improvement_detected(self):
        """Performance improvements are recorded"""
        result = check_regression("agent_response", 3.0, unit="seconds")  # Faster than baseline

        assert result["regression"] is False
        assert result.get("improvement_percent", 0) > 0


@pytest.mark.regression
class TestBaselineMetrics:
    """Tests for baseline metrics structure"""

    def test_baseline_metrics_loadable(self):
        """Baseline metrics file should be valid JSON"""
        metrics = load_baseline_metrics()
        assert isinstance(metrics, dict)
        assert len(metrics) > 0

    def test_all_metrics_have_required_fields(self):
        """All metrics should have required fields"""
        metrics = load_baseline_metrics()
        required_fields = ["p50", "p95", "p99", "max", "threshold_p95", "alert_threshold_percent", "unit"]

        for metric_name, metric_data in metrics.items():
            for field in required_fields:
                assert field in metric_data, f"{metric_name} missing {field}"

    def test_thresholds_are_reasonable(self):
        """Thresholds should be >= p99"""
        metrics = load_baseline_metrics()

        for metric_name, metric_data in metrics.items():
            assert metric_data["threshold_p95"] >= metric_data["p95"], f"{metric_name} threshold too low"
