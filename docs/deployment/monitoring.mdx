---
title: Monitoring & Observability
description: 'Comprehensive monitoring, metrics, and observability setup'
icon: 'chart-line'
---

### Overview

Production-grade monitoring and observability are essential for maintaining reliability, performance, and security. This guide covers the complete observability stack including metrics, traces, logs, and alerts.

<Info>
The MCP Server uses a dual observability stack: **OpenTelemetry** for infrastructure metrics and traces, plus **LangSmith** for LLM-specific observability.
</Info>

### Observability Stack

<CardGroup cols={2}>
  <Card title="Metrics" icon="chart-bar">
    **Prometheus + Grafana**
    - Resource utilization
    - Request rates
    - Error rates
    - Custom business metrics
  </Card>

  <Card title="Distributed Tracing" icon="route">
    **Jaeger + OpenTelemetry**
    - Request flow visualization
    - Latency breakdown
    - Service dependencies
    - Performance bottlenecks
  </Card>

  <Card title="Logging" icon="file-lines">
    **Structured JSON Logging**
    - Centralized log aggregation
    - Correlation IDs
    - Error tracking
    - Audit trails
  </Card>

  <Card title="LLM Observability" icon="brain">
    **LangSmith**
    - Prompt tracking
    - Token usage
    - Model performance
    - Chain visualization
  </Card>
</CardGroup>

### Metrics with Prometheus

#### Install Prometheus

**Kubernetes**:
```bash
## Add Prometheus Helm repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

## Install Prometheus stack
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
```
**Docker Compose**:
```yaml
services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'

volumes:
  prometheus-data:
```
#### Prometheus Configuration

**prometheus.yml**:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # MCP Server metrics
  - job_name: 'mcp-server-langgraph'
    static_configs:
      - targets: ['mcp-server-langgraph:9090']
    metrics_path: '/metrics'

  # Redis metrics
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # PostgreSQL metrics
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  # Keycloak metrics
  - job_name: 'keycloak'
    static_configs:
      - targets: ['keycloak:8080']
    metrics_path: '/metrics'

  # Node exporter (system metrics)
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```
#### Application Metrics

**Instrument FastAPI with Prometheus**:

```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import FastAPI, Request
import time

app = FastAPI()

## Define metrics
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

llm_requests_total = Counter(
    'llm_requests_total',
    'Total LLM requests',
    ['provider', 'model', 'status']
)

llm_tokens_total = Counter(
    'llm_tokens_total',
    'Total LLM tokens consumed',
    ['provider', 'model', 'type']
)

active_sessions = Gauge(
    'active_sessions',
    'Number of active user sessions'
)

openfga_checks_total = Counter(
    'openfga_checks_total',
    'Total OpenFGA authorization checks',
    ['result']
)

## Middleware to track metrics
@app.middleware("http")
async def track_metrics(request: Request, call_next):
    start_time = time.time()

    # Process request
    response = await call_next(request)

    # Record metrics
    duration = time.time() - start_time

    http_requests_total.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()

    http_request_duration_seconds.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)

    return response

## Metrics endpoint
@app.get("/metrics")
async def metrics():
    return Response(
        content=generate_latest(),
        media_type="text/plain"
    )
```
#### Custom Business Metrics

```python
from prometheus_client import Counter, Gauge

## User metrics
user_registrations_total = Counter(
    'user_registrations_total',
    'Total user registrations'
)

user_logins_total = Counter(
    'user_logins_total',
    'Total user logins',
    ['provider']
)

## Conversation metrics
conversations_created_total = Counter(
    'conversations_created_total',
    'Total conversations created'
)

messages_sent_total = Counter(
    'messages_sent_total',
    'Total messages sent',
    ['role']
)

active_conversations = Gauge(
    'active_conversations',
    'Number of active conversations'
)

## Tool usage metrics
tool_executions_total = Counter(
    'tool_executions_total',
    'Total tool executions',
    ['tool_name', 'status']
)

tool_execution_duration = Histogram(
    'tool_execution_duration_seconds',
    'Tool execution duration',
    ['tool_name']
)

## Usage in code
@app.post("/auth/register")
async def register(user_data: UserCreate):
    # ... registration logic ...
    user_registrations_total.inc()
    return user

@app.post("/chat")
async def chat(query: str):
    conversations_created_total.inc()
    messages_sent_total.labels(role="user").inc()

    # Execute LLM call
    start_time = time.time()
    response = await llm.ainvoke(query)
    duration = time.time() - start_time

    # Track LLM metrics
    llm_requests_total.labels(
        provider="anthropic",
        model="claude-3-5-sonnet-20241022",
        status="success"
    ).inc()

    llm_tokens_total.labels(
        provider="anthropic",
        model="claude-3-5-sonnet-20241022",
        type="prompt"
    ).inc(response.usage.prompt_tokens)

    llm_tokens_total.labels(
        provider="anthropic",
        model="claude-3-5-sonnet-20241022",
        type="completion"
    ).inc(response.usage.completion_tokens)

    messages_sent_total.labels(role="assistant").inc()

    return response
```
### Distributed Tracing with Jaeger

#### Install Jaeger

**Kubernetes**:
```bash
## Install Jaeger operator
kubectl create namespace observability
kubectl create -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.51.0/jaeger-operator.yaml -n observability

## Deploy Jaeger instance
cat << 'EOF' | kubectl apply -f -
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger
  namespace: observability
spec:
  strategy: production
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: http://elasticsearch:9200
EOF
```
**Docker Compose**:
```yaml
services:
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # UI
      - "4318:4318"    # OTLP HTTP
      - "4317:4317"    # OTLP gRPC
    environment:
      - COLLECTOR_OTLP_ENABLED=true
```
#### OpenTelemetry Configuration

**Install dependencies**:
```bash
pip install opentelemetry-api \
            opentelemetry-sdk \
            opentelemetry-instrumentation-fastapi \
            opentelemetry-exporter-otlp
```
**Instrument FastAPI**:
```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor

## Setup tracing
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

## Configure OTLP exporter
otlp_exporter = OTLPSpanExporter(
    endpoint="http://jaeger:4318/v1/traces"
)

trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(otlp_exporter)
)

## Auto-instrument FastAPI
FastAPIInstrumentor.instrument_app(app)
HTTPXClientInstrumentor().instrument()

## Manual instrumentation
@app.post("/chat")
async def chat(query: str):
    with tracer.start_as_current_span("chat") as span:
        span.set_attribute("query_length", len(query))

        # LLM call with tracing
        with tracer.start_as_current_span("llm_invoke") as llm_span:
            llm_span.set_attribute("provider", "anthropic")
            llm_span.set_attribute("model", "claude-3-5-sonnet-20241022")

            response = await llm.ainvoke(query)

            llm_span.set_attribute("prompt_tokens", response.usage.prompt_tokens)
            llm_span.set_attribute("completion_tokens", response.usage.completion_tokens)

        # OpenFGA check with tracing
        with tracer.start_as_current_span("openfga_check") as auth_span:
            allowed = await openfga_client.check_permission(
                user=f"user:{user_id}",
                relation="executor",
                object="tool:chat"
            )
            auth_span.set_attribute("allowed", allowed)

        span.set_attribute("response_length", len(response.content))

        return response
```
#### Trace Context Propagation

```python
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator

propagator = TraceContextTextMapPropagator()

## Inject trace context into HTTP headers
async def call_external_service():
    headers = {}
    propagator.inject(headers)

    async with httpx.AsyncClient() as client:
        response = await client.post(
            "https://external-api.com/endpoint",
            headers=headers
        )

    return response
```
### Structured Logging

#### Configure Structured JSON Logging

```python
import structlog
from structlog.stdlib import BoundLogger
import logging

## Configure structlog
structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.StackInfoRenderer(),
        structlog.dev.set_exc_info,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    wrapper_class=BoundLogger,
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

## Usage
@app.post("/chat")
async def chat(query: str, user_id: str):
    logger.info(
        "Chat request received",
        user_id=user_id,
        query_length=len(query),
        endpoint="/chat"
    )

    try:
        response = await llm.ainvoke(query)

        logger.info(
            "LLM response generated",
            user_id=user_id,
            model="claude-3-5-sonnet-20241022",
            prompt_tokens=response.usage.prompt_tokens,
            completion_tokens=response.usage.completion_tokens,
            latency_ms=(time.time() - start_time) * 1000
        )

        return response

    except Exception as e:
        logger.error(
            "Chat request failed",
            user_id=user_id,
            error=str(e),
            error_type=type(e).__name__,
            exc_info=True
        )
        raise
```
#### Correlation IDs

Track requests across services:

```python
from contextvars import ContextVar
import uuid

request_id_var: ContextVar[str] = ContextVar("request_id", default="")

@app.middleware("http")
async def add_correlation_id(request: Request, call_next):
    # Get or create correlation ID
    request_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
    request_id_var.set(request_id)

    # Add to structlog context
    structlog.contextvars.bind_contextvars(request_id=request_id)

    # Process request
    response = await call_next(request)

    # Add to response headers
    response.headers["X-Request-ID"] = request_id

    return response
```
### LangSmith Integration

#### Setup LangSmith

```python
import os
from langsmith import Client

## Configure LangSmith
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = settings.langsmith_api_key
os.environ["LANGCHAIN_PROJECT"] = "mcp-agent-production"

## Initialize client
langsmith_client = Client()

## LangSmith will automatically trace LangChain calls
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    anthropic_api_key=settings.anthropic_api_key
)

## This call is automatically traced
response = await llm.ainvoke("Hello!")
```
#### Custom LangSmith Runs

```python
from langsmith import traceable

@traceable(run_type="chain", name="chat_with_auth")
async def chat_with_auth(query: str, user_id: str):
    """Chat with authorization check"""

    # Check permission (traced)
    allowed = await check_permission(user_id, "tool:chat")

    if not allowed:
        raise PermissionError("User not authorized")

    # Call LLM (traced)
    response = await llm.ainvoke(query)

    return response

## Call function
result = await chat_with_auth("What is AI?", user_id="alice")
```
#### Feedback Collection

```python
from langsmith import Client

client = Client()

## Collect user feedback
@app.post("/feedback")
async def submit_feedback(
    run_id: str,
    score: float,
    comment: str = None
):
    """Submit feedback for a LangSmith run"""

    client.create_feedback(
        run_id=run_id,
        key="user_rating",
        score=score,
        comment=comment
    )

    return {"status": "feedback_recorded"}
```
### Grafana Dashboards

#### Install Grafana

```bash
## Add Grafana repo
helm repo add grafana https://grafana.github.io/helm-charts

## Install Grafana
helm install grafana grafana/grafana \
  --namespace monitoring \
  --set persistence.enabled=true \
  --set persistence.size=10Gi \
  --set adminPassword=admin123
```
#### Application Dashboard

**Import this JSON dashboard**:

```json
{
  "dashboard": {
    "title": "MCP Server - Application Metrics",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [{
          "expr": "rate(http_requests_total[5m])"
        }],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [{
          "expr": "rate(http_requests_total{status=~\"5..\"}[5m])"
        }],
        "type": "graph"
      },
      {
        "title": "P95 Latency",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
        }],
        "type": "graph"
      },
      {
        "title": "LLM Token Usage",
        "targets": [{
          "expr": "rate(llm_tokens_total[5m])"
        }],
        "type": "graph"
      },
      {
        "title": "Active Sessions",
        "targets": [{
          "expr": "active_sessions"
        }],
        "type": "stat"
      },
      {
        "title": "OpenFGA Check Rate",
        "targets": [{
          "expr": "rate(openfga_checks_total[5m])"
        }],
        "type": "graph"
      }
    ]
  }
}
```
#### LLM Observability Dashboard

```json
{
  "dashboard": {
    "title": "MCP Server - LLM Metrics",
    "panels": [
      {
        "title": "LLM Requests by Provider",
        "targets": [{
          "expr": "rate(llm_requests_total[5m])"
        }],
        "type": "graph"
      },
      {
        "title": "Token Cost per Hour",
        "targets": [{
          "expr": "sum(rate(llm_tokens_total{type=\"prompt\"}[1h])) * 0.003 + sum(rate(llm_tokens_total{type=\"completion\"}[1h])) * 0.015"
        }],
        "type": "stat"
      },
      {
        "title": "LLM Error Rate",
        "targets": [{
          "expr": "rate(llm_requests_total{status=\"error\"}[5m])"
        }],
        "type": "graph"
      },
      {
        "title": "Average Response Tokens",
        "targets": [{
          "expr": "rate(llm_tokens_total{type=\"completion\"}[5m]) / rate(llm_requests_total[5m])"
        }],
        "type": "stat"
      }
    ]
  }
}
```

#### Production-Ready Dashboards (v2.1.0)

<Note>
**NEW in v2.1.0** - 7 production-ready Grafana dashboards covering authentication, authorization, LLM performance, and infrastructure metrics.
</Note>

The repository includes pre-built Grafana dashboards optimized for production monitoring. All dashboards are located in `monitoring/grafana/dashboards/`.

<CardGroup cols={2}>
  <Card title="Authentication" icon="key">
    **authentication.json**
    - Login activity rate (attempts, success, failures)
    - Login failure rate gauge with thresholds
    - Response time percentiles (p50, p95, p99)
    - Active sessions count
    - Token operations (create, verify, refresh)
    - JWKS cache performance
  </Card>

  <Card title="OpenFGA Authorization" icon="shield">
    **openfga.json**
    - Authorization check rate (total, allowed, denied)
    - Denial rate gauge
    - Total relationship tuples
    - Check latency percentiles
    - Tuple write operations
    - Role sync operations and latency
  </Card>

  <Card title="LLM Performance" icon="brain">
    **llm-performance.json**
    - Agent call rate (successful/failed)
    - Error rate gauge
    - Response time percentiles
    - Tool calls rate
    - LLM invocations by model
    - Fallback model usage
  </Card>

  <Card title="Keycloak SSO" icon="fingerprint">
    **keycloak.json**
    - Service status gauge
    - Response time (p50, p95, p99)
    - Login request rate
    - Error rates (login, token refresh)
    - Active sessions and users
    - Resource utilization (CPU, memory)
  </Card>

  <Card title="Redis Sessions" icon="database">
    **redis-sessions.json**
    - Service status and memory usage
    - Active sessions (key count)
    - Operations rate (commands/sec)
    - Connection pool utilization
    - Session evictions
    - Memory fragmentation ratio
  </Card>

  <Card title="Security" icon="lock">
    **security.json**
    - Auth/AuthZ failures per second
    - JWT validation errors
    - Security status gauge
    - Failures by reason and resource
    - Failed attempts by user/IP
    - Top 10 violators table
  </Card>

  <Card title="Overview" icon="chart-line">
    **mcp-server-langgraph.json**
    - Service status uptime gauge
    - Request rate by tool
    - Error rate percentage
    - Response time percentiles
    - Memory and CPU usage per pod
    - Request success/failure count
  </Card>
</CardGroup>

##### Import Dashboards

**Option 1: Grafana UI (Manual)**

1. Open Grafana at http://localhost:3000
2. Navigate to **Dashboards** → **Import**
3. Click **Upload JSON file**
4. Select dashboard file from `monitoring/grafana/dashboards/`
5. Select **Prometheus** datasource
6. Click **Import**

Repeat for each dashboard you want to use.

**Option 2: Kubernetes ConfigMap (Automated)**

```bash
# Create ConfigMap from dashboard files
kubectl create configmap grafana-dashboards \
  --from-file=monitoring/grafana/dashboards/ \
  -n monitoring
```

Then mount the ConfigMap in your Grafana deployment by adding volumeMounts and volumes to the deployment manifest.

**Option 3: Helm Chart Configuration**

Configure dashboards in `values.yaml`:

```yaml
grafana:
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'mcp-server-langgraph'
        orgId: 1
        folder: 'MCP Server with LangGraph'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/langgraph

  dashboards:
    mcp-server-langgraph:
      authentication:
        file: monitoring/grafana/dashboards/authentication.json
      openfga:
        file: monitoring/grafana/dashboards/openfga.json
      llm-performance:
        file: monitoring/grafana/dashboards/llm-performance.json
      overview:
        file: monitoring/grafana/dashboards/mcp-server-langgraph.json
      security:
        file: monitoring/grafana/dashboards/security.json
      keycloak:
        file: monitoring/grafana/dashboards/keycloak.json
      redis-sessions:
        file: monitoring/grafana/dashboards/redis-sessions.json
```

##### Dashboard Features

All production dashboards include:

- **Auto-refresh** - 10-second refresh rate for real-time monitoring
- **Time range presets** - Last 5m, 15m, 1h, 6h, 24h, 7d
- **Thresholds** - Color-coded gauges (green/yellow/red)
- **Cross-links** - Navigate between related dashboards
- **Legend tables** - Current, max, and mean values
- **Panel descriptions** - Hover tooltips explaining metrics

##### Required Metrics

Ensure these metrics are exposed by the application:

**Authentication (authentication.json)**:
```promql
up{job="mcp-server-langgraph"}
auth_login_attempts_total
auth_login_success_total
auth_login_failed_total
auth_login_duration_bucket
token_created_total
token_verified_total
token_refreshed_total
session_active_count
jwks_cache_hits_total
jwks_cache_misses_total
```
**OpenFGA (openfga.json)**:
```promql
up{job="openfga"}
authz_checks_total
authz_successes_total
authz_failures_total
authz_check_duration_bucket
openfga_tuple_count
openfga_tuples_written_total
openfga_tuples_deleted_total
openfga_sync_operations_total
openfga_sync_duration_bucket
```
**LLM Performance (llm-performance.json)**:
```promql
agent_calls_successful_total
agent_calls_failed_total
agent_response_duration_bucket
agent_tool_calls_total
## With labels: model, operation, tool
```
**Keycloak & Redis**:
```promql
up{job="keycloak"}
up{job="redis-session"}
keycloak_request_duration_bucket
keycloak_login_attempts_total
redis_memory_used_bytes
redis_db_keys
redis_commands_processed_total
```
### Service Level Objectives (SLOs)

<Info>
**NEW in v2.1.0** - Pre-computed SLO metrics via Prometheus recording rules for efficient monitoring and alerting.
</Info>

#### SLO Recording Rules

The `monitoring/prometheus/rules/slo-recording-rules.yaml` file contains 40+ recording rules that pre-compute Service Level Indicators (SLIs) for fast querying in Grafana.

**Load recording rules**:
```bash
## Kubernetes with Prometheus Operator
kubectl apply -f monitoring/prometheus/rules/slo-recording-rules.yaml

## Docker Compose
## Add to prometheus.yml:
rule_files:
  - /etc/prometheus/rules/slo-recording-rules.yaml
```
#### Available SLO Metrics

<Tabs>
  <Tab title="Availability">
    **Target: 99.9% uptime**

    ```promql
    # Overall service availability
    job:up:avg

    # Component availability
    job:up:avg:keycloak      # Target: 99.5%
    job:up:avg:openfga       # Target: 99.5%
    job:up:avg:redis_session # Target: 99.9%
```
    **Usage in Grafana**:
    ```promql
    # Current availability
    job:up:avg * 100

    # Downtime minutes per month
    (1 - job:up:avg) * 43200
    ```
  </Tab>

  <Tab title="Latency">
    **Target: p95 < 2s, p99 < 5s**

    ```promql
    # Agent response time
    job:agent_response_duration:p95  # Target: 2000ms
    job:agent_response_duration:p99  # Target: 5000ms

    # Authentication latency
    job:auth_login_duration:p95      # Target: 500ms

    # Authorization latency
    job:authz_check_duration:p95     # Target: 100ms

    # Keycloak latency
    job:keycloak_request_duration:p95 # Target: 1000ms
```
    **Usage in Grafana**:
    ```promql
    # P95 latency vs target
    job:agent_response_duration:p95 / 2000 * 100
    ```
  </Tab>

  <Tab title="Error Rate">
    **Target: < 1% errors (99% success)**

    ```promql
    # Overall error rate
    job:agent_calls:error_rate        # Target: 0.01 (1%)

    # Authentication failures
    job:auth_login:error_rate         # Target: 0.05 (5%)

    # Authorization denials
    job:authz_checks:denial_rate      # Target: 0.15 (15%)

    # Token verification failures
    job:token_verification:error_rate # Target: 0.02 (2%)

    # LLM fallback rate
    job:llm_fallback:rate             # Target: 0.10 (10%)
```
    **Usage in Grafana**:
    ```promql
    # Error rate as percentage
    job:agent_calls:error_rate * 100

    # Success rate
    (1 - job:agent_calls:error_rate) * 100
    ```
  </Tab>

  <Tab title="Saturation">
    **Target: < 80% CPU, < 90% memory**

    ```promql
    # Memory saturation
    job:memory:saturation             # Target: 0.90 (90%)

    # CPU saturation
    job:cpu:saturation                # Target: 0.80 (80%)

    # Redis memory saturation
    job:redis_memory:saturation       # Target: 0.90 (90%)

    # Redis connection pool
    job:redis_pool:saturation         # Target: 0.95 (95%)
```
    **Usage in Grafana**:
    ```promql
    # Memory pressure
    job:memory:saturation * 100

    # Available memory
    (1 - job:memory:saturation) * 100
    ```
  </Tab>

  <Tab title="Error Budget">
    **Burn rate detection across multiple windows**

    ```promql
    # Fast burn (1 hour window)
    job:error_budget:burn_rate_1h

    # Medium burn (6 hour window)
    job:error_budget:burn_rate_6h

    # Slow burn (3 day window)
    job:error_budget:burn_rate_3d
```
    **Interpretation**:
    - Burn rate = 1.0: Consuming error budget at expected rate
    - Burn rate > 1.0: Consuming faster (alert!)
    - Burn rate < 1.0: Consuming slower (healthy)

    **Alert on fast burn**:
    ```yaml
    - alert: FastErrorBudgetBurn
      expr: job:error_budget:burn_rate_1h > 14.4
      for: 5m
      annotations:
        summary: "Error budget burning 14.4x faster than sustainable"
    ```
  </Tab>

  <Tab title="Compliance">
    **30-day rolling window SLO compliance**

    ```promql
    # Availability compliance
    job:slo_compliance:availability_30d  # Target: 99.9%

    # Latency compliance (% within SLO)
    job:slo_compliance:latency_p95_30d   # Target: 95%

    # Error rate compliance (success rate)
    job:slo_compliance:error_rate_30d    # Target: 99%
```
    **Usage in Grafana**:
    ```promql
    # Monthly SLO report
    job:slo_compliance:availability_30d * 100
    ```
  </Tab>
</Tabs>

#### SLO Dashboard Example

Create an SLO summary dashboard:

```json
{
  "dashboard": {
    "title": "SLO Compliance - MCP Server",
    "panels": [
      {
        "title": "Availability SLO (99.9% target)",
        "targets": [{
          "expr": "job:up:avg * 100"
        }],
        "thresholds": [
          {"color": "red", "value": 0},
          {"color": "yellow", "value": 99.5},
          {"color": "green", "value": 99.9}
        ],
        "type": "gauge"
      },
      {
        "title": "Error Rate SLO (< 1% target)",
        "targets": [{
          "expr": "job:agent_calls:error_rate * 100"
        }],
        "thresholds": [
          {"color": "green", "value": 0},
          {"color": "yellow", "value": 0.5},
          {"color": "red", "value": 1.0}
        ],
        "type": "gauge"
      },
      {
        "title": "Latency SLO (p95 < 2s)",
        "targets": [{
          "expr": "job:agent_response_duration:p95"
        }],
        "thresholds": [
          {"color": "green", "value": 0},
          {"color": "yellow", "value": 1500},
          {"color": "red", "value": 2000}
        ],
        "type": "gauge",
        "unit": "ms"
      },
      {
        "title": "Error Budget Burn Rate (1h window)",
        "targets": [{
          "expr": "job:error_budget:burn_rate_1h"
        }],
        "thresholds": [
          {"color": "green", "value": 0},
          {"color": "yellow", "value": 2},
          {"color": "red", "value": 14.4}
        ],
        "type": "graph"
      }
    ]
  }
}
```
#### Benefits of SLO Recording Rules

1. **Performance** - Pre-computed metrics query 10-100x faster
2. **Consistency** - Same calculation across all dashboards
3. **Alerting** - Alert on SLO violations, not raw metrics
4. **Reporting** - Historical SLO compliance tracking
5. **Error Budgets** - Multi-window burn rate detection

### Alerting with Alertmanager

#### Configure Alerts

**prometheus-rules.yaml**:
```yaml
groups:
- name: langgraph_alerts
  rules:
  # High error rate
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value }} requests/second"

  # High latency
  - alert: HighLatency
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High API latency"
      description: "P95 latency is {{ $value }}s"

  # LLM failures
  - alert: LLMFailures
    expr: rate(llm_requests_total{status="error"}[5m]) > 0.1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "LLM requests failing"
      description: "LLM error rate: {{ $value }}"

  # Redis down
  - alert: RedisDown
    expr: up{job="redis"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Redis is down"
      description: "Redis instance {{ $labels.instance }} is unreachable"

  # High token usage
  - alert: HighTokenUsage
    expr: sum(rate(llm_tokens_total[1h])) > 1000000
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Unusually high token usage"
      description: "Token usage: {{ $value }} tokens/hour"

  # Low session count (possible issue)
  - alert: NoActiveSessions
    expr: active_sessions == 0
    for: 30m
    labels:
      severity: warning
    annotations:
      summary: "No active sessions"
      description: "No users have active sessions for 30 minutes"
```
#### Alertmanager Configuration

```yaml
global:
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'slack-notifications'

  routes:
  - match:
      severity: critical
    receiver: 'pagerduty'

receivers:
- name: 'slack-notifications'
  slack_configs:
  - channel: '#alerts'
    title: 'Alert: {{ .GroupLabels.alertname }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

- name: 'pagerduty'
  pagerduty_configs:
  - service_key: 'YOUR_PAGERDUTY_KEY'
```
### Health Checks

#### Kubernetes Probes

```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      - name: agent
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 3

        startupProbe:
          httpGet:
            path: /health/startup
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 30
```
#### Health Check Implementation

```python
from fastapi import FastAPI, status
from fastapi.responses import JSONResponse

@app.get("/health/live")
async def liveness():
    """Liveness probe - is the application running?"""
    return {"status": "alive"}

@app.get("/health/ready")
async def readiness():
    """Readiness probe - can the application serve traffic?"""

    checks = {}

    # Check Redis
    try:
        await redis_client.ping()
        checks["redis"] = "healthy"
    except Exception as e:
        checks["redis"] = f"unhealthy: {e}"

    # Check Keycloak
    try:
        response = await httpx.get(f"{settings.keycloak_url}/health")
        checks["keycloak"] = "healthy" if response.status_code == 200 else "unhealthy"
    except Exception as e:
        checks["keycloak"] = f"unhealthy: {e}"

    # Check OpenFGA
    try:
        response = await httpx.get(f"{settings.openfga_url}/healthz")
        checks["openfga"] = "healthy" if response.status_code == 200 else "unhealthy"
    except Exception as e:
        checks["openfga"] = f"unhealthy: {e}"

    # Determine overall status
    all_healthy = all(v == "healthy" for v in checks.values())

    return JSONResponse(
        content={"status": "ready" if all_healthy else "not_ready", "checks": checks},
        status_code=status.HTTP_200_OK if all_healthy else status.HTTP_503_SERVICE_UNAVAILABLE
    )

@app.get("/health/startup")
async def startup():
    """Startup probe - has the application finished starting?"""
    # Check if initialization is complete
    if not app_initialized:
        return JSONResponse(
            content={"status": "starting"},
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE
        )

    return {"status": "started"}
```
### Best Practices

<AccordionGroup>
  <Accordion title="Use Structured Logging" icon="brackets-curly">
    Always use structured JSON logging for easy parsing:

    ```python
    # Good
    logger.info(
        "User authenticated",
        user_id=user_id,
        provider="keycloak",
        duration_ms=duration
    )

    # Bad
    logger.info(f"User {user_id} authenticated via keycloak in {duration}ms")
    ```
  </Accordion>

  <Accordion title="Track Business Metrics" icon="chart-mixed">
    Monitor business KPIs, not just technical metrics:

    - Conversations per user
    - Average conversation length
    - Token cost per user
    - Tool usage patterns
    - User retention
  </Accordion>

  <Accordion title="Set SLOs and Alerts" icon="bell">
    Define Service Level Objectives:

    - Availability: 99.9% uptime
    - Latency: P95 < 2s
    - Error rate: < 0.1%
    - Token budget: < $1000/day
  </Accordion>

  <Accordion title="Use Correlation IDs" icon="link">
    Track requests across all services:

    ```python
    # Generate once per request
    request_id = str(uuid.uuid4())

    # Pass to all downstream services
    headers = {"X-Request-ID": request_id}

    # Log with correlation ID
    logger.info("Processing request", request_id=request_id)
    ```
  </Accordion>
</AccordionGroup>

### Next Steps

<CardGroup cols={2}>
  <Card title="Scaling" icon="arrow-up-right-dots" href="/deployment/scaling">
    Auto-scaling configuration
  </Card>
  <Card title="Disaster Recovery" icon="life-ring" href="/deployment/disaster-recovery">
    Backup and recovery
  </Card>
  <Card title="Alerts" icon="bell" href="/deployment/monitoring#alerting-with-alertmanager">
    Alert configuration with Alertmanager
  </Card>
  <Card title="Security Best Practices" icon="shield" href="/security/best-practices">
    Security hardening guide
  </Card>
</CardGroup>

---

<Check>
**Monitoring Ready**: Comprehensive observability with metrics, traces, and logs!
</Check>
