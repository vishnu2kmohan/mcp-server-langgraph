# ============================================================================
# Service Configuration
# ============================================================================
SERVICE_NAME=mcp-server-langgraph
SERVICE_VERSION=2.7.0
ENVIRONMENT=development

# ============================================================================
# LLM Provider Configuration (LiteLLM)
# ============================================================================
# Provider: google, anthropic, openai, azure, bedrock, ollama
LLM_PROVIDER=google

# Model name (provider-specific format)
# Gemini 2.5 models: gemini-2.5-flash, gemini-2.5-pro
MODEL_NAME=gemini-2.5-flash
MODEL_TEMPERATURE=0.7
MODEL_MAX_TOKENS=8192
MODEL_TIMEOUT=60

# Fallback configuration (tries models in order if primary fails)
ENABLE_FALLBACK=true
# FALLBACK_MODELS=["claude-haiku-4-5-20251001","claude-sonnet-4-5-20250929","gpt-4o"]

# ============================================================================
# API Keys (Provider-specific)
# ============================================================================

# Google Gemini (https://aistudio.google.com/apikey)
GOOGLE_API_KEY=your-key-here

# Anthropic (https://console.anthropic.com/)
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# OpenAI (https://platform.openai.com/)
# OPENAI_API_KEY=sk-your-key-here

# Azure OpenAI
# AZURE_API_KEY=your-key-here
# AZURE_API_BASE=https://your-resource.openai.azure.com
# AZURE_DEPLOYMENT_NAME=gpt-4

# AWS Bedrock
# AWS_ACCESS_KEY_ID=your-access-key
# AWS_SECRET_ACCESS_KEY=your-secret-key
# AWS_REGION=us-east-1

# Ollama (local models)
# OLLAMA_BASE_URL=http://localhost:11434

# ============================================================================
# Authentication & Authorization
# ============================================================================
# ⚠️ PRODUCTION WARNING: Generate a strong secret using:
#    openssl rand -base64 32
# Store in Infisical or secrets manager, never commit to git!

# Auth Provider: inmemory, keycloak
AUTH_PROVIDER=inmemory
# Auth Mode: token (JWT), session
AUTH_MODE=token

# JWT Configuration
JWT_SECRET_KEY=your-secret-key-change-in-production
JWT_ALGORITHM=HS256
JWT_EXPIRATION_SECONDS=3600

# OpenFGA (run setup_openfga.py to get IDs)
OPENFGA_API_URL=http://localhost:8080
OPENFGA_STORE_ID=
OPENFGA_MODEL_ID=

# ============================================================================
# Keycloak Configuration (when AUTH_PROVIDER=keycloak)
# ============================================================================
KEYCLOAK_SERVER_URL=http://localhost:8080
KEYCLOAK_REALM=langgraph-agent
KEYCLOAK_CLIENT_ID=langgraph-client
KEYCLOAK_CLIENT_SECRET=your-client-secret-here
KEYCLOAK_VERIFY_SSL=true
KEYCLOAK_TIMEOUT=30
# Public hostname for token validation
KEYCLOAK_HOSTNAME=localhost

# ============================================================================
# Session Management (when AUTH_MODE=session)
# ============================================================================
# Session Backend: memory, redis
SESSION_BACKEND=memory
# Redis connection for session storage (uses db 0)
REDIS_URL=redis://localhost:6379/0
REDIS_PASSWORD=your-redis-password-here
REDIS_SSL=false
# Session TTL in seconds (default: 86400 = 24 hours)
SESSION_TTL_SECONDS=86400
# Enable sliding window (refresh session on each request)
SESSION_SLIDING_WINDOW=true
# Maximum concurrent sessions per user
SESSION_MAX_CONCURRENT=5

# ============================================================================
# Conversation Checkpointing (Distributed State for Auto-Scaling)
# ============================================================================
# Checkpoint Backend: memory (development), redis (production)
# IMPORTANT: Use "redis" for production with HPA auto-scaling
# - "memory": Pod-local state (lost on pod restart/scale events)
# - "redis": Distributed state (works across all replicas)
CHECKPOINT_BACKEND=memory

# Redis connection for conversation checkpoints (uses db 1, separate from sessions)
CHECKPOINT_REDIS_URL=redis://localhost:6379/1

# Checkpoint TTL in seconds (default: 604800 = 7 days)
# Conversations older than this are automatically cleaned up
CHECKPOINT_REDIS_TTL=604800

# ============================================================================
# Secrets Management (Infisical - Optional)
# ============================================================================
INFISICAL_SITE_URL=https://app.infisical.com
# INFISICAL_CLIENT_ID=your-client-id
# INFISICAL_CLIENT_SECRET=your-client-secret
# INFISICAL_PROJECT_ID=your-project-id

# ============================================================================
# Observability (OpenTelemetry)
# ============================================================================
OTLP_ENDPOINT=http://localhost:4317
ENABLE_CONSOLE_EXPORT=true
ENABLE_TRACING=true
ENABLE_METRICS=true

# Prometheus (for SLA monitoring and compliance metrics)
PROMETHEUS_URL=http://prometheus:9090
PROMETHEUS_TIMEOUT=30
PROMETHEUS_RETRY_ATTEMPTS=3

# ============================================================================
# Alerting Configuration (for SLA breaches, compliance issues, security events)
# ============================================================================
# PagerDuty (https://developer.pagerduty.com/docs/ZG9jOjExMDI5NTgx-events-api-v2)
# PAGERDUTY_INTEGRATION_KEY=your-integration-key

# Slack (https://api.slack.com/messaging/webhooks)
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# OpsGenie (https://docs.opsgenie.com/docs/alert-api)
# OPSGENIE_API_KEY=your-api-key

# Email (SMTP)
# EMAIL_SMTP_HOST=smtp.gmail.com
# EMAIL_SMTP_PORT=587
# EMAIL_FROM_ADDRESS=alerts@example.com
# EMAIL_TO_ADDRESSES=ops@example.com,security@example.com

# ============================================================================
# Web Search Configuration (for search tools)
# ============================================================================
# Tavily (recommended for AI applications)
# Get key: https://tavily.com/
# TAVILY_API_KEY=tvly-your-key-here

# Serper (Google Search API)
# Get key: https://serper.dev/
# SERPER_API_KEY=your-serper-key

# Brave Search (privacy-focused)
# Get key: https://brave.com/search/api/
# BRAVE_API_KEY=your-brave-key

# ============================================================================
# LangSmith Observability
# ============================================================================
# Get API key from: https://smith.langchain.com/settings
LANGSMITH_API_KEY=your-langsmith-api-key
LANGSMITH_TRACING=true
LANGSMITH_TRACING_V2=true
LANGSMITH_PROJECT=mcp-server-langgraph
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# Observability Backend Selection: opentelemetry, langsmith, both
OBSERVABILITY_BACKEND=both

# ============================================================================
# LangGraph Platform
# ============================================================================
# For deploying to LangGraph Cloud
# Get API key from: https://smith.langchain.com/settings (same as LangSmith)
LANGGRAPH_API_KEY=your-langsmith-api-key
LANGGRAPH_API_URL=https://api.langchain.com
# Set after deployment: langgraph deployment get <name>
LANGGRAPH_DEPLOYMENT_URL=

# ============================================================================
# Logging Configuration
# ============================================================================
LOG_LEVEL=INFO
# LOG_FILE=mcp-server-langgraph.log

# Log Format: json (structured JSON) or text (human-readable)
LOG_FORMAT=json

# JSON Log Indentation: null (compact) or 2 (pretty-print for development)
LOG_JSON_INDENT=null

# ============================================================================
# Log Aggregation Platform Selection
# ============================================================================
# Choose your log aggregation platform:
# Options: aws, gcp, azure, elasticsearch, datadog, splunk
# LOG_EXPORTER=datadog

# ============================================================================
# AWS CloudWatch Configuration (LOG_EXPORTER=aws)
# ============================================================================
# AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=your-access-key  # Or use IAM role (recommended)
# AWS_SECRET_ACCESS_KEY=your-secret-key  # Or use IAM role (recommended)
# AWS_LOG_GROUP=/aws/mcp-server-langgraph/${ENVIRONMENT}
# AWS_LOG_STREAM={service.name}/{hostname}

# ============================================================================
# GCP Cloud Logging Configuration (LOG_EXPORTER=gcp)
# ============================================================================
# GCP_PROJECT_ID=your-project-id
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json  # Or use Workload Identity
# GCP_LOG_NAME=mcp-server-langgraph

# ============================================================================
# Azure Monitor Configuration (LOG_EXPORTER=azure)
# ============================================================================
# Azure Application Insights Connection String (recommended)
# AZURE_MONITOR_CONNECTION_STRING=InstrumentationKey=...;IngestionEndpoint=...
# Or use Instrumentation Key (legacy)
# AZURE_MONITOR_INSTRUMENTATION_KEY=your-instrumentation-key

# ============================================================================
# Elasticsearch Configuration (LOG_EXPORTER=elasticsearch)
# ============================================================================
# ELASTICSEARCH_ENDPOINT=https://elasticsearch:9200
# ELASTICSEARCH_USERNAME=elastic
# ELASTICSEARCH_PASSWORD=changeme
# Or use API Key authentication
# ELASTICSEARCH_API_KEY=your-api-key
# For Elastic Cloud
# ELASTICSEARCH_CLOUD_ID=your-cloud-id:dXMtY2VudHJhbDE...

# ============================================================================
# Datadog Configuration (LOG_EXPORTER=datadog)
# ============================================================================
# DATADOG_API_KEY=your-api-key
# DATADOG_SITE=datadoghq.com  # or datadoghq.eu, us3.datadoghq.com, us5.datadoghq.com

# ============================================================================
# Splunk Configuration (LOG_EXPORTER=splunk)
# ============================================================================
# Splunk Enterprise (via HEC)
# SPLUNK_HEC_TOKEN=your-hec-token
# SPLUNK_HEC_ENDPOINT=https://splunk:8088

# Splunk Observability Cloud (optional)
# SPLUNK_ACCESS_TOKEN=your-access-token
# SPLUNK_REALM=us0  # or us1, eu0, jp0, au0

# ============================================================================
# Agent Configuration
# ============================================================================
MAX_ITERATIONS=10
ENABLE_CHECKPOINTING=true

# ============================================================================
# Agentic Loop Configuration (Anthropic Best Practices)
# ============================================================================

# --------------------------------------------------------------------------
# Context Management - Implements "Compaction" technique
# --------------------------------------------------------------------------
# Enable conversation compaction for unlimited conversation length
ENABLE_CONTEXT_COMPACTION=true

# Token count that triggers compaction (default: 8000)
COMPACTION_THRESHOLD=8000

# Target token count after compaction (default: 4000 = 40-60% reduction)
TARGET_AFTER_COMPACTION=4000

# Number of recent messages to keep uncompacted (default: 5)
RECENT_MESSAGE_COUNT=5

# --------------------------------------------------------------------------
# Work Verification - Implements "LLM-as-Judge" pattern
# --------------------------------------------------------------------------
# Enable automatic quality verification for all responses
ENABLE_VERIFICATION=true

# Minimum quality score to pass (0.0-1.0, default: 0.7)
VERIFICATION_QUALITY_THRESHOLD=0.7

# Maximum refinement attempts (default: 3)
MAX_REFINEMENT_ATTEMPTS=3

# Verification mode: strict (0.8), standard (0.7), lenient (0.6)
VERIFICATION_MODE=standard

# ============================================================================
# Advanced Features (Anthropic Best Practices Enhancements)
# ============================================================================

# --------------------------------------------------------------------------
# Dynamic Context Loading (Just-in-Time) - Requires Qdrant
# --------------------------------------------------------------------------
# Enable semantic search-based context loading
# Implements "Just-in-Time" and "Progressive Disclosure" patterns
ENABLE_DYNAMIC_CONTEXT_LOADING=false

# Qdrant vector database connection
QDRANT_URL=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=mcp_context

# Dynamic loading parameters
DYNAMIC_CONTEXT_MAX_TOKENS=2000  # Maximum tokens to load
DYNAMIC_CONTEXT_TOP_K=3  # Number of top semantic search results

# Embedding Configuration
# Provider: "google" (Gemini API, recommended) or "local" (sentence-transformers, self-hosted)
EMBEDDING_PROVIDER=google

# Embedding model name
# Google: models/text-embedding-004 (latest, 768 dims), models/embedding-001
# Local: all-MiniLM-L6-v2 (384 dims), all-mpnet-base-v2 (768 dims)
EMBEDDING_MODEL_NAME=models/text-embedding-004

# Embedding dimensions (must match model)
# Google text-embedding-004: 768 (supports 128-3072 via truncation)
# Local all-MiniLM-L6-v2: 384
EMBEDDING_DIMENSIONS=768

# Google-specific: Task type optimization
# Options: RETRIEVAL_DOCUMENT, RETRIEVAL_QUERY, SEMANTIC_SIMILARITY, CLASSIFICATION
EMBEDDING_TASK_TYPE=RETRIEVAL_DOCUMENT

# LRU cache size for loaded contexts (default: 100)
CONTEXT_CACHE_SIZE=100

# Legacy parameter (deprecated, use EMBEDDING_MODEL_NAME instead)
# EMBEDDING_MODEL=all-MiniLM-L6-v2

# --------------------------------------------------------------------------
# Parallel Tool Execution - Performance Optimization
# --------------------------------------------------------------------------
# Enable parallel execution for independent tool calls
ENABLE_PARALLEL_EXECUTION=false

# Maximum concurrent tool executions (default: 5)
MAX_PARALLEL_TOOLS=5

# --------------------------------------------------------------------------
# Enhanced Note-Taking - Structured Information Extraction
# --------------------------------------------------------------------------
# Use LLM for structured note extraction (6 categories)
# Categories: decisions, requirements, facts, action_items, issues, preferences
ENABLE_LLM_EXTRACTION=false

# ============================================================================
# Qdrant Configuration (for Dynamic Context Loading)
# ============================================================================
# Optional log level for Qdrant service
# QDRANT_LOG_LEVEL=INFO
