# ============================================================================
# Service Configuration
# ============================================================================
SERVICE_NAME=mcp-server-langgraph
SERVICE_VERSION=2.2.0
ENVIRONMENT=development

# ============================================================================
# LLM Provider Configuration (LiteLLM)
# ============================================================================
# Provider: google, anthropic, openai, azure, bedrock, ollama
LLM_PROVIDER=google

# Model name (provider-specific format)
# Gemini 2.5 models: gemini-2.5-flash-002, gemini-2.5-pro
MODEL_NAME=gemini-2.5-flash-002
MODEL_TEMPERATURE=0.7
MODEL_MAX_TOKENS=8192
MODEL_TIMEOUT=60

# Fallback configuration (tries models in order if primary fails)
ENABLE_FALLBACK=true
# FALLBACK_MODELS=["gemini-2.5-pro","claude-3-5-sonnet-20241022","gpt-4o"]

# ============================================================================
# API Keys (Provider-specific)
# ============================================================================

# Google Gemini (https://aistudio.google.com/apikey)
GOOGLE_API_KEY=your-key-here

# Anthropic (https://console.anthropic.com/)
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# OpenAI (https://platform.openai.com/)
# OPENAI_API_KEY=sk-your-key-here

# Azure OpenAI
# AZURE_API_KEY=your-key-here
# AZURE_API_BASE=https://your-resource.openai.azure.com
# AZURE_DEPLOYMENT_NAME=gpt-4

# AWS Bedrock
# AWS_ACCESS_KEY_ID=your-access-key
# AWS_SECRET_ACCESS_KEY=your-secret-key
# AWS_REGION=us-east-1

# Ollama (local models)
# OLLAMA_BASE_URL=http://localhost:11434

# ============================================================================
# Authentication & Authorization
# ============================================================================
# ⚠️ PRODUCTION WARNING: Generate a strong secret using:
#    openssl rand -base64 32
# Store in Infisical or secrets manager, never commit to git!

# Auth Provider: inmemory, keycloak
AUTH_PROVIDER=inmemory
# Auth Mode: token (JWT), session
AUTH_MODE=token

# JWT Configuration
JWT_SECRET_KEY=your-secret-key-change-in-production
JWT_ALGORITHM=HS256
JWT_EXPIRATION_SECONDS=3600

# OpenFGA (run setup_openfga.py to get IDs)
OPENFGA_API_URL=http://localhost:8080
OPENFGA_STORE_ID=
OPENFGA_MODEL_ID=

# ============================================================================
# Keycloak Configuration (when AUTH_PROVIDER=keycloak)
# ============================================================================
KEYCLOAK_SERVER_URL=http://localhost:8080
KEYCLOAK_REALM=langgraph-agent
KEYCLOAK_CLIENT_ID=langgraph-client
KEYCLOAK_CLIENT_SECRET=your-client-secret-here
KEYCLOAK_VERIFY_SSL=true
KEYCLOAK_TIMEOUT=30
# Public hostname for token validation
KEYCLOAK_HOSTNAME=localhost

# ============================================================================
# Session Management (when AUTH_MODE=session)
# ============================================================================
# Session Backend: memory, redis
SESSION_BACKEND=memory
# Redis connection for session storage
REDIS_URL=redis://localhost:6379/0
REDIS_PASSWORD=your-redis-password-here
REDIS_SSL=false
# Session TTL in seconds (default: 86400 = 24 hours)
SESSION_TTL_SECONDS=86400
# Enable sliding window (refresh session on each request)
SESSION_SLIDING_WINDOW=true
# Maximum concurrent sessions per user
SESSION_MAX_CONCURRENT=5

# ============================================================================
# Secrets Management (Infisical - Optional)
# ============================================================================
INFISICAL_SITE_URL=https://app.infisical.com
# INFISICAL_CLIENT_ID=your-client-id
# INFISICAL_CLIENT_SECRET=your-client-secret
# INFISICAL_PROJECT_ID=your-project-id

# ============================================================================
# Observability (OpenTelemetry)
# ============================================================================
OTLP_ENDPOINT=http://localhost:4317
ENABLE_CONSOLE_EXPORT=true
ENABLE_TRACING=true
ENABLE_METRICS=true

# ============================================================================
# LangSmith Observability
# ============================================================================
# Get API key from: https://smith.langchain.com/settings
LANGSMITH_API_KEY=your-langsmith-api-key
LANGSMITH_TRACING=true
LANGSMITH_TRACING_V2=true
LANGSMITH_PROJECT=mcp-server-langgraph
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# Observability Backend Selection: opentelemetry, langsmith, both
OBSERVABILITY_BACKEND=both

# ============================================================================
# LangGraph Platform
# ============================================================================
# For deploying to LangGraph Cloud
# Get API key from: https://smith.langchain.com/settings (same as LangSmith)
LANGGRAPH_API_KEY=your-langsmith-api-key
LANGGRAPH_API_URL=https://api.langchain.com
# Set after deployment: langgraph deployment get <name>
LANGGRAPH_DEPLOYMENT_URL=

# Logging
LOG_LEVEL=INFO
# LOG_FILE=langgraph-mcp-agent.log

# ============================================================================
# Agent Configuration
# ============================================================================
MAX_ITERATIONS=10
ENABLE_CHECKPOINTING=true
