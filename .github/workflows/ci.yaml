name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  release:
    types: [ created ]
  workflow_dispatch:  # Allow manual triggering

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}  # Cancel only for PRs, not for main/develop

permissions:
  contents: read
  pull-requests: write

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  label:
    name: Auto-label PR
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    permissions:
      contents: read
      pull-requests: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Apply labels
      uses: actions/labeler@v6
      with:
        repo-token: ${{ secrets.GITHUB_TOKEN }}

  test:
    name: Test
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: |
          requirements-pinned.txt
          requirements-test.txt

    - name: Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements-pinned.txt', 'requirements-test.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install -r requirements-pinned.txt
        pip install -r requirements-test.txt

    - name: Run unit tests
      run: |
        # Matches pyproject.toml configuration and local make test-unit
        pytest -m unit --cov=src/mcp_server_langgraph --cov-report=xml --cov-report=term-missing

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Run integration tests (containerized)
      run: |
        chmod +x scripts/test-integration.sh
        ./scripts/test-integration.sh --no-cache
      # Integration tests now reliable in Docker - no longer allow failures
      # continue-on-error: true  # REMOVED: Tests now run in isolated Docker environment
      # --no-cache: Ensures Docker builds use latest test code (not cached layers)

    - name: Upload coverage
      uses: codecov/codecov-action@v5
      with:
        files: ./coverage.xml
        fail_ci_if_error: false

  lint:
    name: Lint
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: requirements-test.txt

    - name: Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-lint-${{ hashFiles('requirements-test.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-lint-

    - name: Install linting tools
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install -r requirements-test.txt

    - name: Run flake8
      run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --exclude=venv,tests

    - name: Run black check
      run: black --check . --exclude venv

    - name: Run isort check
      run: isort --check . --skip venv --profile black --line-length 127

    - name: Run mypy
      run: mypy src/ --ignore-missing-imports
      continue-on-error: true  # Allow mypy failures for now (strict mode rollout in progress)

    - name: Security scan with bandit
      run: bandit -r . -x ./tests,./venv -ll

  validate-deployments:
    name: Validate Deployment Configurations
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install PyYAML
      run: pip install pyyaml

    - name: Run deployment validation script
      run: python3 scripts/validation/validate_deployments.py

    - name: Validate Docker Compose
      run: |
        docker compose -f docker/docker-compose.yml config --quiet
        echo "✓ Docker Compose configuration valid"

    - name: Set up Helm
      uses: azure/setup-helm@v4
      with:
        version: '3.13.0'

    - name: Cache Helm
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/helm
          ~/.local/share/helm
        key: ${{ runner.os }}-helm-${{ hashFiles('deployments/helm/**/Chart.yaml') }}
        restore-keys: |
          ${{ runner.os }}-helm-

    - name: Validate Helm chart
      run: |
        helm lint deployments/helm/mcp-server-langgraph
        echo "✓ Helm chart validation passed"

    - name: Build Helm dependencies
      run: |
        cd deployments/helm/mcp-server-langgraph
        helm dependency build
        cd -

    - name: Test Helm template rendering
      run: |
        helm template test-release deployments/helm/mcp-server-langgraph --dry-run > /dev/null
        echo "✓ Helm template rendering successful"

    - name: Cache kubectl
      uses: actions/cache@v4
      with:
        path: /usr/local/bin/kubectl
        key: ${{ runner.os }}-kubectl-${{ hashFiles('.github/workflows/ci.yaml') }}
        restore-keys: |
          ${{ runner.os }}-kubectl-

    # TODO: Fix Kustomize overlays path resolution
    # Kustomize has a security constraint that prevents referencing resources
    # outside the kustomization directory tree. Need to restructure to either:
    # 1. Copy kubernetes/base manifests into kustomize/base/resources/
    # 2. Use kustomize components
    # 3. Restructure to have shared base within kustomize tree
    # For now, validate kubernetes manifests directly instead
    - name: Validate Kubernetes manifests
      run: |
        echo "Validating Kubernetes base manifests..."
        for manifest in deployments/kubernetes/base/*.yaml; do
          kubectl apply --dry-run=client -f "$manifest" > /dev/null 2>&1 || echo "⚠️  Skipped $manifest (may require cluster context)"
        done
        echo "✓ Kubernetes manifest validation completed"

  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, lint, validate-deployments]
    permissions:
      contents: read
      packages: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push
      uses: docker/build-push-action@v6
      with:
        context: .
        file: docker/Dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64,linux/arm64

  deploy-dev:
    name: Deploy to Development
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/develop'
    environment: development

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up kubectl
      uses: azure/setup-kubectl@v4

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBECONFIG_DEV }}" | base64 -d > kubeconfig
        echo "KUBECONFIG=$(pwd)/kubeconfig" >> $GITHUB_ENV

    - name: Deploy with Kustomize
      run: |
        kubectl apply -k deployments/kustomize/overlays/dev
        kubectl rollout status deployment/dev-mcp-server-langgraph -n mcp-server-langgraph-dev

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/main'
    environment: staging

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up kubectl
      uses: azure/setup-kubectl@v4

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBECONFIG_STAGING }}" | base64 -d > kubeconfig
        echo "KUBECONFIG=$(pwd)/kubeconfig" >> $GITHUB_ENV

    - name: Deploy with Kustomize
      run: |
        kubectl apply -k deployments/kustomize/overlays/staging
        kubectl rollout status deployment/staging-mcp-server-langgraph -n mcp-server-langgraph-staging

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.event_name == 'release'
    environment: production

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Helm
      uses: azure/setup-helm@v4
      with:
        version: '3.13.0'

    - name: Set up kubectl
      uses: azure/setup-kubectl@v4

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBECONFIG_PROD }}" | base64 -d > kubeconfig
        echo "KUBECONFIG=$(pwd)/kubeconfig" >> $GITHUB_ENV

    - name: Deploy with Helm
      run: |
        helm upgrade --install mcp-server-langgraph ./deployments/helm/mcp-server-langgraph \
          --namespace mcp-server-langgraph \
          --create-namespace \
          --set image.tag=${{ github.event.release.tag_name }} \
          --set secrets.anthropicApiKey=${{ secrets.ANTHROPIC_API_KEY }} \
          --set secrets.jwtSecretKey=${{ secrets.JWT_SECRET_KEY }} \
          --set secrets.openfgaStoreId=${{ secrets.OPENFGA_STORE_ID }} \
          --set secrets.openfgaModelId=${{ secrets.OPENFGA_MODEL_ID }} \
          --wait \
          --timeout 10m

    - name: Verify deployment
      run: |
        kubectl rollout status deployment/mcp-server-langgraph -n mcp-server-langgraph
        kubectl get pods -n mcp-server-langgraph
