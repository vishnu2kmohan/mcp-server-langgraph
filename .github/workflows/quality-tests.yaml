name: Quality Tests

# ==============================================================================
# Quality Tests - Advanced Testing Suite
# ==============================================================================
#
# Purpose:
#   Comprehensive quality testing including property-based tests, contract
#   tests, regression tests, mutation tests, and performance benchmarks.
#
# Triggers:
#   - Pull requests to main/develop
#   - Pushes to main/develop
#   - Weekly schedule (Sundays at midnight)
#   - Manual workflow dispatch
#
# Jobs:
#   - Property-Based Tests: Hypothesis-driven testing for edge cases
#   - Contract Tests: MCP protocol compliance verification
#   - Regression Tests: Performance regression detection
#   - Mutation Tests: Test suite effectiveness (weekly only)
#   - Benchmark Tests: Performance benchmarking with trend tracking
#
# Features:
#   - Uses composite action for Python setup
#   - Separate cache keys per job for optimization
#   - Artifact retention: 30-90 days depending on type
#   - Benchmark results published to gh-pages
#
# History:
#   - 2025-10-20: Migrated to composite action for Python setup
#   - 2025-10-20: Standardized branch triggers
#
# ==============================================================================

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main, develop]
  schedule:
    # Run weekly on Sunday at midnight
    - cron: '0 0 * * 0'
  workflow_dispatch:  # Allow manual triggering

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}  # Cancel only for PRs

permissions:
  contents: read
  pull-requests: write

jobs:
  property-tests:
    name: Property-Based Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v5

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          install-test: 'true'
          cache-key-prefix: 'property-tests'

      - name: Run property-based tests
        run: |
          # CI uses 100 examples for comprehensive testing (dev defaults to 25 for speed)
          # HYPOTHESIS_PROFILE=ci activates the CI profile registered in conftest.py
          HYPOTHESIS_PROFILE=ci uv run pytest -m property -v --tb=short
        timeout-minutes: 15
        env:
          HYPOTHESIS_PROFILE: ci

      - name: Upload property test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: property-test-results
          path: |
            .hypothesis/
            test-results/
          retention-days: 30

  contract-tests:
    name: Contract Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v5

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          install-test: 'true'
          cache-key-prefix: 'contract-tests'

      - name: Generate OpenAPI schema
        run: |
          uv run python scripts/validation/validate_openapi.py || true

      - name: Run contract tests
        run: |
          uv run pytest -m contract -v --tb=short

      - name: Upload OpenAPI schema
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: openapi-schema
          path: docs/api/openapi.json
          retention-days: 30

  regression-tests:
    name: Performance Regression Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v5

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          install-test: 'true'
          cache-key-prefix: 'regression-tests'

      - name: Run regression tests
        run: |
          uv run pytest -m regression -v --tb=short

      - name: Check for performance regressions
        run: |
          echo "Comparing against baseline metrics..."
          # Results are checked within the tests themselves

      - name: Upload regression results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: regression-results
          path: |
            tests/regression/
            .benchmarks/
          retention-days: 30

  mutation-tests:
    name: Mutation Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'  # Only on weekly schedule (too slow for PRs)

    steps:
      - uses: actions/checkout@v5

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          install-test: 'true'
          cache-key-prefix: 'mutation-tests'

      - name: Run mutation tests
        run: |
          uv run mutmut run --paths-to-mutate=src/mcp_server_langgraph/
        timeout-minutes: 60
        continue-on-error: true  # Don't fail workflow if mutation score is low

      - name: Generate mutation report
        if: always()
        run: |
          uv run mutmut results || true
          uv run mutmut html || true

      - name: Upload mutation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mutation-test-results
          path: html/
          retention-days: 30

  benchmark-tests:
    name: Benchmark Tests
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Required for benchmark-action to push to gh-pages

    steps:
      - uses: actions/checkout@v5

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          install-test: 'true'
          cache-key-prefix: 'benchmark-tests'

      - name: Run benchmarks
        run: |
          uv run pytest -m benchmark -v --benchmark-only --benchmark-autosave --benchmark-json=benchmarks.json
        continue-on-error: true  # Don't fail if no benchmarks exist yet

      - name: Store benchmark results
        continue-on-error: true  # Don't fail workflow if gh-pages push fails
        uses: benchmark-action/github-action-benchmark@v1.20.7
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        with:
          tool: 'pytest'
          output-file-path: benchmarks.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench
          alert-threshold: '120%'  # Alert if 20% slower
          comment-on-alert: true
          fail-on-alert: false

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmarks.json
            .benchmarks/
          retention-days: 90

  quality-summary:
    name: Quality Summary
    runs-on: ubuntu-latest
    needs: [property-tests, contract-tests, regression-tests, benchmark-tests]
    if: always()

    steps:
      - name: Check quality test results
        run: |
          echo "## Quality Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Property Tests: ${{ needs.property-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Contract Tests: ${{ needs.contract-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Regression Tests: ${{ needs.regression-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmark Tests: ${{ needs.benchmark-tests.result }}" >> $GITHUB_STEP_SUMMARY

          # Mutation tests only run on schedule
          if [ "${{ github.event_name }}" = "schedule" ]; then
            echo "- Mutation Tests: ${{ needs.mutation-tests.result }}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check required tests (mutation tests are optional/informational)
          if [ "${{ needs.property-tests.result }}" = "success" ] && \
             [ "${{ needs.contract-tests.result }}" = "success" ] && \
             [ "${{ needs.regression-tests.result }}" = "success" ] && \
             [ "${{ needs.benchmark-tests.result }}" = "success" ]; then
            echo "✅ All quality tests passed!" >> $GITHUB_STEP_SUMMARY
            exit 0
          else
            echo "⚠️ Some quality tests failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
