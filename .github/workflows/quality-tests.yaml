name: Quality Tests

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]
  schedule:
    # Run weekly on Sunday at midnight
    - cron: '0 0 * * 0'
  workflow_dispatch:  # Allow manual triggering

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}  # Cancel only for PRs

permissions:
  contents: read
  pull-requests: write

jobs:
  property-tests:
    name: Property-Based Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-pinned.txt
          pip install -r requirements-test.txt

      - name: Run property-based tests
        run: |
          pytest -m property -v --tb=short
        timeout-minutes: 15

      - name: Upload property test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: property-test-results
          path: |
            .hypothesis/
            test-results/
          retention-days: 30

  contract-tests:
    name: Contract Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-pinned.txt
          pip install -r requirements-test.txt

      - name: Generate OpenAPI schema
        run: |
          python scripts/validation/validate_openapi.py || true

      - name: Run contract tests
        run: |
          pytest -m contract -v --tb=short

      - name: Upload OpenAPI schema
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: openapi-schema
          path: docs/api/openapi.json
          retention-days: 30

  regression-tests:
    name: Performance Regression Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-pinned.txt
          pip install -r requirements-test.txt

      - name: Run regression tests
        run: |
          pytest -m regression -v --tb=short

      - name: Check for performance regressions
        run: |
          echo "Comparing against baseline metrics..."
          # Results are checked within the tests themselves

      - name: Upload regression results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: regression-results
          path: |
            tests/regression/
            .benchmarks/
          retention-days: 30

  mutation-tests:
    name: Mutation Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'  # Only on weekly schedule (too slow for PRs)

    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-pinned.txt
          pip install -r requirements-test.txt
          pip install mutmut

      - name: Run mutation tests
        run: |
          mutmut run --paths-to-mutate=src/mcp_server_langgraph/
        timeout-minutes: 60
        continue-on-error: true  # Don't fail workflow if mutation score is low

      - name: Generate mutation report
        if: always()
        run: |
          mutmut results || true
          mutmut html || true

      - name: Upload mutation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mutation-test-results
          path: html/
          retention-days: 30

  benchmark-tests:
    name: Benchmark Tests
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Required for benchmark-action to push to gh-pages

    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-pinned.txt
          pip install -r requirements-test.txt

      - name: Run benchmarks
        run: |
          pytest -m benchmark -v --benchmark-only --benchmark-autosave --benchmark-json=benchmarks.json
        continue-on-error: true  # Don't fail if no benchmarks exist yet

      - name: Store benchmark results
        continue-on-error: true  # Don't fail workflow if gh-pages push fails
        uses: benchmark-action/github-action-benchmark@v1.20.3
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        with:
          tool: 'pytest'
          output-file-path: benchmarks.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench
          alert-threshold: '120%'  # Alert if 20% slower
          comment-on-alert: true
          fail-on-alert: false

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmarks.json
            .benchmarks/
          retention-days: 90

  quality-summary:
    name: Quality Summary
    runs-on: ubuntu-latest
    needs: [property-tests, contract-tests, regression-tests, benchmark-tests]
    if: always()

    steps:
      - name: Check quality test results
        run: |
          echo "## Quality Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Property Tests: ${{ needs.property-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Contract Tests: ${{ needs.contract-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Regression Tests: ${{ needs.regression-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmark Tests: ${{ needs.benchmark-tests.result }}" >> $GITHUB_STEP_SUMMARY

          # Mutation tests only run on schedule
          if [ "${{ github.event_name }}" = "schedule" ]; then
            echo "- Mutation Tests: ${{ needs.mutation-tests.result }}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check required tests (mutation tests are optional/informational)
          if [ "${{ needs.property-tests.result }}" = "success" ] && \
             [ "${{ needs.contract-tests.result }}" = "success" ] && \
             [ "${{ needs.regression-tests.result }}" = "success" ] && \
             [ "${{ needs.benchmark-tests.result }}" = "success" ]; then
            echo "✅ All quality tests passed!" >> $GITHUB_STEP_SUMMARY
            exit 0
          else
            echo "⚠️ Some quality tests failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
