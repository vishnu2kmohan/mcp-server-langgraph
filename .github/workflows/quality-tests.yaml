name: Quality Tests

# ==============================================================================
# Quality Tests - Advanced Testing Suite
# ==============================================================================
#
# Purpose:
#   Comprehensive quality testing including property-based tests, contract
#   tests, regression tests, mutation tests, and performance benchmarks.
#
# Triggers:
#   - Pull requests to main/develop
#   - Pushes to main/develop
#   - Weekly schedule (Sundays at midnight)
#   - Manual workflow dispatch
#
# Jobs:
#   - Property-Based Tests: Hypothesis-driven testing for edge cases
#   - Contract Tests: MCP protocol compliance verification
#   - Regression Tests: Performance regression detection
#   - Mutation Tests: Test suite effectiveness (weekly only)
#   - Benchmark Tests: Performance benchmarking with trend tracking
#
# Features:
#   - Uses composite action for Python setup
#   - Separate cache keys per job for optimization
#   - Artifact retention: 30-90 days depending on type
#   - Benchmark results published to gh-pages
#
# History:
#   - 2025-10-20: Migrated to composite action for Python setup
#   - 2025-10-20: Standardized branch triggers
#
# ==============================================================================

on:
  pull_request:
    branches: [main, develop]
    paths-ignore:
      - 'docs/**'
      - '**.md'
      - '**.mdx'
      - 'adr/**'
  push:
    branches: [main, develop]
    paths-ignore:
      - 'docs/**'
      - '**.md'
      - '**.mdx'
      - 'adr/**'
  schedule:
    # Run weekly on Sunday at midnight
    - cron: '0 0 * * 0'
  workflow_dispatch:  # Allow manual triggering

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}  # Cancel only for PRs

permissions:
  contents: read
  pull-requests: write

env:
  # Prevent BlockingIOError when capturing large test output
  PYTHONUNBUFFERED: "1"

jobs:
  pre-commit-validation:
    name: Pre-commit Hook Configuration Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - uses: actions/checkout@v6

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          extras: 'dev'

      - name: Validate pre-commit hook configuration
        run: |
          uv run --frozen pytest tests/regression/test_precommit_hook_dependencies.py -v --tb=short
        timeout-minutes: 5

  property-tests:
    name: Property-Based Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - uses: actions/checkout@v6

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          extras: 'dev'

      - name: Run property-based tests
        run: |
          # CI uses 100 examples for comprehensive testing (dev defaults to 25 for speed)
          # HYPOTHESIS_PROFILE=ci activates the CI profile registered in conftest.py
          # Match local development: parallel execution with OTEL disabled
          # Note: Property tests run in isolation, coverage threshold set to 0 (overrides pyproject.toml's 66%)
          OTEL_SDK_DISABLED=true HYPOTHESIS_PROFILE=ci uv run --frozen pytest -n auto -m property -v --tb=short --cov=src/mcp_server_langgraph --cov-report=xml:coverage-property.xml --cov-fail-under=0
        timeout-minutes: 15
        env:
          HYPOTHESIS_PROFILE: ci

      - name: Upload property test coverage
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: coverage-property
          path: coverage-property.xml
          retention-days: 1

      - name: Upload property test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: property-test-results
          path: |
            .hypothesis/
            test-results/
          retention-days: 30

  contract-tests:
    name: Contract Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - uses: actions/checkout@v6

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          extras: 'dev'

      - name: Generate OpenAPI schema
        run: |
          # Allow failure: openapi-spec-validator may not be installed in all environments
          # The script provides helpful error message: "Run: uv pip install openapi-spec-validator"
          # This is non-blocking for contract tests which focus on MCP protocol compliance
          uv run --frozen python scripts/validation/validate_openapi.py || echo "⚠️  OpenAPI validation skipped (missing dependencies)"

      - name: Run contract tests
        run: |
          set -euo pipefail  # Exit on error, undefined vars, pipe failures

          # Match local development: parallel execution with OTEL disabled
          # This will fail the job if pytest exits with non-zero (due to set -e above)
          # Note: Contract tests focus on protocol compliance, not code coverage
          # Coverage collection enabled for reporting, but threshold set to 0 (overrides pyproject.toml's 66%)
          OTEL_SDK_DISABLED=true uv run --frozen pytest -n auto -m contract -v --tb=short --cov=src/mcp_server_langgraph --cov-report=xml:coverage-contract.xml --cov-fail-under=0

          echo "✅ All contract tests passed successfully"

      - name: Upload contract test coverage
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: coverage-contract
          path: coverage-contract.xml
          retention-days: 1

      - name: Upload OpenAPI schema
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: openapi-schema
          path: docs/api/openapi.json
          retention-days: 30

  regression-tests:
    name: Performance Regression Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    # Simplified to single job with pytest-xdist for parallel execution
    # Replaced pytest-split matrix (2025-11-28) to enable pytest 9.x upgrade
    # pytest-xdist handles parallelization internally with -n auto

    steps:
      - uses: actions/checkout@v6

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          extras: 'dev'

      - name: Run regression tests
        run: |
          # Use pytest-xdist for parallel execution
          # -n auto: Automatically detect number of CPUs
          # --dist loadgroup: Distribute by xdist_group marker for better isolation
          # Note: Regression tests focus on performance, coverage threshold set to 0
          OTEL_SDK_DISABLED=true uv run --frozen pytest \
            -n auto \
            --dist loadgroup \
            -m regression \
            -v --tb=short \
            --cov=src/mcp_server_langgraph \
            --cov-report=xml:coverage-regression.xml \
            --cov-fail-under=0

      - name: Check for performance regressions
        run: |
          echo "Comparing against baseline metrics..."
          # Results are checked within the tests themselves

      - name: Upload regression test coverage
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: coverage-regression
          path: coverage-regression.xml
          retention-days: 1

      - name: Upload regression results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: regression-results
          path: |
            tests/regression/
            .benchmarks/
          retention-days: 30

  mutation-tests:
    name: Mutation Testing
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: github.event_name == 'schedule'  # Only on weekly schedule (too slow for PRs)

    steps:
      - uses: actions/checkout@v6

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          extras: 'dev'

      - name: Run mutation tests
        run: |
          uv run --frozen mutmut run --paths-to-mutate=src/mcp_server_langgraph/
        timeout-minutes: 60
        continue-on-error: true  # Don't fail workflow if mutation score is low

      - name: Generate mutation report
        if: always()
        run: |
          uv run --frozen mutmut results || true
          uv run --frozen mutmut html || true

      - name: Upload mutation results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: mutation-test-results
          path: html/
          retention-days: 30

  benchmark-tests:
    name: Benchmark Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      contents: write  # Required for benchmark-action to push to gh-pages

    steps:
      - uses: actions/checkout@v6

      - name: Setup Python and dependencies
        uses: ./.github/actions/setup-python-deps
        with:
          python-version: '3.12'
          extras: 'dev'

      - name: Run benchmarks
        run: |
          # IMPORTANT: Disable xdist AND override addopts to prevent conflicts
          # - -p no:xdist: Disables pytest-xdist plugin
          # - -o addopts="...": Overrides pyproject.toml's addopts which includes --dist loadgroup
          # Without the override, pytest errors with "unrecognized arguments: --dist"
          # See: https://github.com/pytest-dev/pytest/issues/5765
          uv run --frozen pytest -m benchmark -v \
            -p no:xdist \
            -o "addopts=-v --strict-markers --tb=short --timeout=60" \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-json=benchmarks.json

      - name: Store benchmark results
        continue-on-error: true  # Don't fail workflow if gh-pages push fails
        uses: benchmark-action/github-action-benchmark@v1.20.7
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        with:
          tool: 'pytest'
          output-file-path: benchmarks.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench
          alert-threshold: '120%'  # Alert if 20% slower
          comment-on-alert: true
          fail-on-alert: false

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: benchmark-results
          path: |
            benchmarks.json
            .benchmarks/
          retention-days: 90

  quality-summary:
    name: Quality Summary
    runs-on: ubuntu-latest
    # Note: mutation-tests excluded from needs - only runs on schedule (too slow for PRs)
    # Including it in needs would block PRs/push events from completing
    needs: [property-tests, contract-tests, regression-tests, benchmark-tests]
    if: always()

    steps:
      - name: Check quality test results
        run: |
          echo "## Quality Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Property Tests: ${{ needs.property-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Contract Tests: ${{ needs.contract-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Regression Tests: ${{ needs.regression-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmark Tests: ${{ needs.benchmark-tests.result }}" >> $GITHUB_STEP_SUMMARY
          # Note: Mutation tests run independently on schedule - results not aggregated here
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check required tests (mutation tests are optional/informational)
          if [ "${{ needs.property-tests.result }}" = "success" ] && \
             [ "${{ needs.contract-tests.result }}" = "success" ] && \
             [ "${{ needs.regression-tests.result }}" = "success" ] && \
             [ "${{ needs.benchmark-tests.result }}" = "success" ]; then
            echo "✅ All quality tests passed!" >> $GITHUB_STEP_SUMMARY
            exit 0
          else
            echo "⚠️ Some quality tests failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
